{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modu_deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONcO4wSTEolPBZ8fPLgSG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kim-bo-saeng/Practice_N_Learning/blob/master/modu_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POreyGmp4EcC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5995b862-3dd8-4c30-a1ba-61c5aaa3f7f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80wEYGgL3ReA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "058baa24-2bbe-4c1b-ae19-aee42bd343f0"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ULzbk6q3uZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "Data_set = np.loadtxt('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/ThoraricSurgery.csv', delimiter=\",\")\n",
        "\n",
        "X = Data_set[:,0:17]\n",
        "Y = Data_set[:,17]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 17, activation = 'relu'))\n",
        "model.add(Dense(1, activation = 'sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(X, Y, epochs = 100, batch_size = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0uv7_pd6ywI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = [2,4,6,8]\n",
        "y = [81,93,91,97]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49lje2c33lrd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "e7a13c7d-6380-420a-f11d-32ae80245ee5"
      },
      "source": [
        "mx = np.mean(x)\n",
        "my = np.mean(y)\n",
        "print(mx)\n",
        "print(my)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.0\n",
            "90.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNL097_i3rlI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f761466-44b9-463b-b9b9-473a04302fbe"
      },
      "source": [
        "divisor = sum([(i - mx)**2 for i in x])\n",
        "divisor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqeAuJ8G35Qw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c9e7bd0a-d8c6-40b1-daf0-7b7925466213"
      },
      "source": [
        "def top(x, mx, y, my):\n",
        "  d = 0\n",
        "  for i in range(len(x)):\n",
        "    d += (x[i] - mx) * (y[i] - my)\n",
        "  return d\n",
        "dividend = top(x, mx, y, my)\n",
        "dividend"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLeNQaRo4fSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = dividend / divisor\n",
        "b = my - (mx*a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrgcT6cW4tOH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1906d66-92c4-4502-e7a4-617e8d31afbd"
      },
      "source": [
        "a"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USesFcEo4t4c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec0e1e70-8dec-4714-da18-c7d51c82a7eb"
      },
      "source": [
        "b"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "79.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zWMnFEf4uOz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "7d4f8976-ec71-4973-d7b5-639053dd9e20"
      },
      "source": [
        "fake_a_b = [3, 76]\n",
        "\n",
        "data = [[2, 81], [4, 93], [6, 91], [8, 97]]\n",
        "x = [i[0] for i in data]\n",
        "y = [i[1] for i in data]\n",
        "\n",
        "def predict(x):\n",
        "  return fake_a_b[0]*x + fake_a_b[1]\n",
        "\n",
        "def mse(y , y_hat):\n",
        "  return ((y - y_hat)**2).mean()\n",
        "\n",
        "def mse_val(y, predict_result):\n",
        "  return mse(np.array(y), np.array(predict_result))\n",
        "\n",
        "predict_result = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "  predict_result.append(predict(x[i]))\n",
        "  print(\"공부한 시간=%.f, 실제 점수 = %.f, 예측점수 = %.f\" % (x[i], y[i], predict(x[i])))\n",
        "\n",
        "print(\"mse 최종값: \" + str(mse_val(predict_result, y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "공부한 시간=2, 실제 점수 = 81, 예측점수 = 82\n",
            "공부한 시간=4, 실제 점수 = 93, 예측점수 = 88\n",
            "공부한 시간=6, 실제 점수 = 91, 예측점수 = 94\n",
            "공부한 시간=8, 실제 점수 = 97, 예측점수 = 100\n",
            "mse 최종값: 11.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80ycY7KT80mm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = [[2, 81], [4, 93], [6, 91], [8, 97]]\n",
        "x = [i[0] for i in data]\n",
        "y = [i[1] for i in data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axIbVXngBDVe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "637cb8b6-775a-4859-8d2f-1fcbf0ef72e1"
      },
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.scatter(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7fa453a61588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUeqcUGBBD59",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array(x)\n",
        "y_data = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZwv5TvoCB5F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = 0\n",
        "b = 0\n",
        "lr = 0.03\n",
        "epochs = 2001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl2lOBxJCPKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "8d623b09-2511-45d2-9e32-b5038916fb26"
      },
      "source": [
        "for i in range(epochs):\n",
        "  y_pred = a * x_data + b\n",
        "  error = y_data - y_pred\n",
        "\n",
        "  a_diff = -(2/len(x_data)) * sum(x_data * error)\n",
        "  b_diff = -(2/len(x_data)) * sum(error)\n",
        "\n",
        "  a = a - lr * a_diff\n",
        "  b = b - lr * b_diff\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(\"epochs = %.f, 기울기 = %.04f, 절편 = %.04f\" % (i, a, b))\n",
        "\n",
        "y_pred = a * x_data + b\n",
        "plt.scatter(x, y)\n",
        "plt.plot([min(x_data), max(x_data)], [min(y_pred), max(y_pred)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs = 0, 기울기 = 27.8400, 절편 = 5.4300\n",
            "epochs = 100, 기울기 = 7.0739, 절편 = 50.5117\n",
            "epochs = 200, 기울기 = 4.0960, 절편 = 68.2822\n",
            "epochs = 300, 기울기 = 2.9757, 절편 = 74.9678\n",
            "epochs = 400, 기울기 = 2.5542, 절편 = 77.4830\n",
            "epochs = 500, 기울기 = 2.3956, 절편 = 78.4293\n",
            "epochs = 600, 기울기 = 2.3360, 절편 = 78.7853\n",
            "epochs = 700, 기울기 = 2.3135, 절편 = 78.9192\n",
            "epochs = 800, 기울기 = 2.3051, 절편 = 78.9696\n",
            "epochs = 900, 기울기 = 2.3019, 절편 = 78.9886\n",
            "epochs = 1000, 기울기 = 2.3007, 절편 = 78.9957\n",
            "epochs = 1100, 기울기 = 2.3003, 절편 = 78.9984\n",
            "epochs = 1200, 기울기 = 2.3001, 절편 = 78.9994\n",
            "epochs = 1300, 기울기 = 2.3000, 절편 = 78.9998\n",
            "epochs = 1400, 기울기 = 2.3000, 절편 = 78.9999\n",
            "epochs = 1500, 기울기 = 2.3000, 절편 = 79.0000\n",
            "epochs = 1600, 기울기 = 2.3000, 절편 = 79.0000\n",
            "epochs = 1700, 기울기 = 2.3000, 절편 = 79.0000\n",
            "epochs = 1800, 기울기 = 2.3000, 절편 = 79.0000\n",
            "epochs = 1900, 기울기 = 2.3000, 절편 = 79.0000\n",
            "epochs = 2000, 기울기 = 2.3000, 절편 = 79.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fa452f3a048>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R04RGL6vEdBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "cd603483-28bf-49da-8fd7-b38f129296ee"
      },
      "source": [
        "data = [[2,0,81],[4,4,93],[6,2,91],[8,3,97]]\n",
        "x1 = [i[0] for i in data]\n",
        "x2 = [i[1] for i in data]\n",
        "y = [i[2] for i in data]\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "\n",
        "ax = plt.axes(projection = '3d')\n",
        "ax.set_xlabel('study_hours')\n",
        "ax.set_ylabel('prvate_class')\n",
        "ax.set_zlabel('Score')\n",
        "ax.dist = 11\n",
        "ax.scatter(x1, x2, y)\n",
        "\n",
        "x1_data = np.array(x1)\n",
        "x2_data = np.array(x2)\n",
        "y_data = np.array(y)\n",
        "\n",
        "a1 = 0\n",
        "a2 = 0\n",
        "b = 0\n",
        "lr = 0.05\n",
        "epochs = 2001\n",
        "\n",
        "for i in range(epochs):\n",
        "  y_pred = a1 * x1_data + a2 * x2_data + b\n",
        "  error = y_data - y_pred\n",
        "\n",
        "  a1_diff = -(1/len(x1_data)) * sum(x1_data * error)\n",
        "  a2_diff = -(1/len(x2_data)) * sum(x2_data * error)\n",
        "  b_diff = -(1/len(x1_data)) * sum(y_data - y_pred)\n",
        "  a1 = a1 - lr * a1_diff\n",
        "  a2 = a2 - lr * a2_diff\n",
        "  b = b - lr * b_diff\n",
        "\n",
        "  if i % 100 == 0:\n",
        "    print(\"epochs = %.f, 기울기1 = %.04f, 기울기2 = %.04f,절편 = %.04f\" % (i, a1, a2, b))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs = 0, 기울기1 = 23.2000, 기울기2 = 10.5625,절편 = 4.5250\n",
            "epochs = 100, 기울기1 = 6.4348, 기울기2 = 3.9893,절편 = 43.9757\n",
            "epochs = 200, 기울기1 = 3.7255, 기울기2 = 3.0541,절편 = 62.5766\n",
            "epochs = 300, 기울기1 = 2.5037, 기울기2 = 2.6323,절편 = 70.9656\n",
            "epochs = 400, 기울기1 = 1.9527, 기울기2 = 2.4420,절편 = 74.7491\n",
            "epochs = 500, 기울기1 = 1.7042, 기울기2 = 2.3562,절편 = 76.4554\n",
            "epochs = 600, 기울기1 = 1.5921, 기울기2 = 2.3175,절편 = 77.2250\n",
            "epochs = 700, 기울기1 = 1.5415, 기울기2 = 2.3001,절편 = 77.5720\n",
            "epochs = 800, 기울기1 = 1.5187, 기울기2 = 2.2922,절편 = 77.7286\n",
            "epochs = 900, 기울기1 = 1.5084, 기울기2 = 2.2886,절편 = 77.7992\n",
            "epochs = 1000, 기울기1 = 1.5038, 기울기2 = 2.2870,절편 = 77.8310\n",
            "epochs = 1100, 기울기1 = 1.5017, 기울기2 = 2.2863,절편 = 77.8453\n",
            "epochs = 1200, 기울기1 = 1.5008, 기울기2 = 2.2860,절편 = 77.8518\n",
            "epochs = 1300, 기울기1 = 1.5003, 기울기2 = 2.2858,절편 = 77.8547\n",
            "epochs = 1400, 기울기1 = 1.5002, 기울기2 = 2.2858,절편 = 77.8561\n",
            "epochs = 1500, 기울기1 = 1.5001, 기울기2 = 2.2857,절편 = 77.8567\n",
            "epochs = 1600, 기울기1 = 1.5000, 기울기2 = 2.2857,절편 = 77.8569\n",
            "epochs = 1700, 기울기1 = 1.5000, 기울기2 = 2.2857,절편 = 77.8570\n",
            "epochs = 1800, 기울기1 = 1.5000, 기울기2 = 2.2857,절편 = 77.8571\n",
            "epochs = 1900, 기울기1 = 1.5000, 기울기2 = 2.2857,절편 = 77.8571\n",
            "epochs = 2000, 기울기1 = 1.5000, 기울기2 = 2.2857,절편 = 77.8571\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0Sxck79SH5X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "236c86f7-852c-445e-ee71-04603d614c72"
      },
      "source": [
        "data = [[2,0], [4,0], [6,0], [8,1], [10,1], [12,1], [14,1]]\n",
        "\n",
        "x_data = [i[0] for i in data]\n",
        "y_data = [i[1] for i in data]\n",
        "\n",
        "plt.scatter(x_data, y_data)\n",
        "plt.xlim(0,15)\n",
        "plt.ylim(-.1,1.1)\n",
        "\n",
        "a = 0\n",
        "b = 0\n",
        "lr = 0.05\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.e ** (-x))\n",
        "\n",
        "for i in range(2001):\n",
        "  for x_data, y_data in data:\n",
        "    a_diff = x_data * (sigmoid(a * x_data + b) - y_data)\n",
        "    b_diff = sigmoid(a * x_data + b) - y_data\n",
        "    a = a - lr * a_diff\n",
        "    b = b - lr * b_diff\n",
        "    if i % 1000 == 0:\n",
        "      print('epochs=%.f, 기울기=%.04f, 절편=%.04f' % (i,a,b))\n",
        "\n",
        "  plt.scatter(x_data, y_data)\n",
        "  plt.xlim(0,15)\n",
        "  plt.ylim(-.1,1.1)\n",
        "  x_range = (np.arange(0, 15, 0.1))\n",
        "  plt.plot(np.arange(0,15,0.1), np.array([sigmoid(a*x + b) for x in x_range]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epochs=0, 기울기=-0.0500, 절편=-0.0250\n",
            "epochs=0, 기울기=-0.1388, 절편=-0.0472\n",
            "epochs=0, 기울기=-0.2268, 절편=-0.0619\n",
            "epochs=0, 기울기=0.1201, 절편=-0.0185\n",
            "epochs=0, 기울기=0.2374, 절편=-0.0068\n",
            "epochs=0, 기울기=0.2705, 절편=-0.0040\n",
            "epochs=0, 기울기=0.2860, 절편=-0.0029\n",
            "epochs=1000, 기울기=1.4978, 절편=-9.9401\n",
            "epochs=1000, 기울기=1.4940, 절편=-9.9411\n",
            "epochs=1000, 기울기=1.4120, 절편=-9.9547\n",
            "epochs=1000, 기울기=1.4949, 절편=-9.9444\n",
            "epochs=1000, 기울기=1.4982, 절편=-9.9440\n",
            "epochs=1000, 기울기=1.4984, 절편=-9.9440\n",
            "epochs=1000, 기울기=1.4985, 절편=-9.9440\n",
            "epochs=2000, 기울기=1.9065, 절편=-12.9489\n",
            "epochs=2000, 기울기=1.9055, 절편=-12.9491\n",
            "epochs=2000, 기울기=1.8515, 절편=-12.9581\n",
            "epochs=2000, 기울기=1.9057, 절편=-12.9514\n",
            "epochs=2000, 기울기=1.9068, 절편=-12.9513\n",
            "epochs=2000, 기울기=1.9068, 절편=-12.9513\n",
            "epochs=2000, 기울기=1.9068, 절편=-12.9513\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibisFmAgcajf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "7a5077b4-49f1-4a8c-933a-90cdcf9b5753"
      },
      "source": [
        "w11 = np.array([-2, -2])\n",
        "w12 = np.array([2, 2])\n",
        "w2 = np.array([1, 1])\n",
        "b1 = 3\n",
        "b2 = -1\n",
        "b3 = -1\n",
        "\n",
        "def MLP(x, w, b):\n",
        "  y = np.sum(w*x) + b\n",
        "  if y <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "\n",
        "def NAND(x1, x2):\n",
        "  return MLP(np.array([x1, x2]), w11, b1)\n",
        "\n",
        "def OR(x1, x2):\n",
        "  return MLP(np.array([x1, x2]), w12, b2)\n",
        "\n",
        "def AND(x1, x2):\n",
        "  return MLP(np.array([x1, x2]), w2, b3)\n",
        "\n",
        "def XOR(x1, x2):\n",
        "  return AND(NAND(x1, x2) ,OR(x1, x2))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  for x in [(0,0), (1,0), (0,1), (1,1)]:\n",
        "    y = XOR(x[0], x[1])\n",
        "    print('입력 값: ' + str(x) + '출력 값: ' + str(y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 값: (0, 0)출력 값: 0\n",
            "입력 값: (1, 0)출력 값: 1\n",
            "입력 값: (0, 1)출력 값: 1\n",
            "입력 값: (1, 1)출력 값: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYYu1pQ3hF2t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "0688f478-7ce7-42d1-b82d-9f19756dfc2b"
      },
      "source": [
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/pima-indians-diabetes.csv', names = [\"pregnant\", \"plasma\", \"pressure\", \"thickness\",\n",
        "                                                                                                                              \"insulin\", \"BMI\", \"pedigree\", \"age\", \"class\"])\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>plasma</th>\n",
              "      <th>pressure</th>\n",
              "      <th>thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6</td>\n",
              "      <td>148</td>\n",
              "      <td>72</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>33.6</td>\n",
              "      <td>0.627</td>\n",
              "      <td>50</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>85</td>\n",
              "      <td>66</td>\n",
              "      <td>29</td>\n",
              "      <td>0</td>\n",
              "      <td>26.6</td>\n",
              "      <td>0.351</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8</td>\n",
              "      <td>183</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.672</td>\n",
              "      <td>32</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>66</td>\n",
              "      <td>23</td>\n",
              "      <td>94</td>\n",
              "      <td>28.1</td>\n",
              "      <td>0.167</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>137</td>\n",
              "      <td>40</td>\n",
              "      <td>35</td>\n",
              "      <td>168</td>\n",
              "      <td>43.1</td>\n",
              "      <td>2.288</td>\n",
              "      <td>33</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   pregnant  plasma  pressure  thickness  insulin   BMI  pedigree  age  class\n",
              "0         6     148        72         35        0  33.6     0.627   50      1\n",
              "1         1      85        66         29        0  26.6     0.351   31      0\n",
              "2         8     183        64          0        0  23.3     0.672   32      1\n",
              "3         1      89        66         23       94  28.1     0.167   21      0\n",
              "4         0     137        40         35      168  43.1     2.288   33      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S4yAh0j219V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "455bda56-691b-4216-f15a-08654cd14ef2"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 768 entries, 0 to 767\n",
            "Data columns (total 9 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   pregnant   768 non-null    int64  \n",
            " 1   plasma     768 non-null    int64  \n",
            " 2   pressure   768 non-null    int64  \n",
            " 3   thickness  768 non-null    int64  \n",
            " 4   insulin    768 non-null    int64  \n",
            " 5   BMI        768 non-null    float64\n",
            " 6   pedigree   768 non-null    float64\n",
            " 7   age        768 non-null    int64  \n",
            " 8   class      768 non-null    int64  \n",
            "dtypes: float64(2), int64(7)\n",
            "memory usage: 54.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sh5Konkp3VLz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "1e09ddf4-f848-4a37-c4a9-06daf66fc313"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>plasma</th>\n",
              "      <th>pressure</th>\n",
              "      <th>thickness</th>\n",
              "      <th>insulin</th>\n",
              "      <th>BMI</th>\n",
              "      <th>pedigree</th>\n",
              "      <th>age</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "      <td>768.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.845052</td>\n",
              "      <td>120.894531</td>\n",
              "      <td>69.105469</td>\n",
              "      <td>20.536458</td>\n",
              "      <td>79.799479</td>\n",
              "      <td>31.992578</td>\n",
              "      <td>0.471876</td>\n",
              "      <td>33.240885</td>\n",
              "      <td>0.348958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>3.369578</td>\n",
              "      <td>31.972618</td>\n",
              "      <td>19.355807</td>\n",
              "      <td>15.952218</td>\n",
              "      <td>115.244002</td>\n",
              "      <td>7.884160</td>\n",
              "      <td>0.331329</td>\n",
              "      <td>11.760232</td>\n",
              "      <td>0.476951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.078000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>27.300000</td>\n",
              "      <td>0.243750</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>117.000000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>30.500000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>0.372500</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>140.250000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>127.250000</td>\n",
              "      <td>36.600000</td>\n",
              "      <td>0.626250</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>17.000000</td>\n",
              "      <td>199.000000</td>\n",
              "      <td>122.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>846.000000</td>\n",
              "      <td>67.100000</td>\n",
              "      <td>2.420000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         pregnant      plasma    pressure  ...    pedigree         age       class\n",
              "count  768.000000  768.000000  768.000000  ...  768.000000  768.000000  768.000000\n",
              "mean     3.845052  120.894531   69.105469  ...    0.471876   33.240885    0.348958\n",
              "std      3.369578   31.972618   19.355807  ...    0.331329   11.760232    0.476951\n",
              "min      0.000000    0.000000    0.000000  ...    0.078000   21.000000    0.000000\n",
              "25%      1.000000   99.000000   62.000000  ...    0.243750   24.000000    0.000000\n",
              "50%      3.000000  117.000000   72.000000  ...    0.372500   29.000000    0.000000\n",
              "75%      6.000000  140.250000   80.000000  ...    0.626250   41.000000    1.000000\n",
              "max     17.000000  199.000000  122.000000  ...    2.420000   81.000000    1.000000\n",
              "\n",
              "[8 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYXRheHQ3a1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 576
        },
        "outputId": "e3299b3e-421f-427a-ed4b-59b500486b0b"
      },
      "source": [
        "df[['pregnant', 'class']].groupby(['pregnant']).mean().reset_index()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>pregnant</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.342342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.214815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.184466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.360000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.338235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>0.368421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>0.555556</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>0.578947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>0.416667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>0.636364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>0.444444</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    pregnant     class\n",
              "0          0  0.342342\n",
              "1          1  0.214815\n",
              "2          2  0.184466\n",
              "3          3  0.360000\n",
              "4          4  0.338235\n",
              "5          5  0.368421\n",
              "6          6  0.320000\n",
              "7          7  0.555556\n",
              "8          8  0.578947\n",
              "9          9  0.642857\n",
              "10        10  0.416667\n",
              "11        11  0.636364\n",
              "12        12  0.444444\n",
              "13        13  0.500000\n",
              "14        14  1.000000\n",
              "15        15  1.000000\n",
              "16        17  1.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpZnKJ-G4TgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "outputId": "0f7b1ac5-37ec-4aeb-f3a9-d0a7491a1f4c"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "\n",
        "sns.heatmap(df.corr(), linewidths = 0.1, vmax = 0.5, cmap = plt.cm.gist_heat, linecolor = 'white', annot=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe687ec1978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x864 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hHV8ItL5o_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "61130328-247b-48e3-839d-256a60cb57d0"
      },
      "source": [
        "grid = sns.FacetGrid(df, col = 'class')\n",
        "grid.map(plt.hist, 'plasma', bins = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7fe683c16fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x216 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6UqN3Q_6gZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "888f9431-6f8b-4ccb-ae18-3ba6867db94f"
      },
      "source": [
        "seed = 3\n",
        "\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "dataset = np.loadtxt('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/pima-indians-diabetes.csv', delimiter=\",\")\n",
        "X = dataset[:,0:8]\n",
        "Y = dataset[:,8]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim = 8, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, Y, epochs = 200, batch_size = 10)\n",
        "\n",
        "print(\"\\n Accuracy: %.4f\" % (model.evaluate(X,Y)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 10.5440 - accuracy: 0.6159\n",
            "Epoch 2/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 5.4367 - accuracy: 0.6029\n",
            "Epoch 3/200\n",
            "77/77 [==============================] - 0s 965us/step - loss: 2.9292 - accuracy: 0.5208\n",
            "Epoch 4/200\n",
            "77/77 [==============================] - 0s 993us/step - loss: 1.5346 - accuracy: 0.5208\n",
            "Epoch 5/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.8896 - accuracy: 0.5000\n",
            "Epoch 6/200\n",
            "77/77 [==============================] - 0s 961us/step - loss: 0.8072 - accuracy: 0.5234\n",
            "Epoch 7/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7655 - accuracy: 0.6549\n",
            "Epoch 8/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.7306 - accuracy: 0.6628\n",
            "Epoch 9/200\n",
            "77/77 [==============================] - 0s 994us/step - loss: 0.6943 - accuracy: 0.6706\n",
            "Epoch 10/200\n",
            "77/77 [==============================] - 0s 974us/step - loss: 0.6656 - accuracy: 0.6758\n",
            "Epoch 11/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6390 - accuracy: 0.6797\n",
            "Epoch 12/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6198 - accuracy: 0.6823\n",
            "Epoch 13/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6072 - accuracy: 0.6979\n",
            "Epoch 14/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.6112 - accuracy: 0.6966\n",
            "Epoch 15/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5993 - accuracy: 0.7005\n",
            "Epoch 16/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.6940\n",
            "Epoch 17/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5915 - accuracy: 0.7057\n",
            "Epoch 18/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5854 - accuracy: 0.7122\n",
            "Epoch 19/200\n",
            "77/77 [==============================] - 0s 998us/step - loss: 0.5873 - accuracy: 0.6992\n",
            "Epoch 20/200\n",
            "77/77 [==============================] - 0s 995us/step - loss: 0.5820 - accuracy: 0.7070\n",
            "Epoch 21/200\n",
            "77/77 [==============================] - 0s 926us/step - loss: 0.5838 - accuracy: 0.6810\n",
            "Epoch 22/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5759 - accuracy: 0.7109\n",
            "Epoch 23/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5752 - accuracy: 0.7148\n",
            "Epoch 24/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5767 - accuracy: 0.7070\n",
            "Epoch 25/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5737 - accuracy: 0.7122\n",
            "Epoch 26/200\n",
            "77/77 [==============================] - 0s 968us/step - loss: 0.5664 - accuracy: 0.7161\n",
            "Epoch 27/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5801 - accuracy: 0.7057\n",
            "Epoch 28/200\n",
            "77/77 [==============================] - 0s 982us/step - loss: 0.5755 - accuracy: 0.7174\n",
            "Epoch 29/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5654 - accuracy: 0.7161\n",
            "Epoch 30/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.7188\n",
            "Epoch 31/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7122\n",
            "Epoch 32/200\n",
            "77/77 [==============================] - 0s 971us/step - loss: 0.5683 - accuracy: 0.7109\n",
            "Epoch 33/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5661 - accuracy: 0.7018\n",
            "Epoch 34/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5578 - accuracy: 0.7214\n",
            "Epoch 35/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7305\n",
            "Epoch 36/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5657 - accuracy: 0.7122\n",
            "Epoch 37/200\n",
            "77/77 [==============================] - 0s 975us/step - loss: 0.5529 - accuracy: 0.7070\n",
            "Epoch 38/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5537 - accuracy: 0.7227\n",
            "Epoch 39/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5552 - accuracy: 0.7174\n",
            "Epoch 40/200\n",
            "77/77 [==============================] - 0s 999us/step - loss: 0.5539 - accuracy: 0.7292\n",
            "Epoch 41/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5523 - accuracy: 0.7253\n",
            "Epoch 42/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5533 - accuracy: 0.7227\n",
            "Epoch 43/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7201\n",
            "Epoch 44/200\n",
            "77/77 [==============================] - 0s 979us/step - loss: 0.5433 - accuracy: 0.7240\n",
            "Epoch 45/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5648 - accuracy: 0.7122\n",
            "Epoch 46/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7161\n",
            "Epoch 47/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.7266\n",
            "Epoch 48/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5413 - accuracy: 0.7318\n",
            "Epoch 49/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5410 - accuracy: 0.7253\n",
            "Epoch 50/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5396 - accuracy: 0.7318\n",
            "Epoch 51/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7279\n",
            "Epoch 52/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5345 - accuracy: 0.7383\n",
            "Epoch 53/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5354 - accuracy: 0.7240\n",
            "Epoch 54/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5387 - accuracy: 0.7188\n",
            "Epoch 55/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7253\n",
            "Epoch 56/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7174\n",
            "Epoch 57/200\n",
            "77/77 [==============================] - 0s 942us/step - loss: 0.5511 - accuracy: 0.7240\n",
            "Epoch 58/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5298 - accuracy: 0.7331\n",
            "Epoch 59/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5324 - accuracy: 0.7344\n",
            "Epoch 60/200\n",
            "77/77 [==============================] - 0s 975us/step - loss: 0.5280 - accuracy: 0.7292\n",
            "Epoch 61/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5304 - accuracy: 0.7292\n",
            "Epoch 62/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5373 - accuracy: 0.7214\n",
            "Epoch 63/200\n",
            "77/77 [==============================] - 0s 998us/step - loss: 0.5283 - accuracy: 0.7357\n",
            "Epoch 64/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7396\n",
            "Epoch 65/200\n",
            "77/77 [==============================] - 0s 969us/step - loss: 0.5239 - accuracy: 0.7383\n",
            "Epoch 66/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7344\n",
            "Epoch 67/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5193 - accuracy: 0.7396\n",
            "Epoch 68/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7357\n",
            "Epoch 69/200\n",
            "77/77 [==============================] - 0s 950us/step - loss: 0.5224 - accuracy: 0.7370\n",
            "Epoch 70/200\n",
            "77/77 [==============================] - 0s 990us/step - loss: 0.5183 - accuracy: 0.7383\n",
            "Epoch 71/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5164 - accuracy: 0.7344\n",
            "Epoch 72/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.7357\n",
            "Epoch 73/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7292\n",
            "Epoch 74/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7396\n",
            "Epoch 75/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5240 - accuracy: 0.7344\n",
            "Epoch 76/200\n",
            "77/77 [==============================] - 0s 999us/step - loss: 0.5209 - accuracy: 0.7370\n",
            "Epoch 77/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7331\n",
            "Epoch 78/200\n",
            "77/77 [==============================] - 0s 977us/step - loss: 0.5169 - accuracy: 0.7370\n",
            "Epoch 79/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7318\n",
            "Epoch 80/200\n",
            "77/77 [==============================] - 0s 973us/step - loss: 0.5092 - accuracy: 0.7435\n",
            "Epoch 81/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5121 - accuracy: 0.7422\n",
            "Epoch 82/200\n",
            "77/77 [==============================] - 0s 958us/step - loss: 0.5087 - accuracy: 0.7461\n",
            "Epoch 83/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5171 - accuracy: 0.7344\n",
            "Epoch 84/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7305\n",
            "Epoch 85/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7461\n",
            "Epoch 86/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5046 - accuracy: 0.7513\n",
            "Epoch 87/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7500\n",
            "Epoch 88/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.7500\n",
            "Epoch 89/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7474\n",
            "Epoch 90/200\n",
            "77/77 [==============================] - 0s 983us/step - loss: 0.5111 - accuracy: 0.7357\n",
            "Epoch 91/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7305\n",
            "Epoch 92/200\n",
            "77/77 [==============================] - 0s 988us/step - loss: 0.5144 - accuracy: 0.7513\n",
            "Epoch 93/200\n",
            "77/77 [==============================] - 0s 956us/step - loss: 0.5008 - accuracy: 0.7513\n",
            "Epoch 94/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.7461\n",
            "Epoch 95/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7448\n",
            "Epoch 96/200\n",
            "77/77 [==============================] - 0s 993us/step - loss: 0.4999 - accuracy: 0.7318\n",
            "Epoch 97/200\n",
            "77/77 [==============================] - 0s 997us/step - loss: 0.4978 - accuracy: 0.7552\n",
            "Epoch 98/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.7344\n",
            "Epoch 99/200\n",
            "77/77 [==============================] - 0s 949us/step - loss: 0.5071 - accuracy: 0.7500\n",
            "Epoch 100/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5004 - accuracy: 0.7513\n",
            "Epoch 101/200\n",
            "77/77 [==============================] - 0s 981us/step - loss: 0.5015 - accuracy: 0.7448\n",
            "Epoch 102/200\n",
            "77/77 [==============================] - 0s 927us/step - loss: 0.4960 - accuracy: 0.7526\n",
            "Epoch 103/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5054 - accuracy: 0.7409\n",
            "Epoch 104/200\n",
            "77/77 [==============================] - 0s 985us/step - loss: 0.4959 - accuracy: 0.7552\n",
            "Epoch 105/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7487\n",
            "Epoch 106/200\n",
            "77/77 [==============================] - 0s 964us/step - loss: 0.5006 - accuracy: 0.7526\n",
            "Epoch 107/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5023 - accuracy: 0.7539\n",
            "Epoch 108/200\n",
            "77/77 [==============================] - 0s 965us/step - loss: 0.5055 - accuracy: 0.7526\n",
            "Epoch 109/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7487\n",
            "Epoch 110/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5006 - accuracy: 0.7578\n",
            "Epoch 111/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4911 - accuracy: 0.7617\n",
            "Epoch 112/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4916 - accuracy: 0.7500\n",
            "Epoch 113/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7539\n",
            "Epoch 114/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4937 - accuracy: 0.7578\n",
            "Epoch 115/200\n",
            "77/77 [==============================] - 0s 994us/step - loss: 0.4932 - accuracy: 0.7565\n",
            "Epoch 116/200\n",
            "77/77 [==============================] - 0s 998us/step - loss: 0.4913 - accuracy: 0.7474\n",
            "Epoch 117/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.7539\n",
            "Epoch 118/200\n",
            "77/77 [==============================] - 0s 950us/step - loss: 0.4920 - accuracy: 0.7474\n",
            "Epoch 119/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.7552\n",
            "Epoch 120/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.7461\n",
            "Epoch 121/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4847 - accuracy: 0.7526\n",
            "Epoch 122/200\n",
            "77/77 [==============================] - 0s 965us/step - loss: 0.4915 - accuracy: 0.7461\n",
            "Epoch 123/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7461\n",
            "Epoch 124/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7487\n",
            "Epoch 125/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7539\n",
            "Epoch 126/200\n",
            "77/77 [==============================] - 0s 971us/step - loss: 0.4814 - accuracy: 0.7630\n",
            "Epoch 127/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7578\n",
            "Epoch 128/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7591\n",
            "Epoch 129/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7331\n",
            "Epoch 130/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4868 - accuracy: 0.7565\n",
            "Epoch 131/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4831 - accuracy: 0.7695\n",
            "Epoch 132/200\n",
            "77/77 [==============================] - 0s 985us/step - loss: 0.4854 - accuracy: 0.7552\n",
            "Epoch 133/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4851 - accuracy: 0.7643\n",
            "Epoch 134/200\n",
            "77/77 [==============================] - 0s 993us/step - loss: 0.4904 - accuracy: 0.7513\n",
            "Epoch 135/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4940 - accuracy: 0.7500\n",
            "Epoch 136/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4827 - accuracy: 0.7435\n",
            "Epoch 137/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.7539\n",
            "Epoch 138/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4964 - accuracy: 0.7539\n",
            "Epoch 139/200\n",
            "77/77 [==============================] - 0s 982us/step - loss: 0.4841 - accuracy: 0.7578\n",
            "Epoch 140/200\n",
            "77/77 [==============================] - 0s 979us/step - loss: 0.4849 - accuracy: 0.7695\n",
            "Epoch 141/200\n",
            "77/77 [==============================] - 0s 982us/step - loss: 0.4792 - accuracy: 0.7539\n",
            "Epoch 142/200\n",
            "77/77 [==============================] - 0s 973us/step - loss: 0.4827 - accuracy: 0.7591\n",
            "Epoch 143/200\n",
            "77/77 [==============================] - 0s 935us/step - loss: 0.4916 - accuracy: 0.7604\n",
            "Epoch 144/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.7578\n",
            "Epoch 145/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7526\n",
            "Epoch 146/200\n",
            "77/77 [==============================] - 0s 985us/step - loss: 0.4922 - accuracy: 0.7604\n",
            "Epoch 147/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7513\n",
            "Epoch 148/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7513\n",
            "Epoch 149/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7578\n",
            "Epoch 150/200\n",
            "77/77 [==============================] - 0s 963us/step - loss: 0.4795 - accuracy: 0.7591\n",
            "Epoch 151/200\n",
            "77/77 [==============================] - 0s 988us/step - loss: 0.4826 - accuracy: 0.7630\n",
            "Epoch 152/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.7513\n",
            "Epoch 153/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7604\n",
            "Epoch 154/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7656\n",
            "Epoch 155/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.7591\n",
            "Epoch 156/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.7734\n",
            "Epoch 157/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.7539\n",
            "Epoch 158/200\n",
            "77/77 [==============================] - 0s 983us/step - loss: 0.4739 - accuracy: 0.7695\n",
            "Epoch 159/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7656\n",
            "Epoch 160/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.7513\n",
            "Epoch 161/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4800 - accuracy: 0.7630\n",
            "Epoch 162/200\n",
            "77/77 [==============================] - 0s 963us/step - loss: 0.4807 - accuracy: 0.7565\n",
            "Epoch 163/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7643\n",
            "Epoch 164/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.7643\n",
            "Epoch 165/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4844 - accuracy: 0.7526\n",
            "Epoch 166/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7734\n",
            "Epoch 167/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4676 - accuracy: 0.7695\n",
            "Epoch 168/200\n",
            "77/77 [==============================] - 0s 991us/step - loss: 0.4768 - accuracy: 0.7643\n",
            "Epoch 169/200\n",
            "77/77 [==============================] - 0s 994us/step - loss: 0.4706 - accuracy: 0.7643\n",
            "Epoch 170/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7656\n",
            "Epoch 171/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7630\n",
            "Epoch 172/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4719 - accuracy: 0.7656\n",
            "Epoch 173/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.7734\n",
            "Epoch 174/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.7630\n",
            "Epoch 175/200\n",
            "77/77 [==============================] - 0s 1000us/step - loss: 0.4746 - accuracy: 0.7786\n",
            "Epoch 176/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4698 - accuracy: 0.7591\n",
            "Epoch 177/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7695\n",
            "Epoch 178/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4700 - accuracy: 0.7643\n",
            "Epoch 179/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7695\n",
            "Epoch 180/200\n",
            "77/77 [==============================] - 0s 996us/step - loss: 0.4655 - accuracy: 0.7552\n",
            "Epoch 181/200\n",
            "77/77 [==============================] - 0s 973us/step - loss: 0.4737 - accuracy: 0.7708\n",
            "Epoch 182/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7682\n",
            "Epoch 183/200\n",
            "77/77 [==============================] - 0s 986us/step - loss: 0.4652 - accuracy: 0.7539\n",
            "Epoch 184/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7734\n",
            "Epoch 185/200\n",
            "77/77 [==============================] - 0s 949us/step - loss: 0.4666 - accuracy: 0.7682\n",
            "Epoch 186/200\n",
            "77/77 [==============================] - 0s 989us/step - loss: 0.4669 - accuracy: 0.7721\n",
            "Epoch 187/200\n",
            "77/77 [==============================] - 0s 990us/step - loss: 0.4782 - accuracy: 0.7643\n",
            "Epoch 188/200\n",
            "77/77 [==============================] - 0s 945us/step - loss: 0.4758 - accuracy: 0.7721\n",
            "Epoch 189/200\n",
            "77/77 [==============================] - 0s 993us/step - loss: 0.4657 - accuracy: 0.7760\n",
            "Epoch 190/200\n",
            "77/77 [==============================] - 0s 959us/step - loss: 0.4687 - accuracy: 0.7760\n",
            "Epoch 191/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.7669\n",
            "Epoch 192/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4675 - accuracy: 0.7565\n",
            "Epoch 193/200\n",
            "77/77 [==============================] - 0s 959us/step - loss: 0.4743 - accuracy: 0.7799\n",
            "Epoch 194/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4765 - accuracy: 0.7695\n",
            "Epoch 195/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7682\n",
            "Epoch 196/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4741 - accuracy: 0.7669\n",
            "Epoch 197/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7747\n",
            "Epoch 198/200\n",
            "77/77 [==============================] - 0s 977us/step - loss: 0.4668 - accuracy: 0.7578\n",
            "Epoch 199/200\n",
            "77/77 [==============================] - 0s 998us/step - loss: 0.4679 - accuracy: 0.7760\n",
            "Epoch 200/200\n",
            "77/77 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.7721\n",
            "24/24 [==============================] - 0s 924us/step - loss: 0.4587 - accuracy: 0.7708\n",
            "\n",
            " Accuracy: 0.7708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8wQYVZV9Qky",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "outputId": "46bf44e2-1019-44ea-9617-8cec45008bb8"
      },
      "source": [
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/iris.csv', names = [\"sepal_length\", \"sepal_width\",\n",
        "                                                                                                            \"petal_lenght\", \"petal_width\", \"species\"])\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 5)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_lenght</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_length  sepal_width  petal_lenght  petal_width      species\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNLdrXcaCOXf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "outputId": "b218b14a-efff-4e6f-a82f-8108adf0c9ee"
      },
      "source": [
        "sns.pairplot(df, hue = \"species\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.PairGrid at 0x7fc4607c9160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAALbCAYAAADdHJ4ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOyde3wU1d3/32f2kmwCZHPhFgN4Qy1FRcVLi61G26JitX1srYoF1NZSban49EGrrdY+1ra0jyi1Fm1V0OKl1lq0CLRq9OcVi5aiohAVhRAuCbmQy4bNZs7vj8lu9jKb3c1mN5vk+3698gozc2b2ZPmeM3PmfD/no7TWCIIgCIIgCIIgDDWMga6AIAiCIAiCIAhCJpDBjiAIgiAIgiAIQxIZ7AiCIAiCIAiCMCSRwY4gCIIgCIIgCEMSGewIgiAIgiAIgjAkkcGOIAiCIAiCIAhDkowPdpRSC5VS7yql3lFKPaKUyo86nqeUekwp9YFSar1S6uBM10kQBEEQBEEQhKFPRgc7SqmDgAXAdK31VMABXBRV7AqgUWt9OLAE+FWi65511lkakB/5ycRPRpCYlZ8M/WQEiVf5yeBPRpCYlZ8M/giDnGyksTkBj1LKCRQAtVHHzwdWdP/7L8CZSinV2wXr6+v7vZKCkEkkZoXBhMSrMNiQmBUEIR4ZHexorXcCvwG2A7uAZq31P6KKHQTs6C4fAJqB0kzWSxAEQRAEQRCEoU+m09iKsWZuDgHKgUKl1KV9vNaVSqkNSqkNdXV1/VlNQcgIErPCYELiVRhsSMwKgpAMmU5j+wKwTWtdp7XuBP4KfDaqzE5gAkB3qlsRsC/6Qlrre7XW07XW00ePHp3hagtC+kjM5gamNqn31VPbWku9rx5TmwNdpZxE4nXoMlTbgMRsbjJU400YvDgzfP3twClKqQLAB5wJbIgq8xQwF3gN+BrwvNZaBGGCIKSNqU2qG6tZ8PwCattqKS8sZ+kZS5lcPBlDycr7wtBH2oCQTSTehFwk05qd9ViLDrwFvN39efcqpX6mlDqvu9h9QKlS6gPgWuD6TNYp03R0drG/o3OgqyEIAtDQ0RC66QLUttWy4PkFNHQ0DHDNBCE7SBsQsonEm5CLZHpmB631zcDNUbtvCjveAXw90/XIBnv3d3Dub1/G1JpnfvA5xozMT3ySIAgZw9/lD910g9S21eLv8g9QjQQhu0gbELKJxJuQi8icYj+y/NWP2dtygPpWPyte/XigqyMIwx63w015YXnEvvLCctwO9wDVSBCyi7QBIZtIvAm5iAx2+pF17+7mmIoipowfxT/e3TPQ1RGEYU9JfglLz1gauvkG88dL8ksGuGaCkB2kDQjZROJNyEUynsY2XNjd3MGHdW3MPnkiXabm0X/toLHNT3GhvM0QhIHCUAaTiyezctZK/F1+3A43JfklIpQVhg3SBoRsIvEm5CIy2OknNtU0AXDUuJH4Oq1lFt+pbeZzk2U5TEEYSAxlUOYpG+hqCMKAIW1AyCYSb0KuIYOdfmLrnhYAKooLCJjWytnv7Nwvgx1h0GNqk4aOhrhv6dI9PtD1F4ShQqqxHjAD1Pvq6ezqxOVwUZpfSrO/WdqKkBbJxqFduaBHTzAmyzxlOA15VBXSQyKon3h/dwtjRuaR73IAUFLo5sO61gGulSCkRyLPhHSPD3T9BWGokGqsB8wAWxu3srBqYaj8ksolLNu4jKqaKmkrQp9INg7tyt038z72+/fHxOQRxUfIgEdIC+nB+oktu1uoKC4IbY8blc9HMtgRBjmJPBPSPT7Q9ReEoUKqsV7vqw89VAbLL6xayPmTz0/qfEGwI9k4tCvn7/LbxmS9rz67f4Qw5JDBTj/gD5hsq29jQokntG98UT4f1bcNYK0EIX0SeSakezzTDPTnC0K2SDXWO7s6bcsXuYuSOl8Q7Eg2Du3KGcqwPbfTFKN2IT1ksNMPbKtvI2BqJoTP7BTl09TeSbNPGqkweEnkmZDu8Uwz0J8vCNki1Vh3OVy25Zv9zUmdLwh2JBuHduVMbdqe6zJcmamsMGyQwU4/EExXK/f2zOyUFuYBsKvZNyB1EoT+IJFnQrrHB7r+gjBUSCXWTW2S78hnSeWSiPJLKpewqnpVwvMFIR7x4tCb56XeV09tay31vnq8ed6Ycm6H2zYmZWU3IV2U1nqg65Ay06dP1xs2bBjoaoT440sfcevq9/jDN6czIt8S0W3d08LNT73L/fOmc8ZRYwe4hkIKqExcNNdiNhXSWW3N1Caf7P+EmpYaPE4PvoCPipEVTBo1KWui5yG+GpvEqxAimVgPF4afPO5k5k2dh8tw4TJclHqyshqbxOwQJzoOvXlePmz6MGbRgsO8h9F0oMl+NTazE5eRM6uxZSRmhewx4BE0FKht6iDfaVCY5wjtKxuRFzomCIOZRJ4JvR1v6Ghg/j/nR+RhlxeWs3LWyqy9rRPPB2G4kEyshwvDn/zwSZ788MlQmwwu9SsI6RAdh/W+ettFC+zuA4YyGFc4Lqv1FYY+GX29qZQ6Uim1Mexnv1LqmqgypyulmsPK3JTJOmWC2qZ2SkfkoVTP4N/rceEwFLVNksYmDF9kgQBByC2kTQrZRmJOGGgyOrOjtd4CTANQSjmAncCTNkVf0lqfm8m6ZJKaJh+lIyLFd4ahKCl0s6tZZnaE4UtQhBo9syOiZ0EYGKRNCtlGYk4YaLKZuH4m8KHW+pMsfmZWqG3qCKWthVNS4JaZHWFYIwsECEJuIW1SyDYSc8JAk03NzkXAI3GOfUYp9R+gFvih1vrd7FUrPTo6u2ho81NaGPuGomyEm08a2gegVoKQGxjKYHLxZFbOWjlUFwgQhEGFtEkh20jMCQNNVgY7Sik3cB7wI5vDbwGTtNatSqlzgL8Bk22ucSVwJcDEiRMzWNvUCKap2c3slI7IY/22BkxTYxiymMdwI1djNtvIAgGDA4nX4cNQaZMSs4OHoRJzwuAkW8Pqs4G3tNZ7og9orfdrrVu7//0M4FJKxbQIrfW9WuvpWuvpo0ePznyNkySYplY2InZmp7TQTcDU1LceyHa1hBwgV2O2vwkuFRr0TzC1OdBVEvrAcInXocxwa4sSs7nBcIs7YfCRrTS2i4mTwqaUGgfs0VprpdRJWAOwfVmqV9rs7B7slNrM7BQXWAOgvS0HGDMqP6v1EoRsEO7ZEe6fMLl4sqQoCEIWkbYoDAQSd8JgIOORqJQqBL4I/DVs33yl1Pzuza8B73RrdpYCF+lB5HRa2+RDASU2mp2iAhcAdTKzIwxRwj07oMc/oaGjYYBrJgjDC2mLwkAgcScMBjI+s6O1bgNKo/YtC/v3XcBdma5HptjZ6MNb4MLliB03ej3WYKe+RQY7wtBE/BMEITeQtigMBBJ3wmBA5hjTZGeTzzaFDWRmRxj6BP0TwikvLMdQhuRvC0IWkbYoZINofY7bsI878dARcgkZ7KTJziaf7bLTAHlOBwVuB3UysyMMUez8E5ZULuG2129j5hMzmb16NtWN1fKQJQgZRtqikGmC+pzZq2eHYqqls0U8dIScJ5s+O0MOrTW7mjo4+qCiuGW8HpcMdoQhS7R/gqEMbnv9NqpqqoCe/O2Vs1bKsqOCkEGkLQqZxk6fM/+f83nk3EfEQ0fIaWSwkwb72vz4u0xKC+3T2ABGyWBHGOKE+yfUttaGHq6CSP62IGQHaYtCJomnz+kIdFA+ojzOWYIw8MhgJw1689gJ4i1wsWe/DHaE4UFQNxB+Q6ysqAzpBuze+pnapKGjoc9vBdM9XxAGG9Ex783z0nSgKTSjY2BgKIPKisqIAY9oKYTeSNSXuh1uKisqOX/y+RS5i2j2N7OqelXWY0r6fCFVZLCTBrW9eOwE8XrcvFu7P1tVEoQBJagbCKY6VFZUMn/afOaumWvrwZCuR4N4PAjDDbuYX1K5hGUbl1FVU0V5YTm3zLiFhzc/zPxplsNDcL9oKYR4JNOXevO8zJ82n4VVCyNiz5vnzal6CkI0EhlpsLOpA+h9ZqeowEVLR4COzq5sVUsQBoxw3cC6C9Zxwyk3hG6MEOvBkK5Hg3g8CMMNu5hfWLWQ8yefH9q++ZWbOX/y+SysWsgNp9zAugvWsXLWSnkgFOKSTF/adKAppj9fWLWQpgNNOVVPQYhGZnbSoLbJR57TYERe/K8x6LVT13KACSUF2aqaIAwY0bqB3jwY0vVoEI8HYbgRL+aL3EUx27VttZjaFD2FkJBk+tJc6G9zoQ7C4ENe8aRBbZOP0hFulFJxy3jFa0cYxsTz/gjmeCc6nu71BWGoES/mm/3NMdvSFoRkSaYvTae/jfbn6esS6NLnC31BBjtpUNPoo6yXldgAijxWA5QV2YThiDfPy5LKJTHeH8EcbztvkFR0BemeLwiDDbuYv/3021lVvSq0fcuMW1hVvUragpA0yfSlfe1v7fx5+ur5JH2+0BeU1nqg65Ay06dP1xs2bBjoanDCrf/kmIO8XPn5Q+OWaWz3c9XKt7j1K1O59JRJWayd0EfiT9OlQa7EbLap99Xzs1d/FrN6z02fvSmU6iarsaWFxOswxNQme9v3sqt1Fw0HGnhx+4ucNvE0xhaMpTS/FIdyYBhGrrYFidkcJZm+tC/9bb2vntmrZ0ekn5UXlvfZ82kA+vyMxKyQPTKq2VFKHQk8FrbrUOAmrfUdYWUUcCdwDtAOzNNav5XJevUHHZ1d7Gv197o4AcCofBcKmdkRhif+Lj9VNVUxfh/Xd10f+ne4xqcvpHu+IAw2gisZzlk7J7TvyQ+fBGDdBesYUzhmoKomDGKS6Uv70t/2t85G+nwhVTI6FNZab9FaT9NaTwNOwBrMPBlV7GxgcvfPlcDvM1mn/mJ3s7USW2/LTgM4DGUZi4pmRxiGJJVfbZrQugeadli/zb7lcgtCTpKh+BbtgpA0A9zHSqwKA00257fPBD7UWn8Stf984EFt8TrgVUqNz2K9+kQyhqJBvAUu9oqxqDAMSZhfbZqwdzP88Qtwx1Tr997NMuARhgYZjG/RLghJkQN9rMSqMNBkc+npi4BHbPYfBOwI267p3rcrG5XqKztDg53eZ3YAivJd1MvMjjAMCffdsc2vbq+DRy+Gpu3WdtN2a/tbz8KIsQNXcUHoDzIY3wnbliBATvSxEqvCQJOVwY5Syg2cB/wojWtciZXmxsSJE/upZn2ntttQtKQw8cxOUYGLD/e2ZrpKQo6RazE7UPSaXx3w99yEgzRtt/YLWUXiNQNkOL6Hu3ZBYjYJcqSPHe6xKgws2RpWnw28pbXeY3NsJzAhbLuie18EWut7tdbTtdbTR48enaFqJk9tkw9vgQuXI/FX6O3W7AzGle+EvpNrMZuTON3gjXpI8U609gtZReI1A0h8ZxSJ2SSQGBSErKWxXYx9ChvAU8D3lFKPAicDzVrrnE5hA6ht9lGWxKwOgLfATWeXZr8vQFG3yaggDFcCgU7qO+rpNAO4DCelVzxH89638ecV4j7QRknRJIwCeXARhgAFo+GiR3rSiI6cBTN/br1Vb92D6Smlwd8Uk9oTb2ndYb7MutAXomPQO9Ha9pRaixUE/NbAp2A0GJmLpWRjN2AGqPfV09nVicvhosxThtPIpuJCGIpkPIKUUoXAF4HvhO2bD6C1XgY8g7Xs9AdYq7Vdluk69Qc1jT7GjEys1wFrgQKAvS0dMtgRhjWBQCdbm6tZWLWQ2rbakMnomn0bWf7e8pBwdbISx2NhCGAYMGaKpY8wTWirgwfPg6btmEedS/UXbmTBCz1tYekZSznMexgfNn3IgucXJLV/cvFkGfAI8QmPweDAxlMKde/HDoDGTMnIgCdoKpoodgNmgK2NW2PuD0cUHyEDHiEtMt5Daq3btNalWuvmsH3Lugc6dK/CdrXW+jCt9dFa65x3BdNas6vJl9TiBGClsYF47QhCfUd96EYGltfCwqqFfOWIr4S2Fzy/gIaOhoGspiD0H4ZhCcENAx6bHdJPNBw/OzTQgZ7Yr/fVhx4Kk9kvbUVISDAGvROs37599osWtNdl5OMbOhqSit16n/39od5Xn5F6CcMHeR3UBxrbO+kImEktOw1QVGCVE68dYbjTaQZszeUcyhGx3VezOUHIWaKE4v6CEtu20Gl2prRf2oqQMlletCBZU9HOrvixLwjpIPOCfSDosZPIUDRIcYHM7AhDGNO03gjGyf0Oz9V2Gk7KC8sjbmiVFZU4DScPzHyAZn8zq6pXidmcMPRweWD24+AqAF8j7oA/pi2UF5bjMly2+53Kvu0YyqC2tVY0PEJ8ovtol8dKXQsf8GRw0YKgqWh0TEf38y6Hi8qKSs6ffD5F7qLQ/cBlSPq/kB7SK/aBVDx2ADwuB26HwV4Z7AhDjQSGdcFc7dmrZzPziZk8vPlhllQuCZnLVVZUMn/afK5YdwWXrbuMxW8sZv60+XjzvAP5VwlC/2Ka0LIbVv83LJ8F626gxDWCpWFtIahjKPOUxRgw3jLjlrhtZ+6aucx8YiazV8+murEaU4shrxCGXR/dshsufbJnlbagZidDC8N487wRsRvU4kT386X5pcyfNp/FbyyOuB+U5pdmpF7C8CHpmR2l1BHA/wCTws/TWp+RgXrlND0zO8m9BVFK4S1wycyOMPRIYFgXnau9/L3l1u+zlhMwAzgNJ/PWzovJ0V45a6V4MghDB5t2Yjw+l8nfrrI1WpxcPJkVZ69gV+suGg408Nu3fsum+k180vIJK85egalNDGUwd83cGB2EtB0hgnh99BXPRi5akMHV2JoONLFs4zIWnbQoNGOzbOMybvrsTRGx2uxvttXsSEwL6ZJKGtvjwDLgD0BXZqozONjZ6MPtNBiZl/zXV+SRwY4wBEmQ+22Xq738veVcPOViJoyaQG1rregQhKFPnHZidPoo806IKR5cZnrO2jkR+6tqqrheX0/5iHJpO0JyxOuju/zWggVZwN/lp6qmiqqaqoj913ddH1NOYlrIBKkMdgJa699nrCaDiOCy00qppM/xFrjYs78jg7UShP4hJS+PoGFdnNxvt8Mdk4O9cfdGDKB2/3aMOBoehWLH/h3isyAMHuJp10wTlILL11lLT79yh1X+tOtAd8X12wnXORxTdgyXH305k0ZOQqGoaanBoRxUVlRGPEDa6SCEYU68PtrlSeyzYxPTpiJlrye7+4CdNjNeuWhdGqReB2F4k/AJQilV0v3Pp5VSVwFPAqEpCq31sFv3ckdje9J6nSBFHjfVe1szVCNB6B+S9UMIEc+wrjv32+sqYv60+TG+Cbet/yVVNVVUVlSypHJJ6HhQhxBMbROfBWFQENRFRLeD0UfF+pl8ZZn1oPn43IR+O0vPWMrv/v07LplyCQ9vfphLplzC1c9dHdE2wJrxCZ4XfBgUBMDy1LnwIfjzN3ti8JLHoWUXPHpJfJ8dm5g2L32SaqMrZa8nb57X9j4QrdmJV27luytDPmzLvrgMf5df/KaElFBa694LKLUN0IDdNIbWWh+aiYr1xvTp0/WGDQNnx3PMT9dx8qGlXD7jkKTPeeKtGv7yZg1bbz0bt1MaZA6T/HRdCgx0zCZLva+e2atnx6ya02vOdC+rsdW37mJ2mCYneL1FJy3imqprAGsm54aTr8cEFCpCwxMsv+LsFYwrHNf/f/DgZ1jHa87QuscSfke/Pb9sDTxwduz+Wf8HK78OQP0ljzB70522ba4kv4S97XuZu2Yui05axOI3FseUu2/mfdS11zF+xHjGFIwZDA98ErPZpHUPPL0Qpl0MnmLwNVq///bd2Ljs1lqGzouK6fpv/pXZ/46NwUSammTvK/HKhd8v7j7zbm59/daU65AmGYlZIXsk7BW11od0D2g+1f3v0A8wJfNVzC2afZ3s7wgwZmRqMztBY9F9baLbEXKXPuVMRxvWhaVC+OP46hS5i0LbVTVVmGaA8hHlBOKUF58FIaeJq4votN/vKghtxvPb8Xf5Q9qdYJuxKxcwA8xZOye0aIEgRBDww5bV8Nil1kqAj11q7U/ks2MT0/68wj5papK9r8QrF36/8Dg9ousRUiaVnvHVJPcNaWoa2wEYnepgp9tYdO9+GewIuUtQJxBOyjoA07TeCjbtwK0cttdr9jdHXl9ZKWouh8u2vPgsCDmN0w1HzoJv/AnmrbZ+HzkLDGfP8r5BvBOhsz206W5v6LXNBdtks7/ZtlyX7hKtjtBDWP9L6x5w2MQm2MdluM9OUOsThvtAW5/uD8neV+KVC79f+AK+9O9RwrAj4WBHKTVOKXUC4FFKHaeUOr7753SgIMHpQ44dDday06NT1Ox4u41FxWtHyGVK8ktiPD5S0gFEeTqUvPMUS0+/PcZfYVX1qp7rn347JR5L41PmKbP1Y5BlR4WcxlMKpy2CdTeEfHQ4bRG8txrOuyvWz6T40NC+krdWsvT0WL+dYJsLtslV1au4ZcYtEeVuP/12/rb1b6LVESzsPHUCB2Jjs6AMLnq4d5+doB4zrExJ0aQ+3R+Sva/YlYu+X1SMrEjvHiUMS5LR7MwF5gHTgfCE2BZgudb6rxmrXRwGMjf3jy99xK2r3+MP35zOiPzkBdMNbX6ufvgt/vcrU/nmKZMyWEMhTYZ9PnlKq7FFE53n/Y0/YW5/g4YT5+A3nLjNAN6at2k65BT8OoBbOSnxjMZw9szcBMwA9b56Os1OXIasxpaAYR+vOUE8zc7M26zV12ZcA4WjoagCRna/lQ7Tudmtxhbe5oJt0jRNunQXXboLh3LgNtxopQfbalQSs5nCLg5nP26Z2UbH5rerrNUAs7AaGyR/X4ku583z0nQgsm1A1ldjE83OICfhE4TWegWwQil1gdb6iVQ/QCnlBf4ITMVa6OByrfVrYcdPB1YB27p3/VVr/bNUPydb1DT6KHA7KMxzpHSe1+PCYSh2dRuSCkKuYiij7zMp0XnenmKM15ZS9trSiGJl17wTm0bRjdNwymIEwuAinmbHUww1G3p0Ete80/NAGRSCY6VY9Nbm0mqTwvDBLg5dBfax2elL7LMT1GOG76L3WI17qSRj2K6c3XnSHoRUSOV16SSl1LVR+5qBN7XWG3s5705grdb6a0opN/apby9prc9NoS4Dxo6Gdkan6LEDYBiKkgIXu5vFa0cYYoS//VMq0tPB10jnWYup//QsAtrEqQzK3l2NKyw3PK2ZJEHIBeJ5mfgaI7edkbqC8BkbExNTmxFtIF7bkDYj2GIXh53t9rHpcCf22bEhmdgLzc53dfbqlSZxLGSLVKJqOjAfOKj75zvAWcAflFKL7E5QShUBnwfuA9Ba+7XWTWnVeIDZ3tCesl4nSElhHrXNMrMjDCGic8SfWWR5OnTP2nR2dlI9+XPMW3cF5zw5i3nrrqB68ufozC+2Tu/29Zm9ejYzn5jJ7NWzqW6sxtTmQP5VgpAaNvoGLnwINj4Sue0pDZ0SjP2fvfozPtr/EXPXzI1oAwEzYNs24u2XNiPYxmHxobH7LnoEDrREanv2brb6815Ipr8OmAG2Nm5l7pq5nPPkOcxdM5etjVsJmIGUryUI/UVCzU6ooFL/DzhHa93avT0CWI014HlTax2zDLVSahpwL7AZOBZ4E/iB1rotrMzpwBNADVAL/FBr/W5vdRmo3FytNVNuWsfpR45mzmcOTvn8pc9Xs7PRx/9bVNn/lRP6C8knTwW7HPEjZ8E5i0Frdjmdtr45y89azvgR4/vm6yOEI/GaK0TPcL5+D0w8qcfbZOMj8OUlobSgYOzH889ZcfYK5q6Zm/T+QdRmJGYziZ3vGUTuUw74Q2XvPjs2JNNf727bHTduw9OTB1nfL5qdQU4qMztjgPClxDqBsVprX9T+cJzA8cDvtdbHAW3A9VFl3gImaa2PBX4L/M3uQkqpK5VSG5RSG+rq6lKodv/R0ObH19mVssdOkNJCN7v3d5DsAFMY3ORCzGYcuxzxLatBa/BOiOubE3zL1ydfHyEjDIt4zSThflNaw2tLI71NtqyO8DEJxn48/5xOszOl/cOxzUjM2mDnexa9r9OX2GfHhmT6686u+HGb6rUEob9IRbOzElivlFrVvf1l4GGlVCHWzI0dNUCN1np99/ZfiBrsaK33h/37GaXU3UqpMq11fVS5e7FmiZg+ffqAjBY+aQh67OT36fzSQjf+gElDm5/SPqbCCYOHXIjZTBCRZ+10UnTWYvYdcSadDgeuri7KtjyLUylo2oHT6aS8sDzm7Z3TcFLbWouhDOZ9ah7Txk2jyF1Es7+ZVdWrxDNhABiq8Zpxot+ke0pjtWsQo9mJ9s+JaSPKSWVFJedPPp/xheMZ5R6FQoXazPL3lkeUH45tRmLWBruZnWgtTjyNmbP3GHI73KGYjOivDTf1vnr8XX6chn2f7zJcoTJuh5t8Z77ttfKd+RHlRMcj9AdJD3a01v+rlFoLfLZ713ytdXDOeHacc3YrpXYopY7UWm8BziRqYKSUGgfs0VprpdRJWLNN+1L9Q7LBtjor+668qG+DnZJCa4Czq7lDBjvCoCSYZ73g+QXUttUy71PzOHvy2Sx8/mpq22pDvghHrL8X56t3UnbWYpZULmFh1cKI479Y/wuqaqpC28s2LovY9rqKEldGEAaaoGbt0YutB8egNuftJyx/nae+17M/ysck6Cnyu3//jltm3MLNr9wcaiO3zLiF5z95nvnT5ke0nVtm3MLDmx9m/rT5ACx/b7n4jAg92MXjRY/AmCmRAx5PqRWnf/5mZNyGacrs8OZ5Y2JySeUSOro6uGLdFdS21VJZURnT599ReQe+gI/5/5wf2rfsi8v47rTvck3VNRHlmg80R5RbesZSJhdPlgGPkBZJa3YAlFIOYCxhgySt9fb4Z4R0O38E3MBHwGXAN7rPXaaU+h7wXSAA+IBrtdav9nbNgcrN/fW691n24kcsv+xEnEmsWhLNh3Wt/Phv7/CHOdP54pT4ebHCgCL55L0QnWf9t/P/xlXPXhWbn33G7xi39AQAazW2qecRMAM4DWdooBNeftFJi7im6prQ9sqzllM2YnwW/7JBi8TrQJKqv07UfSPaP2d3224aDjRw/9v3c/nRl9tqeYIan+VnLUejB+Pbb4nZTBEvHqO1OK174OmFMO3iuJqOb5sAACAASURBVJoyO+LpbH58yo+56rmrQvsqKyr50ck/IqADuAwXbsPNxasvjjjv7jPv5tbXb014rRzR8YhmZ5CT9MyOUur7wM3AHqAL6z9fA8f0dl73stTTo3YvCzt+F3BXsvUYSLbVtzF2ZF6fBjpgpbEB7GxsT/6ktn3w6p2w9z0YdzSccjUU9v72RRAyRXSetUM57POzHT0+VK61ixh/1DngnUBta23EQCdYvshdFLHtj1q5RxBykr7464QR7imyY/8O5qydEzoWT8sT3B/QASaMTOCTIgwv4sVjtBYn4Lc0ZFtWR+4/+1e9Xj6ezsbj9ETsq6qp4vqTrw/FZ21rbcx5HqcnqWuJjkfoD1LR7PwAOFJrnZMpZtngo7o2xvUxhQ2gyOMiz2mwvSHJ5aebtsP9Z0HLbktY+MFz8OZyuORxqDihz/UQhjkJcrrNrgANvjr8ZgC34aTEMxrDYXUV0TnbTsNpm3ftNFzUfmsd7vYGSt5aidGdCx7UKUS/zWv2N0dsG4aD2v07cBtOvPllNHU2Sw63kHuEax8qpvfM5OR7re2aDdbqhN0aNtNdSIN5ABOsH60xDAMDI9SWgi8D4ml5ivOLqayoxGW4QvvFr0QArHg8clbsjE20FsfpxvzMAhpOnIPfcOI2A5T860EMl6dX7514/behDO6ovCOu7tLuPF/AZ3stXyDy+aiyohJDGdS21oZiG5B4F1IilcHODiwT0WGJaWo+3tfGGUf1Pf1MKcXYUXlsb0hiZqcrAH+eAx3NcM5voGwyNH4MVT+Hh74CV/wDxnyqz3URhikJcrrNrgDVjVtZ8EJPvvXS05cwufgIDIczJmf7+unX2+ZwP7/9RX654ZfW+V9YwmRPKQY9OoWg5idcswOEtm9b/0uqaqqorKiMub7kcAs5Q9DXpOo2OPk7kRqd8+6C6mfh6AvggbMxD/k81TO+y+/+cy+XTLkkRqMTrsWpqqliVfWqGO3DLTNu4c4372T+tPmU5FkPfdE6OmkjwxhPKZy2KKEWx8wvofqEi1jw/PfC+vnbmexvx1gxK67ep8hdFBOTd3/hbvxdfn700o8i+nRvnjf0eXb9fsXIiph9S89YGjEwCvb/waWsw8uIrkdIhVR8du4DjsTy1gktNa21vj0zVYvPQOTm7mzyMeOXz/OtUw/hzE/1fcDzf//YQrOvk39ee1rvBd/4AzzzQzjtOjj4cz37W/da+wvL4MoXwV2Q9Gd/3Pwxz25/lqK8Is477DzyHLJIgg1DO588QU53fesuZtv44gQ1NNE523dU3hFXVxChwQnLuY5+C+11FdHUUY/fDGAYjtBAp7fr50AOd64wtON1MGCa0FILD5wd267mPQPLz4Gm7dQv2MDs578X11cnXIsT1DqU5pdS76uP0PJsqt8U4VsyyPxKQGI2cySp2Ynbz5/xO8qWnhD33N1tu7nt9dsiZvJHuUfx45d/nDD+7GYfIXaGJnyfoQxbz54B0PWIZmeQk8rMzvbuH3f3z7Dio7pWAMankcYGMGZUPu/UNqO1Rqk47SdwAF76Pxg7FSadGnlsxBg4dSH88yfw7M1wzq+T+ty129Zyw8s3hNa6X/HuCu770n2MLZSFEoYVCXK6/XF8cfxxfHF60xVEnB+Wcx2uUwgSXIygdv+OCE1PvOtLDreQMxiG5atj167MQGi/33D26qsT3K/REVqcgBmI0PIEywf7cvErEUIkqdmJ288bjoh90ed2dnVSVVMV0Uc/MPOBpOLPrt8Het1np/URXY/QF1JZevoWAKVUgdY6BYX90GBbvbXs9HivJ0HJ3hk7Mo+OTpO6lgOMGRVn4PTOX6FlF5z8XSvXO5ry42DK+fDGvfCp8+CQz8WWCeP9hve58ZUbOXjUwcw/dj47W3dy98a7ufq5q1k5a6XM8Ax1op3de/FXcMfxSHAbPZqd8OPxdAXRGpxkPUCiP7/Z32zv6zAMPUWEHMU0rXZ1+Tpoq7NWYavZYLUrw2lpKLasJl8Z3H3m3ZR5yuK2mWBbCX8LHq3lCZYPanbi6SikjQxDktTsxO3nza7I60V577gcrpj+WKP7Nf7CY99Qhm3sR+t6JN6FRCSd4KiU+oxSajPwfvf2sUqpuzNWsxzjo7o2PC4HXo8rceFeCA5wPulNt/PWChh1kDWoicdxc6yp5TWLLH1PHLTW/Pz1n+NxeLj6uKspzi9matlUrjzmSrY0buH+d+7v658iDAaCGp0/fgHumArPLLJyuL0TreNR/h8lntEsPX0J5YXlACHNTomn+3h37nXw+MbdG1lSGVl+SeUSNu7eGHm+20syRH/+xt0bmT9tPovfWMxl6y5j8RuLmT9tfkQ+uCAMGMH29cDZcP9MWHcDnHGT9cB53l2w5jo4bRHmZxZQ56vn1tdv5caXb+TWGbdGtJlbZtzCqupVLD1jKd48L9WN1cxePZuZT8xk3tp5zJ82n8qKylD5JZVLQm+/o9uk+O4MY4KanXU3wPJZ1u/TFsVodkryy1h6+u1R/fztlDgK494bAErzS2P64/6Mv6D+LBj7c9fMjYn9pWcspWJkhcS7kBKpaHbWA18DntJaH9e97x2t9dQM1s+WgcjNnXP/G+xoaOe2rx6d1nV2Nfm49vH/8H9fP5YLTqiILdD4Mdx5LBw/F47+eu8X++RVeOE2OPvXcPKVtkWqtlexoGoBc6bM4fQJp0ccW/afZWzcu5HV/7WacYXj+vYHDT2GVj65XQ73kbPgnMVW6k2Kq7FB7Ju36BzuVdWruOG4BZjt9T2rsZ17e6/+DeGEf75hOJlrl1ueu3qEbDO04nWwEU8jccnj8NTVoRme+ivWMXvdZaE4PqbsGOYfO59Dig7BaTgxMDAMg5L8Eho6Gmw1OOFanjJPGU7Dvk0OgtWpJGYzRQo+O+Yrd8WuxvbZ71n/O3FWY4unD3vk3EcwtZl2/MW7/oqzV2BqcyBXYxPNziAnFc0OWusdUTqTrnhlhxpbd7dw2JgRaV9n9Mg8DAUf1bfaF9iyxvodrdWxY+JnYPyx1oDn2Isgf1RMkRWbV1DmKeNzB8Wmun3tiK/x5p43uf+d+7nh5BtS+TOEwYJdDveW1Zafgtfeo8NwOHs19AzPvQ765kR751x/5CWU/3Fmz46zfpl0lcM/P17OtuRnCzlBPI1Ee7010One9uuuiDjeVL+Jq567inUXrIt50RRPgxOt5Qknnh5CGGak4LNjvLaUsteWRu4/+cq49wWIH5sdgQ7KR5SnU/Ner29qM+b6Eu9CKqQyFN6hlPosoJVSLqXUD4H3MlSvnKK5vZPd+zuYWJyeXgfA6TAYV5TPB3vjDHbeXw3eSTAqCfd4pawZIF8jrL8n5vCWhi28uedNzphwBo5o4SFWZzGjfAZPbH2Cfb5ha580tAn6gIQTlYedDkG9QDjlheW42xv65fPiXl/ys4VcIF778jVGbAc1EuHEi2OJeaHPJNvf9/G+kOnYlNgXMkUqg535wNXAQcBOYFr39pBny54WACaUJL/Mc28c5PVQvcdmsONrtFLTJpyU/MXKjoAJJ8NrvwVfU8ShJ6qfwGW4OPWg+LNEMw+eid/0s+rDVcl/pjB4CPqA9JKHnQhTm9T76qltraXeV4+pzdAx23zt05dQ8tbK0OeZl/6VejS1+3dQ37oLsxeNWTSiRxByBtO00oSadli/TdO+fV34kCUKr5gOsx+HOasoMWFp5R1UVlRyR+UdPHjWg/xx5h9ttWfxYj5orBjdBoVhTnhcKgdc9HBMf2/mF1PfUkvt/u3Ut9Ri5hf36b5Qkl/Csi8u4+4z7+aBmQ9w95l3s+yLy/qtP5b+XsgUqazGVg/MzmBdcpYtu/cDMLHfBjsFvPVJEwcCXeQ5w2Zcqv8JugsmnJLaBafNhqcXwGu/gzNuBKDT7GTttrUcO/pYRrjjp9+NHzGeI4qP4PEtjzPv0/NyOc9b6AuGYZnCfevZuHnYvZHIsNAwTSYHNCuPW4Q/rxD3gTZKDA/GqQvhlO9iugupppMFa8NM4cJMShNWXxlMLp7MylkrB4seQRiK9GbGG92+PKVw3lLYXwuPzYam7RjeiRx28WNcdex3+MEL1/ZqhhgT84abls4WLv77xWKiKERiF5eXPA5f+b11vLMd01NKdVM1C8Lj7vTbmVx2BEYf7gv+Lj+3vn5rRCz2F9LfC5kiYQQppX6rlFoa7ycblRxo3tvdQmGeg5LC/plKrSj20KU1H9dHrci2ZY21XGTZ5NQuWHIoTJoBr98NbVY62vpd62k80Mgp4xMPnE6rOI2a1hrW71qf2ucKgwPDsMSp3gnW7yQHOmCJQIMDHbDypxc8v4CGju40tdbdGA9/nbKH/ovyP86k7KH/wnjwPHCPgOWzaMgfwYIXFkae/8JCGnx1yVe/W49QPqKcMk+Z3PiE7NNe1/NACdbvRy+29ke3L4fTemnVPdAJlm9q3RUa6IBNWwojPOZRhNziE50nDDPs4vLhr1tZIstnwcqv09DVFhroQLAPvpYGX33K94WE94N+QPp7IRMkE0UbgDd7+ekVpZRXKfUXpdT7Sqn3lFKfiTquugdOHyilNimljk/9z8gsW3a3MKG4IL4JaIoc1K39idDtaA3bXoTx06AvjfvYS8DfannvAM9+8iwep4ejRydePW762OmMcI3gL1v/kvrnCkOahIaFXZ32gthujVjQSDHmfDP5VDZBGHCSFX73Ut6fV9inxTbENFSIS7y49BSHNv2Gwz5+dOp9sMSiMFhJmEeitV6RzIWUUr/VWn/f5tCdwFqt9deUUm4gOhfsbGBy98/JwO+7f+cEWmu27G7hM4eVJi6cJOVFHhSwdU8Ls+heiKBuC7Tvg3F9XNq6eJKl3XnjHvRnv89LNS8xpWRKyHiuN1wOF58t/yzPbX+Oho4GyY8dYiRaSro34hoWoqwc8TDTxBBHzgKHC+atxt1tiBhjCmqksBBkuClqiml4gtAvBAXdccx4Iwhq0qLK52vLVNTj9NDsb+b+t++n3lePgcLcvwvDMGxjW0xDhynJ9Hvx4jJsgQy32RXHKNpFfeuuyPsCQOtu6yWWwwUjxlkzlcFrOdxi8iwMSvrziWFG9A6lVBHweeA+AK21X2vdFFXsfOBBbfE64FVKJbEUWXbY2eSj9UCACcX9o9cBcDutFdne79YCAfDJy9bvsWn4+Ez9GvgaqX79Tvb69iY1qxPk1INOpUt38c+P/9n3zxdyDrMrQHXjVmavncfMJ89h9tp5VDduTXqRgLgLEPz9h5ZJ6fJzLNO6I2dZJxw5y9pe8WVYPgvvu6vtTUHzk1w2NNoU9Y9fsLZNEWgLWcRTGmvGe+FDMWaNdAVgzzuWmeh5d4XKm0edS52nkFtfvzXUDq45/hp+fdqvuW39L6jevw3z79faxraItochyfZ7vS2Q0b1dotw2BqJLaAm0R94Xmqox931gGeQunWb93vNOhGm5N88rJs/CoCRpU9GEF1LqLa318VH7pgH3ApuBY7HS3n6gtW4LK/N34Jda65e7t58DrtNax3UHy6Z52HPv7eGKFRv46Zc/zZHjRvbbde94diu1TT5euu4Ma8fj82DbS/C1B6wlpfvKmuu4X+1nSYHiN6f9Jukbotaan7z6E8YXjmf5Wcv7/vmDnyFleFffuovZdqacZy3v1UsnnAjDQhQlf/8hxvt/7yngnQjzngEzYM30LD8n9Kax/pJHmL3pzr6bgiZrkjd8GVLxmrO07oGnF8K0i60UIV+j9UD55SWRcdhcYz0kNm23VmObcQ0Ujqa+eGKEqShY7eCnn/0pV/7zSqtNHPMDyp65zja2B5lpaCIkZhORSr8XPQPkKQXfvogZIdPssmb3dQC3cmIYLi5ec2lsv3zcIsoe+q/Iz7xsDRRZBujxTD+HgcmzmIoOclIyFe3j9Y8Hvq+1Xq+UuhO4HvhJqhdSSl0JXAkwceLEBKX7j3d27kcBE0rS99gJZ2JJAeu3NdDS0cnIPCd8/DKMm5reQAdg6n/x8uZ7mOQel9KbP6UUJ407iVUfrGJ32+4YozshdQYqZsPxm4G0NTMRhoVNOyB8oAM9N+SSQ6zjYTdof0FJejneqWolhD6TC/GaswT8VqpmeLomWOa84YRr2Go2wGOXAuC/9m3bdhBMM65tq8VfUBI3tsU01J4hG7Op9HvBBTLCido2DIOykT3+NbX7d9j3y3mFsZ/Z1RnaFM2OMFjpz1dDdk/pNUCN1jq4zNdfsAY/4ewEwi17K7r3RaC1vldrPV1rPX306OQ9QtJlU00TBxV7KHD377hwUqnVqWzZ3QL7PoC2Ohg7Ne3rto6byr/z8/h8S7O16EEKnDTuJDSaf3z8j7TrIQxczIYT18wwFc1MOInM6KKOu9sbQt4iD8x8gDu6vUYicrzt/EuS/Tyh38iFeM1Z4sWhUpFx63DZlnMr+3ZoajPku2MUlGEeda7EdgoM2ZjNtBl0vPvCgbbIgt6JVkwHz+vW7PTanwtCDtKfg507o3dorXcDO5RSR3bvOhMrpS2cp4A53auynQI0a6139WO9+ozWmo07mji0rDBx4RSZVGppgN7btd8yEoV+Geysb64moBSVDbsZufPfKZ07rnAck0ZNYs22NWnXQ8gNSjyjWXr6kljNjaePDwaJTEqjjns/ft0+x9tVZJVPlJveD6aogpA28bQRzyyKjNvCsbbanhJPWUw7vP202zGUweI3FjNn7RzmPn811V+4ETNaByQMPzLc75Xkl8XqeCrvoGTkQbExPqIny8PrKuq9PxeEHCWhZkcp9TQQt5DW+rwE508D/gi4gY+Ay4BvdJ+7TFnrOd8FnAW0A5f1pteB7OXm1jS2c+qvqrhsxsF8aUr/pnVprfn2Qxs479iD+IXjHti8Cr6xMu00tp9uXckze//FCzV78I/9FNXn3JbS+Wu2reHxrY/zzH89w4SRExKfMPQYcvnk6azGZn/BBKsEhR2vdzhstQohzVAyuemyGltvDLl4zVnC41Apa6ATntYWjFtPaeyKVr59mH+/lobjZ+MvKMEd8BPwTmTus98R/UM/MeRiNpP9XusezFfuouHEOfgNJ24zQMm/HsT43ELobI+7Glt/aEAHKaLZGeQk88Tzm3Q+QGu9EZgetXtZ2HENXJ3OZ2SKTTXNABw2ekS/X1spxcSSAt7ftR/4F5QdkfZAR2vNSw3vMGXERBoOOZyK99eS3/gJHcWTkr7GieNO5PGtj7Pu43V86+hvpVUfITcwHM7+vRHZ5YjHOe6Plxse1Awlk5ue6PMEIRuEx2HTjlj9TjBuHc6QoDtEwI/x/t8pC9O71X5rnegfhPhkst8L+DFeW0rZa1G+8CdfaZmMxqE/NKCCMBAk47PzYjYqkov8Z0cTLoc1KMkEE0sKeWvrJ2jHFtS0S9K+3gftu9jrb+ac0SdSN/5gxlc/x7j//IWPT//vpK9R5injcO/hPLPtGRnsCBZRbxgD+V7qO/bRaXbhMhyU5pfS3Nliu1JUMDc81uOhu+tJxb8kY3+eZl+bH3+gC7fTQWmhG8OQF3mpMmS/x2D8myboLtAmoOA7L0HTJ/DKHdZiBL3FrU2cuw+0iX9OjpCTsZvszE5XoFdvHFv62O/G689dhovdbbvp7OrE5XBR5inD2VddaBRDbCVCYYBIOmKUUpOVUn9RSm1WSn0U/Mlk5QaajTuaOLi0EJcjMw1rUkkBR3RtRaGh7MjEJyTg5YZ3AZg6chKBvBHsm3AipdXP4mxvTHBmJCeNO4nqxmo+ahrS/71CMkRpagKv/o6tzR8yd+1lnPPkOcxdexnVzR/ys1d/xswnZjJ79WyqG6sxtaW5SagZGmBNjmlqtuxp4at3v8KMX1Xx1btfYcueFkyzf5bkHy4M2e8xGP9PL4T6rday0nccbS2vHlx++oybLH+p3uLWJs5HjTmaJZWRbWNJ5RJGuUZl6Y8TIEdjN1mfnaCvUy/eOLb0sd+17c8r72B/oI25a+Za94Q1c9nauJVAP8z2mNqkurGa2atn295fBCFZkvbZUUq9DNwMLAG+jKW9MbTWN2WuevZkIze3y9Qc/dN1nHp4GZfNOCQjn/FhXSu7nr6V/3H9GS5+FNzppctd/p8l7DrQyM+OsJY7zW/Zy9HP/5KdJ3yT2hPnJn2dpgNN/PcL/813j/0u35323bTqNAiRfPJwojQ1uxe8ydznr455s7fopEVcU3VNaDtcd5BQMzSAmpy6lgN89e5XqGn0hfZVFHt48qoZjB6Zl5U6pElOxOsQ+B7tCcb/zNtg3Q2xb8KD+y9bAyPLe4/bqDjfhckv1v8ixo3+Ryf/iPGif0iZvvaxORm7yfrshPs6hZcL88aJSx/73ej+XCsnl9p49qw4e0XaFhY55OszBKaohzepzDN6tNbPKaWU1voT4KdKqTeBrA92ssGHda20+7syotcJMqG4gPFGNfvyKihNc6DTFujg3/s/4otl00L7OkaOoXHcpxnz7lPsOu4itDO5jtub5+WI4iNY+/Ha4TjYEcKJ0tR0Ohy2OdtF7qKI7XDdQULN0ABqcvyBroiHHICaRh/+QNeA1GewMmS/x2D8e4rttWXB/VonflCMivPA/h1U1VRRVVMVUex/Tvyf/qq9kAQ5GbvJ+uyE+zqFlwvzxolLH/vd6P58RxxdZqeZRB0SIL4+Qn+RymDngFLKAKqVUt/D8sLJ3EhggPnPjiYgM4sTBHE7FMc4PuRddTynpnmt15veJ6C7OHrkwRH79xx2GsWv3E3Z1n9SN+XcpK934rgT+dN7f+KDxg84vPjwNGsnDCYicqSdTkqOOhejW1jt6uqyzdlu9jdHbLvp9h+J4+idzdXUesvHdzsdVBR7Yt7qup2Ofrn+cKG37zH6+yn2uGj0debO99XbG+6gtsHX2KNxqJgOM64B7yTweOEHb1tluwKWViKJN+amNnHG0T/0l9ZBSA672P3SlDEopdjZ2B6KUSB77dzptlIjp11sDaiDKZPRmhqHK7Zc004wnNCwzfo9Yhw4Xfaf0w+4HC77OFZOaltr09LZuB1u0bUJ/UIq0fcDoABYAJwAfBNIPjdqkPGfmiY8LgfjvfkZ+4y81h0Us59XDhyW9rVebthMvuHm8CijsJbSw2jzTmDspie6hbXJccLYEzAwWPfJurTrJgweYnKk186j+gs3WGaHQNmGB211BquqV4W2l55+OyWv3N2Ta77nHUvz0Fvueab+ngT5+KWFbv4wZzoVxR7AekD/w5zpoYebdK8/XIj3PRZ7XBHfz41PbuL9XPq+kvV52vgInHeX9WB5xk1W6to9n4Pls6BxG6y5zorzQGdCrUWwjT28+WFuj/I6WVK5ZKgvO51zRMful6aMYcGZR3DhPa9FxOjH+9qyF7eeUjhtkRVny2dZv09bZO0Pp3BsZLntb8CkUyxN2dJp1u+971pxmSHKPGW294RfrP9F2jqbkvwSlp6xNFIjdMZSSvJL+vVvEIY+SWt2QicoNQprxeiWzFQpMdnQP3z5ty+hNdw4a0rGPqNs2yomv7yQsw/8gkfnTqUor29vibTWfHH9jRyUX8r3Dv5yzPGSmjc57M2VbD37VponnZL0dRf/azEHAgdY9ZVVqDSXxR5E5FQ+ebaJmyN9xl2U7d8DvkYCnZ3UH3pKz2ps7zxNc8kEyz+kvYGSt1ZiHHsRPGZpx0LahvDt6NzzDJFMPn46MzM5kO+fM/Fq9z3ua/NHfD/3fPME/vfvm3NHH5GKz5NpghmwHiDjaXfmPWN/POx64W3sq4d9lblT5+JQDvIceZR5ynA5MvcWPkfImZgNEh67SikuvOe1mBj93/Onctnyf0Xsy1jcJqvZiS531Xp4+Oux5817ptclpdMlYAao99XTaXbiVE5+sf4XEemZ6ehscmQ1tmHzADRUSXq+XCk1HXgAGNm93QxcrrV+M0N1GzA6Ort4b3cL50zNrEh0RN1GOo18tuoK3tvXxSnlfUtf+LB9F3v8TZw1+gTb443l0/BvXs24/zye0mDnxHEn8tDmh6huquaI4iP6VDdhcBE3R7p9n/XmEKvTGHfNO9bNs2EbrF1EzC3slDCtV1DbEL4dnXueIZLJxzcM1ecHlpzM9x8g7L7H6O/H63Hl1veVqs9T047etTtmIOH1wtvYkx8+yZMfPgnAugvWDYeBTk4SHrs7G9ttY7TA7YjZl7G4TVazE13OcNifl2EfHKfhDC1GUNtaG6NDS0dnYyhDZjuFtEnl6fp+4Cqt9UsASqlTsQY/x2SiYgPJe7v2E+jSGdXrAIys/zdtIw+hq93B+w0mp5QnPseO4JLT0XqdINpwsOfQzzHh3acpqKumffTkpK57wpgTWLl5JWu3rZXBzjAhbo50e0NPoSNnWQa4TTvAcGIedW6PM3xwZscXttx5UPMQvp0lH53+0OQM5PUHO9HfT5OvM7e+r3h+I0pZMzmGEanBUcq+fFDTYzh71VqY2sRQBg+e9SANBxq4/+372VS/SXQIOUS8Nt3ujxzYZDRu48Wlw23N5gT1YI6ocmaX/Xl2OrAMrYLpdriprKiMWWVQ4lsYSFKJ7K7gQAdAa/0yMCRtc3sWJyjM2GeorgMUNG6m03s4RW54b1/f3xC91PAuFfmllLhHxi1TN+kUupx5jNv0l6SvOypvFEeVHsW6j9eRarqjMDgpcXttfHFup+StlVaBI2dZOeIPnA13TMVcfy/VX7iB2ZvuZOZz32b2pjstjc/2N6zy3olw4UPWA1/4dpZyrtPV5Az09Qc70d/PE2/uYNmlJ+TO92XnN3LeXfDMIktr0xWI1OC8fo8Vv9HlNz5i7R8xJq7WIqjVmbtmLnPWzmHxG4v5/vHfp7KiUnQIOUS8Nj2ptCB7cRvPB+dAS6Qe7EBLZLl/r4yNzwsfghFR/jnJ+vj0AW+el/nT5rP4jcVctu4yFr+xmPnT5uPN86Z9bUHoK6n4CTzgKAAAIABJREFU7NwBeIBHAA18A+gA/gSgtX4rQ3WMIdP6h2sf20jVlr387pLjM6ZVGVH3Fkev/Rrbj13I9z88ERODpy9IfSapLdDBqa/9D18om8aF4z/Xa9kJb/+NMdteYdPsP9EZ3fnF4cUdL7Ji8woe//LjHFVyVMr1G4TkXD55Vmndg/n3ayNnara9hnHKd6zldZWK8HWov+QRZm+6M77Gp6gC3rgPJp4U+ab7nMWJfSD6iUyvljbAq7HlfLwOitXYWmotz5K2OnjlDqjZ0ONXEu5j8o0/WfF75k3Q5Yf8UYCCTp/1oHnKd+x9T771LPUOh60ebsXZKxhTMGY4ucIPupjN+mpsViUiZ16UA/5QGRtb364C3dWT4vbeajjsdCulzeyy4vLkKyM1O8lqgvpADnnj9Cei2RnkpJLGdmz375uj9h+HNfg5w+4kpdTHQAvQBQS01tOjjp8OrAK2de/6q9b6ZynUq9/ZuKOJQ0ePyKgof2TdvwHwFR3OIaNgzScmAVPjTLHzXN+0xXbJaTv2HPZ5xm57ibFvP0nNZ65M6vrHjz2eh957iHUfrxsug53hTcCP8f7fKeteajpE8GYZpVnwF5T0rvH53gZ4bSm8FvU5M3+eoT8glnQ0Oblw/cGO3feTU9+XYVgD+ftnRu4P+pWEPxB6imHLavjMVSENWwQnXh5Xa+HH3qMqmNom5A7x2nRW4zbaByeeXqzT1zOQadgG666LvdaJl0duJ6sJ6gPijSPkIkkPdrTWlWl8TqXWur6X4y9prZM3gckgzb5OPqpv4+uTihMXToMR9Rvx548mkFfMIaPgQBd83GxyeHFqOcAvN76Lx3AzuSCx4MdfUEJj+bGMfm81tSdciukuSHjOSPdIPlXyKdZ+vJYFxy0YTquyDV2i3xjml0DbHuvBznDCzF+B96CemZjtb/RodKI0C+72ht41PvFyyNMQYmd7JkV8dFIjemUrhwLDMHL7ewvXSAR9dEYdZLWH77wETZ/A1rVQUAaXr4N8b3xtxOXrYmeInG7cDod4huQogYDJ3tYDdHaZuBwGY0bk4XQO8AA0up92eez1YC5Pj47HcCan2YmnCeoHLaV44wi5SNKtWSk1Vil1n1JqTff2FKXUFZmr2sDwzk7LHPHwMZlfnMBXZPnrHDLK2rd5X2r5slprXtz3Np8eOQmnkdwgafdhp+H0tzH6/bVJf86J406kpqWGd/e9m1L9hBwkOlf7lbssH4YHzrZ8GdZcZ/k0BDUHGx+Boy8IaXR4ZlFETnjJWyttND5LejQ+cXPIx/Wx+tn1tREfndSI/r4uvOc1Pqhr48YnN+X29xbUSIT76Pyh0lpGOjjgn/4ta1nf+2fCcz+Drz8YG9drrrOOr7vBus6Rs6zrFowWz5AcJRAweX9PCxfe8xqn/foFLrznNd7f00IgkB0vMFvsNDUtu6z0yXA92Jk3WfuD5dZcF6e/HRN5/XiaoILk0tt7Q+JcyEVSSWNbjrX62o3d21uBx4D7EpyngX8opTRwj9b6Xpsyn1FK/QeoBX6otR6wp+qN3YsTHFqWucGOq30veW21NB5kZf5NHAEOZS1ScN7hyb/xfr+thr3+Zr485uSkz2krnkRL6SGM3fQX9n76y+gk3rBPHzudle+t5KkPn2Jq2dSkP0vIQdrr4NGLe97oHTc70pdh2sXw52/G396y2vp92RrQGsPpZrKnlJWzVvb4ILi9GOfeDmf9smfm6LI11syRw2UNdBx9W2Z9X5ufbz+4IbRSUk2jj28/uCFjfhfZ/rzBjt33dd0Tm/jJuVNy+3szDBgzxdKShWtumrbDU9+DSx6PbCfBdjDvGeu3UtaLgOD+4HmXrYGR5WAYGMDk4smRbWVgPEOEMPa2HmD+n96MiNn5f3qTP3/nM5R7PQNTqeh+umk7PHoJzPq/yH3N22H1f8fG5SWPQ3u9NVB/cXGsRjIY7996tt9XYzOUIXEu5BypRF+Z1vrPgAmgtQ5g6XAScarW+njgbOBqpdTno46/BUzSWh8L/Bb4m91FlFJXKqU2KKU21NXVpVDt1NhU08S4UXmMyO/bw1gyjKjfCEB70eEAuBwwYSS8l+LMzgv73kYBR486OKXzaid/gbzWvZRtWZdU+QJXAdPGTOOZbc/Q2ZU5J+ahRrZiNiUS+TIE/ULibYN1Q9XayhMfMRbD4aTMU0b5iHLKPGUYDqeVa959HKfLutGWHGL97uNAB7LvazOcfHT6I17jfV9Bf52c/t6C2h07LYOdf8mW1ZZ/iXeCdV7wQTP8PK0jHiCDniGhtiIPgGnRHzHb2WXaxmygawBnduJpalxRqeeuAvu4bK+3Zn8eu9TatrtvBzVBwX66HwY6oUtLnAs5RioR2KaUKsWaqUEpdQrQnOgkrfXO7t97gSeBk6KO79dat3b/+xnApZSKWbJDa32v1nq61nr66NHpT7XGY1NNM4dkwV/HVE46whYVOGQkbE5x+ekX973NoQXjGeVMrL0JZ/+Yo2gtOZjyN/+ESlKQOKN8Bs0Hmvl/O/9fSp81nMlWzKZEMFc7SFBTEyToFxJvG1LP7TZNK6e8aYf1O8XlTQMBk9omH5/sawMILf8aJNrvwjQ1dS0H2NnYTl3LgZjUqfDr1Tb5ek1XCXpu9PZ5Q4X+iNd431fQXwfA7w8k/f1nnej2AdZ2dDsJ7g/OjMc7L+jXI2SE/ohZl8OwjVmnI/LxqLOzi52N7Xyyr42dje10dmZw4B4vnjrbI/d1ttuXi/Y1s8vgSLNfFoTBRCqDnWuBp4DDlFKvAA8C3+/tBKVUoVJqZPDfwJeAd6LKjFPdqnel1EndddqXQr36jX2tB9jV3MGhZZnz1wFrZqdj5CR0mGDvkCLY265p8CXX4dT7m3m39ROOGXlI6hVQipqjzsLdVs/o959J6pRPl36aIncRT33wVOqfJ+QO0bna0ZqaoF9IvO1Uc7vT9HOIzqe/5el3+X0vPi2JNDap5ueLj05qFHtcMT46v7rgGJ54cwe/uuAYbnn6XbbUtfHTp97JHX1EOPF8dxJpzxL59ciDZM5SVuCK6VN+f+kJlBX0DBA6O7t4f28r37j3dU779Qt8497XeX9va+YGPPE0NcWHRu4rPjS2nJ2vWbRGMoM+O4KQi6Tis/N1YB0wAbgAOBn4SW/+OkqpQ7Fmc8DSBz2stf65Umo+gNZ6mVLqe8B3sQxKfcC1WutXe6tLpjxLXtiyl3kP/Isfz/oUny4v6vfrA2AGOOmxY2ka/3l2HzU3tPvfdfDj1+Hhcwv47EGJ03z+uusVbq5eyS2TZzPB04c3Wlpz5Ct3k+9rZNMlD6GdifPoH33/UZ7f8TzPf/15ivMzu1rdAJLzHhBp09tqbA4XFI6Fjoae455S8O3rW253mn4OtU0+LrzntYg0ky9NGcPNX/40QMzqaHUtB/jq3a/EuJ8HtSJ216so9vSan5/jq7HlVLzWtRzgxic3ccEJExgzMo/SEXm0dHRS0+hj2Qsf8u8dTVQUe/jJuVP4zkNvAom//6wT3j6UsvxNDCO2nURrz3rz6+kH/5IhRE7FbG2TjxWvfMTXpk/EYSi6TM1fNmxn7oxDQzG5s7Gdb9z7eky/8diVp3BQcWqZFUkT3U8HXzAl2pcoTiGjPjtDlJzp8IW+kUry/E+01o8rpYqBSuA3wO+xBj22aK0/osefJ3z/srB/3wXclUI9MkZwJbZDMjizU9BUjSPgw9et1wlycGhFtq6kBjsvNrxDqWskFfl9NOlSitqjZnLUK3cz5t2n2XPs1xKecupBp/KPT/7B3z74G5dNvaxvnysMPNH+DRBr8Bl9vK83wDT9HOzy6f+xeS8/njWFiaWx7TSRxqYv+fnio5M8/kAX/9i8l39s3gvAY1eewjfufT2iTFDDE749oPqIaOzaR5DejHB78+vpB/8SITN0dpnc89LH3PPSxxH7Z59ycOjfAVPb9xuZXF0wXhwmsy+RYXMGfXYEIRdJJY0tOF87C/iD1no1MKRyOTbVNDO+KJ8Cd+YWJxhZ9y8A2r2TI/YX51k/ySxScMDs5LXG9zhm1CFp+d60lB1O8+jJjP/3oxidvoTlK0ZWcGTxkTy25TG6zBwWGgu5Q7zc8yQ1P8nm0wdJpLFJ9XpCakR//+FanSBBDU/49pD5/tOMdyH7JNMnOA1lXyZ3ZnhTQ+JUGGak8lS/Uyl1D/BF4FdKqTxSGyzlPG/vbObgDOt1Ru35F/78MjrzY1PPDhllLT+diH81bcVn+jm2L3qdKGqPOotPvfRbxrzzFLuP+0bC8pUTK1n2n2X/n707j2+rOhP//zlXm+V93+I4G1mBALGBAKVsnUCHdGFCW1pCp+2UdaZ0Zrow/bWd6UyX+TJ02g7tQFg6LVtbmGQokBYKZSckEDskIWTf4zje7cSLbG3n94csxbIlW5Ila/Hzfr30snV1dXVsPTrS0T3neXjr+FtcNvOyST++yHD+uef+NKpRrvkpz7WxZnVdIDVsTZGdNavrKM8NfaalJMfKo1+6gCOdA2RbTQw4PcwqyQ6ssYn2eCI6/jVO/vTT6xqP8asvnk9TlyPwfNQU27nnhd0Amff/n2S8i6lXnmvj1188n2MjYnRmsT0oJstzbdy/uo7bR/Qb96dz3EqcimkmmsHOp4FrgB9rrXuUUlXANxLTrKnnT05w5aLyiXeOldbkt77DQOFC31zwUebkw7OHvLg8Gosp/DdGr3W+j1WZWZQ7c9JN6iueQ0/5Iiq3PknbmR/Dax1//vGy8mUU2gr57e7fymBHTGyS9RzMZoNFFXk8detFuD1ezBFUNx9ye/nuMzsCH0oe+nz9pI4nImcYioUVeTx9xyU43R7sVhOtJ4eCn4+b6vjBJ8/mO9d6Mu//n8D6JSIxDEPhcutRMVoftC7PYjGxqDyXJ29ZjturMRuK8lwbFkuaZmWUOBXTTMSRrbUe0Fr/n9Z63/D1E1rrFxPXtKn1/vB6nURmYsvqPYR1sIOBosUhb5+TDy4vHOgJP5XNq738uWMrZ+fNxmrEZ7pd86KrsQydovyDZybc12yY+XDNh9nQvIHDJw/H5fFFhptkPQez2aC60E5tSQ7VhfZxPxiHKwLa2X96Lno0xxPR869xmlGUjccLNz826vl4rBGlVOb+/xNYv0TEX2e/M0SMBvcZ4BvwzCjKZlZJDjOKstN3oOMncSqmEYnuYf7kBImcxpbf6luv01+0KOTtc4aTFIw3lW3rqYN0uk5RNyrBwWT0F82ip2IxlVv/N6K1O1fMvAKLYeHh9x+OWxuEiIfpVAQ0HcjzIVKdxKgQmU8GO8O2N52kOsHJCfLb3sVlLcCZXRXy9ppcsBiwuyv8mZ2XOrZiVibOyZ/8ep2Rmhes8J3d2THx2Z0CWwGX1VzG+oPrOdZ7LK7tEGlgiovRjS4S6nZ7wxYNjUcR0ImKkorIeL0apUIv7E6roqxSfDGjjH59W8yhExSkVYxORGJYTHMy2Bn2QfMpZoVIZRs3WpPfsinseh0AswG1eb7006EPoflzx3uclTsLuym+CyP7i2cNr915KqKzOx+d81EMZfDL938Z13aIFDfFxehCFQnd3drLt5/eHrJo6GSLgE5UlFRExv9/fGTDQe67cdmYgo2FWYn7UimupPhiRgn1+u5zuMcUwk2rGJ2IxLAQMtgB6Btyc7zHQW1xgoqDAVmnDmIbOEF/ydnj7jcnH3Z2eAlV7HVH7xFahrrjOoVtpOaFvrM7ZTvXT7hvUVYRl864lN/v/72c3ZlOBtpPZ/AB38/ffda3PQFCrcG57fFGVtXNDFwfuSZn5AL5DXddwdN3XMLCiryIi4BGsuZHTMz/f1w2u4RfvLKP765cwpO3LOe7K5fw85f30p4u/88pjneRWKFe35//1bs4nJ70jdGJSAwLEVU2toy1p6UXgJkJHOwUNr8OQF/J0nH3O6MA/nxM09ynmZEX/AHtpY73MGFwbv7chLSxv3g2p0rPoHL7OtrOug49uuryKNfOvZYNzRu4Z/M93HvlvQlpk0gxU1yMLtx8+tFFKUfOr59MEVCZvx8f/v9jod0SVGTU7zvXpsm3ylJ8MaOEe317tebWxxqDtqdNjE5EYlgIGezA6cFObbF9gj1jV9j8JkPZVbjs4+exX1jk+7m1zcOMvNMn3vxT2BbnziTHnJWwdrbMu5wF7zxM0cHX6Zp/1bj7FmUV8bG5H2PtvrW80fQGH675cMLaJaaQ1+v71i9USlJ/MbqRb55xLkbn9Wo6+5043R6UUqxYUs6qupkU2i30OFysazw2pijlyPn1I+9vNZsosJlo73dGlDLWv+Zn5AeijJu/PwX8/8cehyvw/FUXZJFl8dUxMRmK1pMODMOgyG6h2+EKPF8lOdaIz8SNa7w4jtQUxLuYOuFe37k2My/9w4cxGQqPV7O24SgmQ3Gksx/LcHp0gLa+IVweb2CbYaigviZusRtOLDEtMSyEDHYA9rScwm4xUZqgAmGGe5D81nfoqZ64Ls2cfF+Sgq1tHq6dd/rb6z39TRwb7ODKGeckpI1+JysW4cgtp3Lr/9J1xpVh1xf5rZi9gg3NG/j3d/+dC6suxBbntURiivnnd48uNle+xPemmuBidP459f6pJrdeOpuvXLVgTDG/9VubgLHz60Pdf+W5NWPuv6g8N+SAZ3RRzGjX/Aifwixz4Hn6uyvn84tX9vHXF8/h9ie2BP6vd69ayht7WvnYuTVBRV4f+nx9VFMPQ5oojiMlxRczSqjX929vvpAeh5sv/npzUB+x5UgnX/nddmqK7Pzm5gs55XCPKUacbTXx+f95N76xG06sMS0xLISs2QHY3dJLTbEdNcEH+1jltW3G5BmccAob+AY68wp8g52RXurYioHivIJ5CWljgDJoOeNycjr3k9e8bcLdzYaZzy3+HE29Tfyk4SeJbZtIvInmd48sRvf3O3w/o/0AOY7Rc+qXzS4JDFTAN+Xk9scbub6+NuT8+tH3v76+NuT92/qGQj7+ZNf8CJ/2fic/f3kvN108hzue2MKqupnctW570PNw17rtXF9fG/gA6d8elzVS8VqnkOB4F1Mr1OvbUCpkH7GoqjBw3enWY+L0tscbOdI5MHXr+2KNaYlhIRI/2FFKHVZKva+U2qqUaghxu1JK3auU2q+U2q6UWpboNo2ktWZXyylqixK3Xqfo+Kt4DQv9xaGLiY62sBDeb/fg8pxOUvBS+xYW5NSQb05cO/06a+pw2fKo3PZURPufWXImK2at4De7f8NLR15KcOtEQkUyvzuBxehGz6kvtFtCzrHv6nfymQc3cetjjby4sw23xxvy/iZDhby/e5zsaiOLYpbl2WSgEwOXx+t7Xrw6sHYn1PMQ7vmZ9BqpeK5TkOKLGWX069sfoyP5YzNwH0XIfbKtpjHbEra+bzIxLTEsprmpmsZ2hda6I8xtHwXmD18uBO4f/jklWk8NccrhTlxyAu2l+OgL9JUsRZsiW2uzpBieOQQ7OjycV2HmQP8JDjlaubH6isS0cRRtstA2+yKq97yE9dQJnPmh6wKNdP2C69nfs5/vbvgu8wrmMbcwMUkURAjRzuOe4jU5o9fQjJ7XPnqNzq2XzmbZ7BIK7RaKc6ysWFIetMD91ktnU1Vo55WvXRY0v/5490Cgrov/g4nHq0PO0TfLACbu/M+j1+vFbCj+9PeXYjEUz/ztJWRbTWOex5oie9jnZ9JrpMaL49Hxby8BR+fk1vaItGU2Qq8LNJsMnrxlOT0OF0oRMk4HnMEDmxVLylHK1xdNeg2Pxw19LeBxgcniu8jaGyFikgo9+ieAR7XPJqBQKTXxp+s42d1yCkhcJrbcjq3YBlo4VRH5+O2sEt/PTSd8HelLHe+hgGWJnsI2Qvus5aCgbNcfI9rfbJi57ZzbMCszX37xyxw7Jemop0S0NRQm2t8/v7uw1nd9kvO7J6pbM/r27z27g5Xn1vD99Tv5zIOb+OKvN/OVqxawYkk5QGANzuce2sSV//k6X/z1ZlaeW8NLH5wI3H9kzYy1DUe5P0QNjfIErc+brvzP47ef3s7+9n7+9bkP6OjznX37xH9vGPM8+p+HEz39Y2qcxGWNVLg4tpeMjf/WHfDcP0gNkmmqxG7lK1ctCPQ531+/k69ctYAnNh4KXPd4dchaPGeU5wS2rVhSzp1XLeDTD2ycfI0uj9sXl7/6KNx7ru+nowdu+E3c+mYhphMVqp5LXB9AqUNAN6CBB7TWD466fT3w/7TWbw1ffxm4S2s9ZsqbX319vW5oCHtzVB54/QD//vxuHrqpntwEFBGb1fBDKvc8wp4Pr8FriXxAdcdrMLvQxCN/mcOqxh+iteZbZ3w67u0bzxnv/JLck8fZtvq3aJNl4jsATb1N/Mfm/yDXmssDf/EAcwvS7gxPQr7yj2fMBulr9X1AG/1t35f/7JuuEMv+8chiNay9d4jr7tsw5hvRp++4hLI825jbH7ipju+v3zlm/1994Xy6+p1UFdr53EObQt7+Fz99A/B96Pjex89Cax11NrY0lBLx6n8ev7tyCd9fvzPwM9zz6P/2/F8+diYVeVlTl41toD10/F/9I3hy9enr4V4/Ih5SImb9jncP8JkHx/Yp3125JJCOuqbIzk8/fS5dA86gsz8/uO5sFCpwVvrTD2wM29dF5WSTb4AzOk7/5iVf0iA5CznVZCpAmpuKV8mHtNbL8E1X+1ulVEz5iZVStyilGpRSDe3t8SuGtaell+IcS0IGOmhNyZHn6S9eGtVAB+DsEth8wsOBvlb29h9PWCHR8bTPvgiLo4fCw29HfJ+avBq+Vv81+l393LD+Bv5w8A8JbGFqS1TMBol2HvcUr8mZqG5NtGt03B7vhPPrX9zZhtY6MCffajUzoyibWSU5zCjKzqSBTlxNJl5H1tUZb43OmLVWXo3ZbCRmjVSoOA4X//ai4OtSgyQtxKOPDbdmZ3QtL38tnpHx63J7A7GrdejjxLSGx+MKHacuh6y9ESIGCX+laK2PD/9sA54GLhi1y3Fg5ojrNcPbRh/nQa11vda6vqwsfqdtd7WcoiZByQny2huxDTRzsmL0nzyxc0phwA2PHdkCkJTBzsnyRQzZiyjbuT6q+83Kn8X3LvoeM/Nm8k9v/hO3vHgL29u3J6iVqStRMRvEvzZhpPHmcYfbXynoOQZ9rXg9bjocHTT3NdPh6MCrY5/S469rMdKtl84G4EhnP0BgahNAj8M1Zn9/vRY4vQZn9O2eEVNFpC5ObCYTryPr6oz8OdLI59F/fcrXToWLf0d38PXJrFHT3ri9fsT4YolZl8vD8e4BjnT2c7x7AIvJiChWR6/PGd3PhOrrYu6L/OtzRiqs9W2fQhLLIlMkdLCjlMpRSuX5fwdWADtG7fYs8PnhrGzLgZNa6xOJbJef2+PlQFs/tQlar1O+/yk8pix6y6Mf7JxbBmYFr3ZtZW52JSXW/AS0cALKoH3WcgqOv4etpymquxZlFfH1+q/z6YWfZkfnDm7844187g+f46k9T3HKeSpBDZ6Gol1jE2r/Tz8Gf/wm/OwsvOv/kX3de7nxDzdy9bqrufEPN7Kve1/Mb3JFdkvQXHf/mpvPPLiJy+55jc88uCloLce6xmMh58ava/StAQu3Bmdtw9HAdamLM/X89UvWNR7j7lVLAz9HPk/33bgs8Dwmbe2UvcQX76Pjf+tvT1+fzBo17WVf9764vX5EfLlcHna39QX1PwNO97h9jr9PmVWSPe7aMv9rIC7rz3IqQsdpztRNrZRYFpkkoWt2lFJz8Z3NAV/mt99orX+olLoNQGu9RvmK2/wCuAYYAL443nodiN/6h/1tvXzkJ29w+2Xz+PCC+H7zbnL2Urd2Oacql9O85OaYjvHNd7o5WnI3n6r8EB8tr49r+yJlGTzFOS/+Gy3nfIqm5bH9HQ63gzea3uCt429xvO84VpOVq2ZexSfP+CTLq5djqJQ6FZ9S88kjMplsbEr5Bjp7fNMNOz73W27c/l809zcHdq/OqeaJa5+g1F4addPae4f49tPbA5mOZhTZuSHE/Pgnb1kO+L4dLbJbgtZwFGaZfWtuPF7MJoPSbAsdA67A9bIcKz2D7qmrYp5aUiZeR2Zjc3s1gy4PWRYTHX1OWk4N8vLOVq5aUkFJjpWqgqzkrJ3qa/UlIzj3s76pa45uOPouLL8VtJ70OogORwc3/uHGuL1+MlTSYjbc+px1t12E26sD6/paTw5QWZgTWPfnH7CMl1USJs48GbG+VtjwCzjvRjBM4PXAe0/AJX83ZWvJJJaDTJs3lEyV0NTTWuuDwDkhtq8Z8bsG/jaR7Qhnd0svkJhMbCWH12PyOOieEXu66JLSHRzVUGtZGMeWRceVlU9PxWJK9rxI0wVf8nW8UbKb7Vw9+2pWzFrB4VOHeev4W7ze9DrPH36eZeXL+OGHfkhNXk0CWj9N+NcmxLJ/z7HAQAfAmV0c9OYG0NzfjNMT2xoGp9vDizvbAimHX/vG5WHr3swqyQlsG72gt7oweHpItTW46yqTdThJ569fAr4PlVf95A1e+8blfOK/NwT2earRd4b49W9cnpy1U26nL973jFpLeOEtvnUQk+T0OOP6+hHxFW59zqDby2X3vBa0fcNdVzBj1BT3iRINjHwNTK6hTth4r+8y0oW3TP7YEZJYFplkqurspKQ9Lb0YCmaM+iA1aVpTsfcJBnNn4siPPV10l3k7nt4qjrRXcGbRQBwbGJ2O2gspevd/KDz6Dj2zL475OEop5hTMYU7BHG5YeAMbmjewdu9aPvuHz3L/R+7nrNKz4thqEZFR9UisA11U51SP+TbPaoptWpjVbOLWS2dzfX0tJkNhGa5pMbreynhrN9xuL219Q7g8Xiwmg/JcG2ZzSp0NFKP4n3eLoVh720V09jtZ89oB3jvWE3i+3W4vZrMRv2/DI5GAOlIjWU3WuL5+RHyZjeBiShMRAAAgAElEQVQ6XODrf0yG4o1vXoFXawyl2Ha0M371cmJqaGLjNBISyyKTTOtPDLtbeqkqsGON8wen/LZ3ye3eSdfMFb5pQjFod3VzxHUMm+NMGpqTWxPkZMViXLY8Snc/H7djWkwWLp95Od9Z/h0shoVbX7qVAz0H4nZ8EaFRa3iKtzzBvZf/lOqcasD35nbvlfdSnFUc0+ELs8ysPLeGL/56M1f+5+tj1uj458eXhZnX7nZ72d3ay6cf2Mhl97zGpx/YyO7WXtxumTeeyvzP+2ce3MT1azby/fU7+frVC1mxpJz7blzGo28fYndrLy6XZ9w6THEX5zpSoxVnFXPvlffG7fUj4qssxxpyzZ9Haz730CYuv+c1frD+A2aX5cenXk6sEhynkZBYFpkk4XV2EiFe6x8uvfsVqgvt/P1HFsShVactfPVm8lvfZe+lP0fH+C3Ic92v80Tn85zluJV3j8zml59sI8eavOeq5oPnqDzwOttW/xZXTklcj90+0M6P3vkRRVlF/G7l78ix5Ex8p8RJmTUQU2bUmh+vvYQuZw9OjxOryUpxVnHM66qaexwha0/85ubluD1ePF7N2oajfPnDZ4Sc/hHu/k/detGYqW3TVErG63jP+y9e3sdTjU2BtVqh1lDEVJskUnGsIxXy8NpL12BXXF4/GSppMdveO8TDb+wPnGn2eDW5WSY+teZ0DIar9ZXQmAwlwXEaURMklv1kzU6am7bT2PqH3BzrdrB8bnw/uGedOkhx08u0zbku5oEOwIbebcywlHNetoWNhxXvnbDxoVmDcWxpdDpqL6Rq/6uU7H2JlvNuiOuxy7LLuO2c27hn8z3cs/kevnfx9+J6fDGBUWt+DIjbAlRXmLo4J3ocfObBTYFtn794TlT3d3vkzE4qG+9596/b8a/VilttkkhFu8Yt2sMrYzou4E4LTreHB948zANvHg5se+Vrl0VU6yuhMRlKguM0oiZILIsMMS2H6HA6OUFtSXyTE8zYsQavYfVNYYtRi7ODw85mltjnUFswRK7VzebjyZ3KNphXTm/JXEp3v+DLWhRnC4sXsmL2CtbtW8fmls1xP75IjkhrWICv7k5zjyNoilq4+5tN07brSnne4YxWkdbZiVttEiEmEKoWjlcTtC1cjSiJSSHS17T9xLC7xVfrZVYcM7HZeo9SdvBpumdcgcdWEPNxNvb5CnAusc/FULC4dID3TthwTfEXS6O1116A/WQTuS2jSyXFxyfO+ASl9lJ+vPnHkss/Q5Tn2iasYXH/6jr+9bkPQq7JCXX/NcmozyIi4vVq9rT28ujbh7jvxmUTPu9l8axNIsQEQtXCybEZQet41jUeG7OuR2JSiPQ2baex7TpxihyridI4fmiaseM+tDLomP3xSR3n7b5t1ForyTflAnBmWT+bm/PZ2W7lnMrkpX3srj4Hz/u/p2zX8/RVnR3349tMNj55xid5+P2HefHwi1wz55q4P4aYWmazwaKKPJ669aKgujg/vG4p//Ix3+j9X5/7IJCdranbwW2PNwbW5IS6v2RjS12d/U5ufrSBpm4H3QNufvWF8zEZCpvZ4NG3D7GqbiZ/86G59Dhc/PzlvfzwuqUsrMjj6Tsuma51ksQUMgw1Jt7cHi8/f3kv3125hEK7hR6Hi/Vbm4Jqf0lMCpHepu1gZ/eJXmYWZ6NizJY2mq2vibID/0d3zZW4s4piPk6Ts5VjzlauKTid4nl+kQOrycu7TVlJHex4zTY6Z5xLycHXOfKhv8VrjX8igeVVy/nToT/xX1v+i6tqr8JissT9McTUMpuNMckE/HVxjnT2B6WhhrFrckLdX6Qmp9sTWO/wVGNTUF2dB948DCPWSgD8y8c88atNIkQERsebvw8a3Q/duHw2tSVJTZYjhIiTafn1qNer2dVyito4TmGb8f59oKBj9scmdZyNvdtRKBZnnV6wbTFpFhQP0NBsS8Rymah01F6IyT1Eyf5XE3J8QxmsWrCKpr4m1u1bl5DHEKlD1uRkllBrImqK7GGfZ1kHIZJN+iAhMt+0PLPT1O2gf8gTt+QEtt4jlB1YS8+MK3BnxZ7dzau9vNG7hdm2anJNwW1bUjbAjvZcDnabmVfsnmyTY9ZfVMtAfhWlu1+gfcnKhDzG2aVnM79wPr/c8UtWzV8lZ3cyzMgikllWg19/8XyOdTnItpoYcHqYWWwPWpMzUdHJKS1KKQJC/d/9ayJufrSBi+eWcMtl87CYfM/Fr794Pl/41Waauh2yDkKkjPJcW8g+yGZJYlFRIURcTcvBzq5AcoL4nKKeue1noAza535yUsfZM3iYdnc3l+SeO+a2xaX9KDSbj2cxr7hvUo8zKUrRUXsBtTuewd51CEdx6JTBk3sIxbVzr+VnW37G+oPruW7+dXF/DJEc/gXs/nUdK5aUc+dVC/juMztOfwi+qT7wwWL0/v4PyQsr8jAMNeHtIjHG+78vrMhj/Vcu4Vj3IF/41buB2++/cRn33biM/CwL2TYTpTk2eY5E0mmtGXR5g/qgNavrePC1Azzw5mHpU4TIANPyPO2uE6dQwMziya8DyO7eTemhZ+msvRq3Lfa1OgCvnWrEpiwsypo95rYci5c5hYNJT0EN0FlTh9cwUbrr+YQ9xtmlZzMrfxYPvf8Qbm/yzmSJ+Bq5gB1gVd1Mbnu8MXC9qdvBzY810NnvDLl/U7eDmx+N/HaRGOP93w1DMeD0cvuo5/X2J7Zgt5hY/ct3UCj54ChSQlvf0Jg+6LbHG7m+vjZwXfoUIdLblAx2lFImpdR7Sqn1IW77glKqXSm1dfjy5US354PmU1QVZmGLw3zxmVt/gtdsp3OSa3UGvUNs6nufJfa5WIzQJ9yWlPVz9KSF1r7kznN323LpqTyLkr1/RnkS8waglGLl3JUc6z3Gi4dfTMhjiKk3cgE7TFzAb/T+0d4uEmOi/3u4oqImQ8nzI1JKuMK2phGDcYlZIdLbVJ3Z+Sqwa5zbn9Ranzt8eTiRDdFas/VoD/NKcyd9rNz2LRQ3/ZmOWSvxWCZ3vHf6djCknZxjXxB2nyWl/QA0Nif/7E577QVYhk5ReHhjwh7jvPLzmJE7gwfff1Dq7sSR16tp7x3iePcA7b1DeL1Tl/Vi9AL2iQr4hVvwHuntIjHG+7+PV1TU49Xy/IiUEi5WzSaDJ29ZzgM31bFiSbnErBBpLOGDHaVUDXAtkNBBTKRaTg3S3jfEvPJJDna0pva9e3BZC+iqnXw9mJdPvUOJqYAaa0XYfUqz3VTkOGlIgcHOqfKFDNmLKNv1x4Q9hqEMrp1zLQd6DvDqscRkf5tu/GstrrtvA5fc/SrX3beBPa29UzbgGV3Ub13jsTFFQ0cuXA9VBDCa20VihPu/F9kt7Gnt5V+f+4C7Vy0Nuv2+G5extuGoPD8ipZTn2sYUEb1/dR0/WP8Bn3lwE99fv5M7r1pAkV0S5QiRrqYiQcHPgG8CeePss0op9WFgL/APWutjiWrMtmM9AMwrm9xgp+DEWxS0vsOJhZ/Ha86a1LH2Dx5j7+BRrs6/aMK6P4tL+3nzaCH9TkWONYl5qJVB+6wLqdn9AlldRxgsnpWQhzm/8nyeOfAMD25/kCtnXhm3ukjTVbi1Fk/fccmU1DoJVdSvyG4JW1Qy1P7R3C4SI9z/fWR8tfc6+e7KJZTkWKkqyMJmNvjyh8+Q50ekFIvFxKLyXJ68ZTnu4bOSoQodT1UfKYSIv4Se2VFKrQTatNaN4+z2HDBba70UeAl4JMyxblFKNSilGtrb22Nu09ZjJzEbilmTSTutvdS+92OcWaV011wV+3GG/bHnLWzKyjnZ4aew+S0pHcCjFVtbkt/pts++GI/JQuX2tQl7DJNh4qNzPsrOzp1saN6QsMdJhHjFbDylwhoXf1G/GUXZlOXZMJuNoOujPwiP3j/a20Vkoo3XUP/3kfH13rEebn2skevX+Ka6luZlyfMj4ipefazFYmJGUTazhouIhip0LGt2hEhfiZ7GdgnwcaXUYeB3wJVKqcdH7qC17tRaDw1ffRioC3UgrfWDWut6rXV9WVlZzA1qPNLFrJJsLJMoGFZ66Blyu96nbd71aGNyp7Y73Sd5p+99zsteiM2YeGpHbcEgORYPDSmQlc1ty6Vj5gWU7H0J80BXwh7n4uqLKckq4cHtD6KTXVU1CvGK2XiaijUuo9cEud3ecdcIJXMNkTgtHvFqNZtYsaScB26qC1rvoJSS51XEXbz6WJfLw/HuAY50+tbFrlhSHnS7rDMTIr0ldBqb1vpbwLcAlFKXA1/XWq8euY9SqkprfWL46scZP5HBpDicHt472sM1Z1XGfAzD1c+sLf+Pgfx5nKz60KTb9ELP23jRnJ9zZmSPr3xT2bacyMXtBXOSk4e3zvsw5YffpnLbWpouuiUhj2E2zFwz5xqe2PUEDa0NnF95fkIeZzoYWfQxEcUdw9XR8ad2lTo5ma0wy8xXrloQSDvtX//w4o5mLpxXJs+rSDkul4fdbX1jYhZ8Z3hkHaAQ6S8pH5WVUv+mlPr48NU7lVIfKKW2AXcCX0jU4zYe6cbt1ZxZnR/zMWbsWIPV0U7LwptATe7f1+Pu5cWTG1mcNYdC83hLmoItLh1gwGWwpyP5ne9QbhmdNcso3/F7LP2dCXucS2dcSoGtgIe2P5Swx5gORq612HDXFTx9xyVx/QAaUR0dqZOTsdr7nWPr6zzeyCXzy+V5FSmprW8oZMz+y8fOTEgfKYSYelM22NFav6a1Xjn8+z9rrZ8d/v1bWusztdbnaK2v0FrvTlQbNh7swFCwsCK2wY6tr4nqnQ/RU3kJjsKJ19dM5Pfdr+LSLi7PDzlzL6wFxQOYDc27TcmfygbQvOhqlNdD1ZbfJOwxrCYrV8+6mo0nNrK9fXvCHmc6SOQal3jX0RHpRerriHQTrs6O26tlHaAQGSLJk6Cm1lv7OphXlovdGsPcW62Zu+nbaGXQOv+GSbfl8FAzL57cyHnZiygxF0Z1X5tZs7Ckn7ePZeFJgfIzQzmldMy6gLJdfyCrJ2GJ9Lh85uXkWfO4Z/M9UncnRcW7jo5IfSPXXEl9HZHqRq8RDFtnRwY4QmSMaTPYae5xsK3pJMtmFcV0/7KD6yg88SZtZ3wGd1bJpNri1m4eaFuH3cjiivzY1p8sq+yjZ9DE+63Jn8oGcHzhNXhNFma98TNIUBKBLHMWn1rwKba2b+X3+3+fkMcQkzO6/sqWw51jalisWV0XqFkhdXLS2+i6TY++fWjM8y31dUSqCFVnzKt1yDo75bmpMXNCCDF5U1FnJyW8sKMFgAtnF0d9X1vvUWZv/j79hQvpmvkXk2qH1prHOv7AoaHjfKroL7AbsXWoi0v7sZs9vHbYzrlVyZ8H787Ko2nJSmZv+19K9/yJjkWTL7QayiXVl/DW8bf4ScNPuGLmFRRlxTZ4FYkxuv6KUorvPbuD765cQqHdQo/Dxb0v7+WH1y0NTA+ROjnpa/SaqwfePAzAk7csx+PVmAwl9XVEygi1RvCzD73Dc1+5OKjOTnmuDYtFzkIKkSmmzZmdP2w/QW1xNlWF9ol3HsFwD7Lw9dtBezl+5m2TSkqgtWZt15/508mNLM85m0X22TEfy2xAXVUvm45l0TmQGk9j+6wL6S2ZS+2G/yar+2hCHkMpxU1LbqLP1cf3N30/rVJRTxcj1wRprXlxZxu3PtbIZx7cxK2PNfLizragtRtSJyd9hVpz5R/w1JbkMKMoW+rriJQRbo3gwJA3UGdnRlG2DHSEyDCp8Sk5wXYcP0nj0W4unV8a3R29HuZt/CY53bs4ftbtuLIrYm5Du6ubn7Y8zrrulznHvoCP5F8Y87H8PjTzJF4Nz++bRIHUeFIGB+pW4zVMnPGnf8Fw9ifkYWbkzuCv5v8VLx15icd2PpaQxxDxIWtyMps8vyKdSLwKMT1Ni2lsv3zrEHaLiSsXlU+8s5/2Mm/T/0fp4fW0zP8sfWXLAje1ODvY4TjA7sHDtDo76facwq09WJUZu5FFrimbPFM2uYZvEHLM2crewSOYMLgq7wIuyl2KUpP/lrPY7mZpRR8v7Mvho/MHKMlO/qJ9l72QA3U3sXDjAyz4w7fY95c/wmPLjfvjXD37ag72HOTHDT+mPLuca+YkZtqcmJxE1/URySXPr0gnEq9CTE8qHacB1dfX64aGhoj23XK0m1X3vc1fnl3F6uWzIrqPyXmKMzZ8jeKml2mb+1e0z7ueHncvb/dt463e9zg4dByAXMNOqbmIPFM2ZmXCpT0MeZ049BCD3iEGvIMoFIWmPObYZrAsexEF5vh+8O9ymPnPTTNZVu3kaxf3EIcxVFwUNW9jbsPjOErmsn/FP+PMr4r7Yzg9Tv6z4T85cPIA377w23xqwafiMYhMyH8wmpjNNF6vprPfKWtyEiPp8SrPr4hSUmNW4lXEQAIkzWX0mZ2ufidff2obJblW/mrZjInvoDXFR19gVuOPsA20sGfB53ipsIKNx3/J+479aDSVlhI+kn8hC7NmUWTKj8sZmskotrv5yJxunj9Qwu9353Dd4sRMHYtWd/U57L/AzNzGJzjzf2+lafmX6Vj0UbTJErfHsJqs/GPdP3Lftvv4/qbv886Jd/h6/depyo3/wErEzr8mR2QmeX5FOpF4FWL6ydjBzntHu/nm2u00dTu465qFZFvD/KlaYz+5n8Lm18nd/780DR7h0fwK3ppxPttdb+Np91Jkyufi3HM4234GZZbUy/51+awemvus/GZ7Hm19Jm44u4+CrORPaTtZeSYfXPF15mz5LbPfvJeqLb+lc8Ff0DP7YgZK58Vl4GMz2/jqsq/yx0N/5LkDz/Hy0Ze5qvYqVs5dSV1lHfnW2ArICiGEEEKI9JdRg517X97H+8dPcqCtj4Md/RRlW/jG1QtZUl0AQMGex/h954u4vU7cniE87j5OeQboMjRtZhMtxWagEoBSPcAFOWexxD6XKktp0s/gjEcpuGFJGwU2Dy8fLODVQ3bmFrkozfGwpMzFNfMHktY2Z3Yxey65g/z2PVTuf42qrb+l+r3f4DVZGMqrwplbhiu7CK/JhjZZ6Zr3Yfqqzo7qMQxlsHLuSpZXLeeVo6/wZtObvHjkRRSKGbkzqMmroTirmGxLNjnmHBaXLObaudcm6C8WQgghhBCpIi3X7Cil2oEjEe5eCnQksDmpYDr8jTA1f2eH1jru2Q6ijNlYpXocSPsmJ1T70jlew0nH5yFVpHLbwNe+3UmI2VT/v4xH2p48/vYnpJ8VUyctBzvRUEo1aK3rk92ORJoOfyNMn78zVqn+/5H2TU6qty9eUv3vTOX2pXLbIHntS/X/y3ik7cmT7u0Xp02LOjtCCCGEEEKI6UcGO0IIIYQQQoiMNB0GOw8muwFTYDr8jTB9/s5Ypfr/R9o3OanevnhJ9b8zlduXym2D5LUv1f8v45G2J0+6t18My/g1O0IIIYQQQojpaTqc2RFCCCGEEEJMQzLYEUIIIYQQQmSkKRnsKKVMSqn3lFLrQ9z2BaVUu1Jq6/Dly1PRJiGEEEIIIURmM0/R43wV2AXkh7n9Sa31301RW4QQQgghhBDTQMLP7CilaoBrgYfjdcxrrrlGA3KRSyIuCSExK5cEXRJC4lUuCbwkhMSsXBJ4EWluKqax/Qz4JuAdZ59VSqntSqm1SqmZEx2wo6Mjbo0TYipIzIp0IvEq0o3ErBAinIQOdpRSK4E2rXXjOLs9B8zWWi8FXgIeCXOsW5RSDUqphvb29gS0Voj4kpgV6UTiVaQbiVkhRCQSfWbnEuDjSqnDwO+AK5VSj4/cQWvdqbUeGr76MFAX6kBa6we11vVa6/qysrJEtlmIuJCYFelE4lWkG4lZIUQkEjrY0Vp/S2tdo7WeDdwAvKK1Xj1yH6VU1YirH8eXyEAIIYQQQgghJmWqsrEFUUr9G9CgtX4WuFMp9XHADXQBX0hGm1KR16vp7HfidHuwmk2U5FgxDJXsZgkhRMSkHxOZQOJYiPQ1ZYMdrfVrwGvDv//ziO3fAr41Ve1IF16vZk9rLzc/2kBTt4OaIjsPfb6ehRV50sEKIdKC9GMiE0gcC5HepqSoqIheZ78z0LECNHU7uPnRBjr7nUlumRDReb/9fX68+ce81/Zespsippj0YyITSBwLkd5ksJOinG5PoGP1a+p24HR7ktQiIaK3v3s/X/rTl3hk5yN88YUv8kHHB8lukphC0o+JTCBxLER6k8FOirKaTdQU2YO21RTZsZpNSWqRENG7Z/M9WEwWfnDJD8i2ZPNfW/4r2U0SU0j6MZEJJI6FSG8y2ElRJTlWHvp8faCD9c8RLsmxJrllQkRmT9ce3j7xNitmraA6t5oVs1aw8cRGmnqbkt00MUWkHxOZQOJYiPSWlGxsYmKGoVhYkcfTd1wi2V9EWnrmwDOYDTOX1VwGwAWVF7Bu3zr+fOTPfOGsLyS3cWJKSD8mMoHEsRDpTQY7KcwwFGV5tmQ3Q4ioebWXFw69wNmlZ5NrzQWgLLuMWfmzeOXYKzLYmUakHxOZQOJYiPQl09iEEHG3s3Mn7Y526ivqg7YvLl7Mjo4dDLoHk9QyIYQQQkwnMtgRQsTdphObAFhSsiRo+4KiBbi8Lt7veD8ZzRJCCCHENCODHSFE3G1s3sjM3JkU2AqCts8vmg/AltYtyWiWEEIIIaYZGewIIeLK4XbwXtt7LCldMua2HEsOldmV7OralYSWCSGEEGK6kQQFKcDr1XT2OyXLi8gI77VuweV1cXburJC31+bXsrNz5xS3Skw16ddEupBYFSKzyWAnybxezZ7WXm5+tIGmbkcgf//CijzpbEX68XrZ9Op3sWjNque/z7Fr76av6qygXWblz+Ldlnc5OXRyzDQ3kRmkXxPpQmJViMwn09iSrLPfGehkAZq6Hdz8aAOd/c4kt0yIGGx+mC39TSxQWZhtucx9+Ucojztol9q8WgB2d+1ORgvFFJB+TaQLiVUhMp8MdpLM6fYEOlm/pm4HTrcnSS0SIkbuIZxv/Sc7bTZmlp7J0bOuw9bXRtHBN4J2q86tBuDQyUPJaKWYAtKviXQhsSpE5pPBTpJZzSZqiuxB22qK7FjNpiS1SIgY7V7PbmcXLgVzc6o4WbGIwZwyynb9MWi3QlshdrNdBjsZTPo1kS4kVoXIfDLYSbKSHCsPfb4+0Nn65wuX5FiT3DIhorT9SbblFQMwL7sKlEFX9VLyTmzHNHgqsJtSisrsSg6ePJislooEk35NpAuJVSEynyQoSLCJsrwYhmJhRR5P33GJZIIR6WvwFOx/mW1zz6TEcFNkyQWgp+osqve9TMHRzXQtuCqwe2VuJQd6DiSrtSLBRvdrSilMyrc+Qvo3kUyh3pPlPViIzCaDnQSKNMuLYSjK8mxJbKkQk3T4TfC62aZczM2uCmzuL5yJ22In78S2oMFOVU4VG5s30u/qJ8eSk4wWiwQzDEVJjlUyXYmUMd57srwHC5G5ZBpbAkmWFzFtHHiFNms2Le4+3xQ2P2XQVzyHvObtQbtX5fj2OXzq8BQ2Ukw16QNFKpF4FGJ6ksFOAkmWFzFt7P8zOyvOAGBOdmXQTb0lc7GfbMLs6A5sq8zx7SNJCjKb9IEilUg8CjE9yWAngSTLi5gWug5C92F25ZeigFp7WdDNfUWzAMhp2xvYVpFdgaEMGexkOOkDRSqReBRiepLBTgJJlhcxLRx8DYCdZkWVrRibYQm6eaBgBhpFTvvpwY7ZMFOeXS6DnQwnfaBIJRKPQkxPU5KgQCllAhqA41rrlaNuswGPAnVAJ/AZrfXhqWhXokmmNTEtHNsMWYXsHGwLSk7g57VkMZhbTvaIwQ74zu4cPnl4ihopkkH6QJFKJB6FmJ6mKhvbV4FdQH6I2/4G6NZan6GUugG4G/jMFLUr4eKVaW2iFNZCJE3TZjpK59Lm7ODKknNC7jJQOIPcjv1B28rsZWzo3oDWGqUkltNduD5Ksk2KVBIqHuX9VYjMlvBpbEqpGuBa4OEwu3wCeGT497XAVUo++QTxp8u87r4NXHL3q1x33wb2tPbi9epkN01Md4MnoXMfuwt8CQdm2ctD7ubIq8TW347hHAhsK8suw+F20DXYNSVNFYkjfZRIVxK7QmS+qViz8zPgm4A3zO0zgGMAWms3cBIomYJ2pQ1JlylS1vEtAOzM8n1TOnNUcgI/R14FAPaeo4FtZcP7NvU1JbKFYgpIHyXSlcSuEJkvoYMdpdRKoE1r3RiHY92ilGpQSjW0t7fHoXXpQ9JlpqdpEbPHfS/t3dpJhbWQbFPo6Ur+wU5W94jBTvbwYKdXBjupYDLxKn2USIZ49LESu0JkvkSf2bkE+LhS6jDwO+BKpdTjo/Y5DswEUEqZgQJ8iQqCaK0f1FrXa63ry8pCf3ucqSRdZnqaFjHb1AD5M9jjaKEmqzTsbkPZJXgNM/buI4FtpXbf/jLYSQ2TiVfpo0QyxKOPldgVIvMldLCjtf6W1rpGaz0buAF4RWu9etRuzwJ/Pfz79cP7yGTZESRdpkhJWsPxBgZLzuDYYAczssaZfWqYGMwtw951erBjM9kotBXKNLYMIH2USFcSu0JkvqnKxhZEKfVvQIPW+lngl8BjSqn9QBe+QVHGc7u9tPUN4fJ4sZgMynNtmM2hx56SLlOkpJNN0N/OgcIqdM8hauzhz+yAbypbTvfhoG2l9lI5s5MBRvZRXq8XjwatfRmuRvdVkvlKpJJI31+jec8WQqSWKRvsaK1fA14b/v2fR2wfBD41Ve1IBW63l92tvdz2eCNN3Q5qiuysWV3Hooq8cQc8kr5VpJTh9Tr7snOgh3GnsQEM5lVQfHwbhmsQryULgPLscg70HEh4U0XiGYaiJMfKntbewIJv/7fkCyvyMAwVyHwV7nYhkmGi99dY3rOFEKlDXqVJ0NY3FOg0wbcY8rbHG2nrG0pyy4SIwvEGMA5+b1QAACAASURBVCzsw4VVmSm3Foy7uyOvEoUm6+SxwLYyexltA204PZL5KBNMlNlKMl+JdCTv2UKkNxnsJIHL4w2Z/cXtCZedW4gU1NQIxXPZN9BCVVYxhhq/OxnM9dXgyeo5PW2tLLsMjaa5rzmhTRVTY6LMVpL5SqQjec8WIr3JYCcJLCYjZPYXs0meDpEmPG448R6ULWBv/3FqxktOMGwo27eP7eTpgY0/I9ux3mMh7yPSy0SZrSTzlUhH8p4tRHqTV2oSlOfaWLO6Lij7y5rVdZTnypockSbad4PLQXfRLDpdvcyYYL0OgNdsxZmVj+3U6cGOv7BoqDM7x7oG+NpT2/jL/3qTf3xqK4c7+uPXfpEQE2W2ksxXIh3Je7YQ6S0p2dgyncvloa1vCLdXYzYU5bk2LJbT31yazQaLKvJ46taLcHu8mMNkdpkOWYu82kvXYBdOjxOryUrx8HSocNtFijjeAMC+7Hxg4uQEfkM5JUGDnQJbAWbDzPH+40H7bTzQyZce2YzWmvnlebywo4WXd7XxyJcu4NyZhXH6I0S8hcpsVWS30NnvDGRpy8sy8+Qty1EKQFGWY834fi7ZpJ8d3+j32sIsM+39zqDMawvLc3nyluVB7+uSnCC0SONK4k9MFRnsxJnL5WF3Wx+3j8jacv/qOhaV544Z8FQX2sMeZzpkLfJqL/u693HnK3fS3N9MdU419155L/MK53Gg58CY7fOL5ktHmCqON4Itj316EIh8sDOYXUpB58HAdUMZlGaVBp3Z2dfay5ce2UxJjpV/umYRJbk22k4N8sM/7uJvn9jCH796KQV2S3z/HhE3IzNb+fuxn760h7++eA53rdse6M/uXrWUN/a08rFza4KyXGVaP5ds0s+OL9R77f2r6/j5y3t5cWcbNUV2fv3F83G5NTc/lrnvx/ESLt5Gx1Wk+wkRDxJRcdbWNxQY6IBvEePtMWRtmQ5Zi7oGuwIdHUBzfzN3vnInHY6OkNu7BruS2VwxUlMDlC5g38AJck12CszZEd1tKKcU60Anyn369VBiLwnU2hlye/i737yHxaT41kcXUzI8TaQ8P4uvXHkGJ046+OlLe+P/94iE8Pdjq+pmBgY64OvP7lq3nevra8dkucq0fi7ZpJ8dX6j32tsfb2RV3czA9WNdjsBAx79N4jS0cPE2Oq4i3U+IeJDBTpy5vTp01havjuo40yFrkdPjDHR0fs39zbg8rpDbJT1xihjq863ZKV3AvuHkBEpF9u3mUM5wkoJTLYFtJfaSwJmdX751iD2tvdx66TyKR63jOKM8j8sWlPPEO0do6h6I0x8jEsnfjxXaLSH7M5OhMr6fS7aw/axX+lkI/15bOOLscbbVJHEaoXDxNjquIt1PiHiQwU6cmQ0VOmtLlKe6p0PWIqvJSnVOddC26pxqLCZLyO1WkyxiTgkntoL2okvms7//BDMiyMTmd3qwE5ykoHuomyPd3fz8lf3Uzypi2ayikPdftWwGWvsGRSL1+fuxHocrZH/m8eqM7+eSLWw/a0g/C+Hfa3scrsD1AadH4jRC4eJtdFxFup8Q8SCDnTgrz7Vx/6isLffHkLVlOmQtKs4q5t4r7w10eP45u6X20pDbi7OKk9lc4Xe8EYDWgkoGvENUR/G8DOb41vZkjRjslNh9A6D/fmMzQy4Pn7uwNuz9S3JtXDCnmLWNTQw43bG0Xkwhfz+2rvEYd69aGtSf3b1qKWsbjo7JcpVp/VyyST87vlDvtfevrmNd47HA9ZnFdh66KbPfj+MlXLyNjqtI9xMiHpTW0U2vSgX19fW6oaEh2c0Iy+l0097vDGRtKcuxYrWagzK+WMwGZkPhcIbPQCTZ2JKSpSUh/+BUj9moPHkTNG1m4xVf45b37+Ubc1exOHdmZPfVmvOe/w6dC1dw9ENfAWB/z35+9M6P8DR/ibqyi7njijPGPcTuE6f41/U7+Y9VS/n0+RE+buZK+Xgd3R+aDIVXg0mBYRgU2S10O1wZ3c8lW4r1sykXs6MzqJZmW+l0uIKypYJvTe54GVSFTwZmY5MOKc1JNrY483o1BzoHxmRRm1+Wy772vqDt91y/lP94YQ/tfUMhM7uMzGqUqQxlBApLRrJdpIDjjVAyn0MDvnU3VbYovolTiqHsEmwnTwQ2+WvtuIxOPn5udbh7BiyszKOqIItnth6XwU6Kc7k87GnvnzA7Zab3c8km/Wx4breXPW19QRkB16yuY1FFXmAwMx2yo8ZTpHEl8SemSkoOodNZuCxqbX1DY7Z/Y+12brt8nmR2EemjtwVOHYeyhRxytGI3rBFnYvMbyi7G1nt6sJNjzgOvmfKifmqKJj6WUoqL5pWw8WAnbb2DUf8JYurEKzulEInS1jc0JiPgbaNidDpkRxUik8lgJ87CZXZxebzjZnyRzC4iLQyv16F0AYcGWqi0FUWcic3PmV2EtbcVhqfQbj3iwuMqoqSwL+JjXDS3BK+G599vmXhnkTTxyk4pRKKEe292e7yB69MhO6oQmUwGO3EWLrOLxWSMm/FFMruItNDUAIYJiudycKCFqhgWkw7ZizB5nJgHTwLw+q5BDE8RbiPy+go1RdlUF2bx8q7WqB9fTJ14ZacUIlHCvTebTac/Hk2H7KhCZDIZ7MRZuCxq5bm2MdvvuX4pa147IJldRPo49g4Uz6MfTbvzJFW20Cmix+PM9g2QrL2t9A16aTgwRHFWMV1D0Z2lObemkE0HuyQrWwqLV3ZKIRKlPNc2JiPgmlExOh2yowqRySRBwTiizYbm378428JTt16E1jrofgsr8nj6jkuCsrH94nPnZUQGojTKqiJi5XH5prHNX8Fhh++MSlTJCYYNZfsGSLa+Vja31+L2Qm1BCdt6TzHoHiArwjVA59UW8ccdLby9v5OPLKmIuh1icibqH71eTc+gmxmFNp68ZXkg01V5ri0oOYGIH+mHo2c2GywoywmK0bIca1CmtdHv35nwnp3OJM5FtKIa7CilLgZmj7yf1vrROLcpJUSbfSX8/vbA/iGzq+VMxV+TWF7tZV/3Pu585U6a+5sD+fLnF82XDiiTtGwH9yCUL+FgLJnYho08s7Pp0BCFOYqagmK29ULHUAs15rkRHWdhZR5ZFoNX97TJYGeKTdQ/SvaqqSf9cGzcbi972/vHzcYG0yM7ajqQOBexiDgylFKPAT8GPgScP3ypT1C7ki7a7CvTOVtL12BXoOMBaO5v5s5X7qRrMPI1GCINHH3H97N8MYcGWjFhUGYriPowHosdtzkLo6eF7UedLJ5hUGD1FRbtHIx8KpvFZHBWdQGv7m4jHeuFpbOJ+rvp3B8mi/TDsYkkG5tIHRLnIhbRnNmpB5boafKpItrsK9M5W4vT4wx0PH7N/c04PfLBJqMc2wS5FZBdwmFHK2W2AswqtulIzuwinO0tuL2waIZBgdV3tqcz2nU7tYU0HOlmb2sfCyvzYmqLiN5E/d107g+TRfrh2ESSjU2kDolzEYtozvntACoT1ZBUE232lemcrcVqslKdE1wMsjqnGqtJFm9mDK19yQnKFgFwYOBETMkJ/Jz2IsynWsmyQE2xwm7KxaysdAyemPjOI5xbUwjAq3vaYm6LiN5E/d107g+TRfrh2ESSjU2kDolzEYsJX81KqeeUUs8CpcBOpdSflFLP+i+Jb2JyRJt9ZTpnaynOKubeK+8NdED+ObTFMaQlFimq56ivoGj5Ytzaw1FHO5UxrNfxG7IXUehsZ16FgclQKKUosBZHfWanJNfGjCI7b+/viLktInoT9XfTuT9MFumHYxNJNjaROiTORSwimcb241gPrpTKAt4AbMOPtVZr/S+j9vkCcA9wfHjTL7TWD8f6mPESbfaV6ZytxVAG84vm88S1T0h2lEx17F3fz/IlHB/sxK09kzqz06JKOYd+ziofBHyFdfOtxXREsWbHb0lVPm/t78Dl8WKRb2OnxET93XTuD5NF+uHYmM0GiyryeOrWi3B7vJhNBuW5tqDkBCJ1SJyLWEw42NFavw6glLpba33XyNuUUncDr49z9yHgSq11n1LKAryllHpea71p1H5Paq3/Lsq2J1y02Ve8Xo3L4/VVB3d76OgbBHxv7i6PN+wbfrQprlORoQxK7aXJboZIlGObwGKHwlkc6v4AiC0Tm99eZxnnAEtzO3DhW2uTbyli36ntUR/rzKp8XtrZyvamHupmybd7UyVc/+h2e2nrG8Ll8WI2FFkWg0GXh7beQbyaMSn5RfxIPxwbrbXvMvy7x3M6hi1RDH4y4b08HUici2hFk6DgL4C7Rm37aIhtAcPJDPqGr1qGLxmZ4MDt9rK7tTcofeX9Ny7Dalb8zSONYdOvSopWkRYOb/Ct1zFMHBrw1dipnMSZna19lXwKqNAdNDEHgAJLMQPuXgbcfWSbcyM+1uLqfAA2HeySwU6ShewHV9fReKiD+ZUF3LVuu/RzIqW4XB52t/Vx+3DMrlhSzleuWhC4Hi4V9WjyXi5E6opkzc7tSqn3gYVKqe0jLoeACb+GVUqZlFJbgTbgJa31OyF2WzV8zLVKqZlR/xUpIFT6ytuf2ILJMI2bflVStIqU19cG7bugcikAhwZaKDBnk2POiulwWsPbPb5cJ7mO9sD2fH9GtiinsuVnWagtzubtA7JuJ9lC9oOPN3LlkqrAQMe/Xfo5kQra+oYCAxuAVXUzg65Hmopa3suFSF2RTHL8DfAx4Nnhn/5LndZ69UR31lp7tNbnAjXABUqps0bt8hwwW2u9FHgJeCTUcZRStyilGpRSDe3t7aF2Sapw6StHf6EzOv2qpGjNXKkesxE79Ibvp3+w42id1Fmdlj4Th4aKcCpL0GAn1vTT4Fu303i4myF53cQsHvEarh/0ai39nIi7eMSs2xscm4V2S0ypqOW9XIjUFclgxwScAv4W6B1xQSkV8ZwRrXUP8CpwzajtnVpr/1cmDwN1Ye7/oNa6XmtdX1ZWFunDTplw6Su9oybtjU6/KilaM1eqx2zEDr0B1hwoOQOtNQcHTkxqvc6udiug6LcWkjMw4syOJbYzOwBLqvMZdHvZduxkzO2a7uIRr+H6QUMp6edE3MUjZs1GcGz2OFwxpaKW93IhUlckg51GoGH4ZzuwF9g3/HvjeHdUSpUppQqHf7fjW/eze9Q+VSOufhzYFWnjU0mo9JX337gMj9czbvpVSdEqUt6h16HibDBMdLv6OOV2UDWJNJ+72i1kWzwMZeWTO3C6Po7dlIPFsNExFF2tHYDFlfkoYOOBzpjbJSYvZD+4uo5Xdp7g7lVLpZ8TKac818b9I2J2XeOxoOuRpqKW93IhUlck2djmACilHgKe1lr/cfj6R4FPTnD3KuARpZQJ38DqKa31eqXUvwENWutngTuVUh8H3EAX8IVY/5h4C5dZZWS2oZGZWhaU5fDkLctxezVmQ2ExKTxe+L/bL8bh8gT2HX2Mkhwr/3f7xeNmbAvTQBhoB7cTzFbILgPDwKu9dA12+dIyGlYMw2DQPSgpGkX0uo9A92GYvwLwTWEDJpV2eleHlTkFgwxkFVLVvS+w3V9rJ5b007lZZmaVZLPxQAdf/cj8mNsmJmdkGl9/NrZsq0HxmVUoBU/eshyv1hhKYTMbdPY7KbJb6Ha4MieDVZh+OapDjOzDTVYKbYX0DPVIqt0EsFhMzC8Jfu8usVuDrvsHOs09jrAZ2jIi3XocYnfSTZDYFwkQTTa25Vrrm/1XtNbPK6X+Y7w7aK23A+eF2P7PI37/FvCtKNoxJcJlVjmjNIc9bX1B2YbWrK5jQVkOe9v7ufflvfz1xXOCsg7dc/1S/uOFPbT3DY17jImyvYxqILTthN991lfwsbAWbvgt3rJF7Dt5gDtfuZPm/maqc6r5wSU/4GdbfkaHo4N7r7yX+UXzpbMQkdn/Z9/PKt/L+NCAbyASa0HRbodBa5+Z+soe+o1C7M5TmDxDeEy+DxP5lugLi/otqcrnz7vaGHR5yLLI1JFkMZt9HwR3t/by3NYmrj1nBnc8sSXQ19134zL+sO04H15YwRt7WvnYuTVBfWFaZ7AK0y9TviTiD41e7WVf976gPvynV/yUNVvX8GrTq4EiitKPx8fgoJt9nf1B2dfuX12Hy+Xir9a8Q02Rnd/efCEnHe4J37OjLVeRUuIQu5NugsS+SJBooqVZKfUdpdTs4cu3geZENSzZwmVWCZVt6LbHG2nvd3Lb442sqps5JuvQN9Zu57bL5014jImyvQQZaD/dKYHv5+8+S5ejPdBRADT3N/OdDd/hS2d/ieb+Zu585U66Brvi9F8SGW/fi5BbCQU1ABwaaMWqzBRb8mI63N5OXwHROYWD9NsKAchxnM6ilm8pomPwBL6s9dFZUl2A0+Nly9HumNom4sffx11fXxsY6ICvr7vjiS1cX1/LXeu2c3197Zi+MK0zWIXplxmIfPF812DXmD78H179Bz4x/xOB69KPx0+nwzkm+9rtjzdSUZAduD7k1pN/z051cYjdyZLYF4kSzWDns0AZ8PTwpXx4W0YKl1lldOaW0dvDZXIptFsmPsYE2V6CuJ2nOyW/nqM4ve5AR+HX3N9MgbUg8LvTk6YfJMTUcg3Cwdegph6U71v2Q44WKm1FGCq2b90PdVswlKYq18nA8GAndyA4I9ugZ4ABT1+4Q4S1uCoPQ8EmWbeTdP6sbCZDhezr/NvD3Z62GazC9Mu4I+9znR7nuH24/7r04/Ex3nu6n6GY/Ht2qotD7E6WxL5IlIgHO1rrLq31V7XW5w1fvqq1ztjhdbjMKqMzt4zeHi6TS4/DNfExJsj2EsRs9Z1mHqmwFqthpjqnOmhzdU41J50nA79bTbJgUkTg8FvgHoQZ9YFNB/tbJpV2+mC3mYocJxaTZiDLf2ZnxGDHUgLElpEt22pmTmkOGw/KYCfZ/FnZPF4dsq/zbw93e9pmsArTL2OOvM+1mqzj9uH+69KPx8d47+l+Xs3k37NTXRxid7Ik9kWiRFJU9GfDP59TSj07+pL4JiZHuMwqobINrVldR1mOlTWr61jXeGxM1qF7rl/KmtcOTHiMibK9BMku882n9XdOw/Nri+1l3HvlvYEOw79m53/e/5/AfNfiSWTSEtPIvhfBZIPKswEY9DhpHuqaVCa2Q90WqvN8Uz8c1jy8GEEZ2fKtvoFULIMdgMVV+bx3tAeHM03PDGQIfx+3tuEo9924LKivu+/GZaxtOMrdq5aytuHomL4wrTNYhemXyY48LXJxVvGYPvynV/yUZ/Y9E7gu/Xj8lNitY7Kv3b+6jtaTA4HrNrOa/Ht2qotD7E6WxL5IFDXR3HilVJ3WulEpdVmo27XWryekZeOor6/XDQ0NCX+cibKxuT1ezCOysrhcHtr6hvB4NSZDDRcUVVgtikGnN6JjRNnACbOxWQwLaM2gZwiLYaY0qxSz2RLycG6vmw5HBy6PC4vJQqm9FLMRTQ6LjJCQVdFTFbNx4/XCT8+Ewv+fvTePk6uqE/afc2vpql7Se6fT6SQkpEMIJJAFBCOYsIy7qKjgsDjihqhI8OPMqDjIiL6vw0/CBAdxAQcEYRxQgwuvCyYsIQgJhISsnX3pJL3v1V3LPb8/bld1LffWcruqu6r7PB/4dNWpc889lf72t+6pe85zZsFl3wJgb/9xPvr697h59nu5sGJBxk12+TQ+90wdH1zQzjtmGd/UvW/LDzhVs4QXl30FAF9wgAf23MHH5t7ClTM/nvE53jjaxX/8aS+PffptvKOpJuPjC5C8jdf4fCgESAkupyAQlCMzIwW1JW66h4KFa7CKJz4ve6vB15GR4crKSKXrOiEZIiRDODVnJEfH189zY1XexezQUJAOnz/GxtYXCMXEpK7LhM9sTROm1wgFSygI/acgFACHy1iv6cjONUC6MZqOjU2X+nhfqxTwL1UB6amnw3vpOIGXpZS+ZPUnE1ZmFadTo6Ei9pa2rkv2tw/E2Nu+f/USHnn5EGuuPCvBLmTWho0OQun0xGKhUeOtQQ8Fae7ax60b10TMJutWraWpcgFaXAIL6kH2de1jzYY1MRaUBZULpuKAR9HyOvS1wHnXRIrCJja72ulDXUYczSwdXdQ7UFRBadQ0No+jGLfmsX1nZ2H9NDQBmw+2T5XBTl5ilg9/euMKmmpLaW7rTygvWPuaGdF52abhKpzDo6koqjDN0U0VTRzsORhjsFLGqvTRdcmhrsGUMalpIuYz28rYWrCxrOvQticnNjYzy5pVjJrFfvRzda2isEMmEXwj8KYQ4hUhxD1CiA8IIexP3p9kmNnb/uXp7Vy9fNaE2YU6fW2RgQ6MmEw2rqHTl2hXafe1R5JHuO6aDWtojzJlKaYQu9aD5oTGt0WKDvlOI4Dptgc7xh3FGWVRgx1PJaWDpyPPhRBUuKtpt6mf9rodnFlbyisHJ+1ywoIgmc3SrLxg7WupyKLhKlmOjjdYKWNV+ljFaqqYtHtc3pJDG5uZZc1ujKprFYUdMhEUfFJKuQD4CHAM+C9g/JyEeY6VvS1sZ5sIu5CVmc2vBxPqBkIB07oBPZDTPiryECmNwc6M86CoNFJ8aPA0te5y3Da/PTvY5aK22I/HOTp1drCoguKhLrSoOAvrp+1y9oxpvHmsm4HhxDhXjA+WNssRS1t8ecHa11KRRcOVVY4OWuV5ZaxKC6tYTRWTdo/LW3JoY7OyrNmJUXWtorBD2oMdIcT1QogfA08BVwA/BC7JVccKDSt7W9jONhF2ISszm9nFqsvhMq3r0szX9ygmMSffhO4jMGdlTPHBwZNjNrE1lMXuSzFQVIFAUuIbNaiVuSvpGDpla68dMDYXDeqSLUfUfjsThaXNcsTSFl9esPa1VGTRcGWVo51WeV4Zq9LCKlZTxaTd4/KWHNrYrCxrdmJUXaso7JDJNLb7gPOBnwK3Sin/Q0q5OTfdKjzM7G3fv3oJT289NmF2oSpvLetWrY0xm6xbtZYqb6JdpcZbw9rVaxMsKPFzZxVTgN3PgHDArNEpbLrUOew7TX2RPQtO37CgfdDJzLLYb/JGNxaN1k9XMaz7GAj22jrXWfVlODTBZrXfzoSRzGZpVl6w9rVUZNFwlSxHxxuslLEqfaxiNVVM2j0ub8mhjc3MsmY3RtW1isIOKW1sMZWFOAe4FHgH0ATslVLekKO+WZJts5WVdS263DXi0/cFjMdm9rRow5pDExQ5NSQiu4YWCwObZfVQkE5fG349iMdRhC51/HoAt+ZCc7gZCg0lGk70AC7NhVsbeV1zo2kaQ0HjcZUELeCzNMAVgA0oGXlnChpXpIQfrgB3KfzD3ZHiE0MdvPvVb/FPM6/g0upzM252x2k3/76xis+c38KC6tGpHyVDXbxv6728dN4t7J99OQDNvTt45ujDfPP8HzOn9Cxbb+Pbz+zE49JY/6V32Dq+gMibeA3nS13XCeoSl0MQCMmRx1qMdU0IgUOApmmFb7CKJz5He6pgYGRdmpSABGcRFNeiC1Kap6LzaCAUoN3XTlAP4tScVHuq6Q304g/5ESOhIITId5Nm3sRsGDMbm8eT+t/P6toh7zC7boD0yuKuL6KvKdyakypvbYLwyOx6AEjrGiE+xmu8NbgcsXdtIubYkWsVZWNTpCLt6BBCTANmA3OAM4ByoOC3D7YyqphZg+756BL+4//tpa1/mAevX87C6WWRAY+uy9xbhmyYfTSHk5rSGaZmtrtX3s19r99Hu689YkapL6k3NafE1L34Lpqe/SZafytc+wR67UKaew4oG9BkoHU3dOyHt90SU3xwxMRW77E3je1Yj5FqZpTG3tkZdE9DR8TstVPuMj4Y24dO2h7snD1jGr97s4W+oQBlHjW9IdeE8+jav+zlk2+fyyMvH+KTb5/Lvzy9PZIPH7x+Oeue28efd7UWvrnKCrMc/fFfwI6noekKeOZLkXL9+t/QrIUieXN142puPv/mGMtUdB7VpR5jXTOrf9fKu/jlrl/yxaVfVPk3TYaHgzR3DPCFx7ZGYvVH1y9nQU0JRUXJL5GsjK15hVlMXv8bY8Nos2sJE8NrpKk0DK/JzGup7r4EQgGau5tNjYPRAx6n5qS+pD47/z6KKUEmmfAl4APAduAaKeVZUspP5qZb40cm1qCvPbWdm1edyfEuHzc/tpXW/uGU7WTVzDIGW4qZme2OTXdw0+KbEswoZuaUmLqb76Tz0q9Gzt/pa1M2oMnCrvWAgDkXxxSPaqftTY053uuk2Bmi1B27eFdqDnxF5TH66Wlu4xx29dMA5zRMIyQlWw6rdTvjQTj/Xb18VsRCGR7oAJGcefXyWZHnBW2ussIsR//qBlh63ehAZ6S8s+dITN68qumqBMtUsrxsVv/OTXdyVdNVKv9mQPugPzLQASM2v/DYVtoHJ0lsmsVk10Fb1xLpGF7HYl5TpjVFrsjExrZESnmLlPKXUsrj8a8LIe7PbtfGByujSsDCGlThdUUeB0N6ynayamYZgy3FysxW7i6PPA6bUazMKTF1i6si57e0vikbUOGxaz1MXwTe2Ds4hwZPU+b0Uua0tzfU8V4HtSX+kY0kYxkoqoi5s+NxePE4im3rpwGappfi1ASbD6p1O+NBOP+F7ZPhn9FE58/w84I1V1lhlaM1R0K5v6gkJm+Wu8uT5tH4vGxVP1yu8m96BHVpbg7U7QlS8g6zmHQV27qWSMfwOhbzmpVZMGhikFUoMiGb97hXpq6Sf1gZVVwW1qBuXyDy2OnQUraTVTPLGGwpVma2Hn9P5HHYjGJlTompO9gZOb+l9U3ZgAqL9mZo251gYQNjGlu9276J7USvk7oSczXoYNxgB4ypbO1juLNT5HQwv66Ulw+obwTHg3D+C9snwz+jic6f4ecFa66ywipH66GEcvfwQEze7PH3JM2j8XnZqn64XOXf9HBqwtwcOFmmV5rFZGDQ1rVEOobXsZjXrMyCebz+TFEgTPkJvZlYg+75n3GnqQAAIABJREFU6BIe3HggMv+8rrQoZTtZNbOMwZZiZma7e+XdPLzj4QQzipk5JabuxXdR9cIPIuev8tYqG9BkYPczxs/ZFye8dMh3ihk2f599w4LeYQd1xebf7A0UVVA81ImI+vauzF1Jxxju7AAsapjGrpZeenxq/4VcE85/T289FrFQfv/qJTH58MHrl/P01mOR5wVtrrLCLEd//BfwxuPwwR/GlFeVz4nJm+ub1ydYppLlZbP6d628i/XN61X+zYCaYjc/un55TKz+6Prl1BRPktg0i8nKebauJdIxvI7FvKZMa4pckZGNLWlDQrwupVyWlcZSMF42tmi7msuh4dAEQ4EQHpfxbWQgpFNcpDE4rEeMQ0VOwVBAz52ZJUMbWzTBYID2oXYCehCX5sTj9DIYHBy1pYRChmVFBvE4POhC4B+xnUgpGQ4NG3YUZzGuoV5lY8uAgrCx/WQV+AfgfffGFHcH+rlk8z9zzYxLeVdt5n/iu9tc/NvfqvnUeSc5u2Yw4fW5p7dywf7f8tTlD9BfbCyO3XDyN7zV9Sr3X/wswmzuWxrsaunhO3/Yzc9uXMEVi6wX3RY4eROv8TY2TYAuDQGZEMb/UoLLIQjqmBotC5ro3CyEoW/XtFEbWyhoTGcTGkgdXF6CReW0D3cQCAVwOVxUe6rp8feg6zo6OrrU0YSGS7gIyEDkuYaGpmkx9rbo8jzPvxMas4FAyPhcHzGv1ZUWEQpJWza2giEUhP5TEAqAwwWl9cYfY/8p0IOgOY0yZ2qZS/x1RI2nBmfccWOxsfmDfjqGOiI2tipPFX2Bvom+tpgkt/mmLtn8ay7YYDAzqljZ1aItbW+fV831F8/hlsdfj7G4LKwrxeXK0fQMTUtqS7FClzoHeg9aGtP0YIDm7n3cuvH2KMvKvcyrOJPmnoMJdpQF5U0xCU4Tmvr2pZDpOQ4tb8CyROfIoUFDmzvD5oaiJ3qNNDO9xOrOjtFu6WBrZLBT7q7Grw/RH+yhzFVh67zz68pwOQQvH+iYzIOdvMEsj/r9Qfa2xZquHrhuGX948wQfWjZr8tjYrEyZtQuhbU9s+VUPwHPfRi+t58AV34w1W122jjMrzuRA94EY69rnz/s8t0fn5qjcrfJu+gQCIfa09ieY18q9Tv7xp3+PMQdG21YLGl1PjMHrfwOBAfif62OtgdPPBYf1ZWGq64gw8XGZzNAWfZwudQ71Hoqpt3b1Wh7c9iAbjm9QpleFbbIZLf+ZxbYmnHQsbZ+9dF5koBOu84U4S1u+kMqQYlhWbo+zrNxO+1CnuR1lSK2FmFTs+YPx02wKW1g7PQYTm0vTqfCYLzId8BiDmdIoE9C0KP20XdxOjbPqy3hpf2pboSI3tA0kmq5uefx1Prpi9uSysVmZMvtPJZavvwVW3kbnsusSzVZ/u5V2X3uCde32+NysbGu2aO0fNjWv+YMypizetlrQWNnYwgOdcNmvbjDiNQl2TWvpHmdWb82GNVzVdFVG51Mo4kl5Z0cI8TvAcq6blPKDIz//O3vdmnjSsbQ5NFEwFpdUhhS/tLKghEzLA8qOMrnY/Tvj273yxoSXDvpO4RJOatxltpo+3uugtjiA1Rf4vvBeO76ovXai9NNzy862dV6AxQ3lPPHaMVp7h6ib5rHdjsIeVqarcO6cNDY2KwtbKGBe7q3E73Ra5NZARpY2RfpYxWN8boq3rRY0mdjYQsnXN9o1raV7XCoTbLrnUyjiSefOzv8H/CDJ/5OSdCxtIV0WjMUllSHFLawsKA7Tcpeyo0weBjrgyCaYlXhXB4xpbPVFlbanDRgmNusPJ11zGnvtmN3ZGaOkYHGjcdfoxWZ1J3IisDJdhXPnpLGxWVnYHC7zcl8X7sFOi9zqysjSpkgfq3iM/34y3rZa0GRiY3MkX7Nj17SW7nGpTLDpnk+hiCflX7OU8vlk/49HJyeCdCxtP33hIA9ctyzB4hJtacsXUhlSDMvKvXGWlXup8VSZ21E8ap74pGHfs8aC6Tnmg52Dg6eoL7K3bmYoKGgfdFqu1wkzWFQeo58ucnjwOkrGtLEowJzqYsq9Ll7arwY7E0FtSaLp6oHrlvHUlqOTy8ZmZcosrU8sv+oB2HQfVa8/nmi2umwdNd6aBOvavfG5WdnWbFFXWmRqXnM7RYI5MB8/x21hZWO75rFEa2BpfdKm7JrW0j3OrN7a1WtZ37w+o/MpFPGkbWMTQjQB/wdYBETmg0gp5yU5xgO8ABRhTJl7Skp5Z1ydIuBRYDnQAVwjpTycrC/ZMFtFG9i8bgdBXRIIxlrUrCxt0TYXt0NDEzAU1HFqgmK3xqA/DRublVUt2priGvkGKhSI1NGlbhjT9CBuzYXmcDMUGjIeS8lQaBi35qTKW4sWt9Aw2pDicXgYDg1HjCc1Di9CStp1HwE9FGNrK3YWMxT0JdhXYtpzetB1Hb+e2phi97hxIm/sVuPG/9wARzfD1Q8Tv+vnsB7gwpdu4/11F/KhevPBUDIOdDr517/UcMPiUyyuG7Csd+G+p6jpP8FTV/w4UvbYgXup8czgK+f8R8bnjeb+vzWz73Qfr37jismxGD6WCY/X+DxZ6XXR5QsQ0nVCuiQkJQ4hJo+NLZy7dd0wWekhY1F3SR0MtALC+PIgbF1DgtDQHW460RnSA2iaA49wUoaDDuknKEM4hHGXSwgRsbL5Q34EAodwEJIhJBK3w025u5yOIcPi5tScuDU3UsgYO1uyfDrB9swJjdnh4SDtg6PmtbBiOrqs2uumLxBK+OzPS+KvJbzV4OuIvbbQQ4nmNXTob40qq0N3uGLjwl2BFtfWsB6gc6gzxpZW5IwTPJnEVzAUjLGsVXuqcTqcCfVCeoh2X3tMvd5Ar7KxKcZEJnORfg7cCawFVgOfIvWdoWHgMillvxDCBbwkhHhWSvlKVJ1PA11SyvlCiGuB7wPXZNCvjNF1yd7TfXz20S3Ulhbxz+8+i689tT3Guha2BMXbhYJBnb2t/dwcZxd6fk8rK+ZWWbYT1wFzc0/NWdC601goWFoHl3/bWMw6Uke//jc0Cz+3RpnR7l55N/e9fh/tvvaYx+tWraWpckHMgCdsSAmEAjR3N8cY1u5bvRaPVsTNz91i2na8ASXarlLjreG2Zbdxx6Y7kppWxnKcIkeEAnBwgyEmMFE8H/G1oiNt77ETNrFZ7bETZrCoguL2HQg9hNSMi75prsox39kBWNJYzssHOthzqo9FDdPG3J5ilOhcerzLxz8squPWyxew7rl9fPLtc/mXp7fHWK/uf24ff97Vmjw/5jPh3L3he/C2z8MzX4q1We14GpqugL//OOZ1feH7ab7ymzG5+9533stpAbdFWdbuWnkXv9z1S24+/2bml89n/+D+BBPm/PL5Cfn73lX3su30NpbWL40ptzJepWPGmowEgzrN7QMxn9+//Ozb6PEFI+KCf1hUx5cvXxBjbMvbWDW7lvj4L+D5/4C9f7A2r13zGDg88MuPjl5f/OP/0uwUsXGxai1Nf/0u2p7fQ8Vs/J9+jgP+dtOYdI9sSGoVX07NyS1/vSXmuGnuaXz6T5+OlD30rofo9fcm2l8rF6iNRRVjIpPM5pVSPodxN+iIlPLbwPuSHSAN+keeukb+j7+VdBXwyMjjp4DLhd2NNdIk2rR286ozIwMUGLWuWVmCWvuHI4kyXP+Wx1/nqmWN6beTzNzzqxuM5ytvGx3ojNTp7DkS+bAEY6HeHZvu4KbFNyU8vnXjGjp95haqdl97gmHttg1rOD7QYtl2vAEl2ppy0+KbIgOW8LFWxhS7xylyxPEtMNwHM5ebvjxW7fTxXieakFQXJ1/4OuCpRJM6JUMdkbJprio6hk4x1r3AFs80puApK1v2ibdWXr18Fjc/tpWrl8+KDHRg1Hp19fJZkecFaWML5+7zPzE60IFRm9XS64zyuNc7l12XkLs7hzsjA51w2Z2b7uSqpqtYs2ENHUMdpiZMs/LbN97OO2e/M6E8XePVVMm7Zp/f/qCMMbRdvXxWgrEtb2PV7FriVzcY8Rd+bmZe+5/roedI7PVF34nEuNi4hs5l10XqdIigZUyGsYqvlv6WhOOihQRh8YCp/dWnpiErxkYmg51hIYQGNAshviSE+DBQmuogIYRDCLENaAX+IqX8e1yVmcAxACllEOgBqk3a+ZwQYosQYktb29guWqJNaxVel6mdxcoSFG1ji64vpbnlxbQdK3OPHhwt91Ym1PEXlSQ1lcQ/9lsY04K6uXnN6/QmlMW0F2VAiU5SmdiC7B5XiGQzZnPG/r8amx/OOM/05YODpxDA9DHssVPtDZBqttJAUVg/HWtkC0g/vYGxXYRVlbhprPQqSUEK7MRrvLUynE+t8mqF1xXzvOBsbOHcbZKf6T5qbBpq8rq/uCoh13mdXst8bpgwrQyZ5uW61MdkvCrEvJtpzJp9fmsC0xiOJm9j1epawhuVr63Ma67imCKr6wt/8ehdfSs7azDqWsMqvsyuL+LvJGpCszQUKhRjIZPBzleAYuBWjPU1NwCJOxDGIaUMSSnPBxqBC4UQ59rpqJTyJ1LKFVLKFbW1tXaaiBBtWuv2BUztLFaWoGgbW3R9IcwtL6btWJl7NOdoua8roY57eCCpqST+sdvitq9TMzev+YK+hLKY9qIMKNHWlExsQXaPK0SyGbM5Y/9fjI0P3ebfWxwaPEW1expFWuqdtc043utIamILE9lYNEo/HTayZWMq27kzy3n1UCdDgTy8YMkT7MRrvLUynE+t8mq3LxDzvOBsbOHcbZKfqZhtrI0wed3MvOYL+izzuWHCtDJkmpdrQhuT8aoQ826mMWv2+a1LTGM4mryNVatrCV/X6HMr81pgMKbI6vrCPTj6ZZOVnTV6iplVfJldX+gyVu+tS93SUKhQjIW0BztSytdGpqT1ArdKKT8St/Ym1fHdwAbg3XEvnQBmAQghnEA5hqggZ0Sb1h7ceIB7ProkwbpmZQmqKy3iQRO70PrXj6ffTjJzz8d/YTzfdJ9h7YmqU1U+h3VxZrS7V97NwzseTni8btVaqrzmyb/GW5NgWLtv9VoaSxos2443oERbUx7e8TB3r7w7LUOL3eMUOaC/DU6+CTOXWVY5MHiShqKEG61pEdThdL+TuhRT2AB8RdOQiBj9dHivnbHqpwGWzCxnOKjz2uHJP1VnPIm3Vj699RgPXr+cp7ce4/tXx+bDH42Uh58XpI0tnLu3PQEf/GGizeqNx43yuNerXn88IXdXFVVxX5xl7a6Vd7G+eT1rV6+l2lNtasI0K7931b08f/T5hPJ0jVdTJe+afX67nSLG0Pb01mMJxra8jVWza4mP/8KIv/BzM/PaNY9B+ZzY64uymYlxsWotVa8/HqlTLZ2WMRnGKr4aShsSjoseGIUH3Kb2V6+yvyrGRiY2thUYkoLwzoI9wE1Syq1JjqkFAlLKbiGEF/gz8H0p5e+j6nwRWCylvHlEUPARKeXHk/VlvGxsVgSDumFjC+k4HRrFbo2B4QzbmQAbWzSBUCDGeFLj8OII+emUQfwyFNu2hQFF2djSJy9tbG8+Cb/5PLz/Pqien/ByUIa48KXbuLzmfD4+45KMmz/e62DNs7Vcs+g0y2f0p6z//tfuoaVuKS8tvRUAf2iY+3f/Kx+e81neM+u6jM8fzVAgxGcf3cKn3zGXr7/X/ialeciEx6uVjU3XdYK6JKRLHJqgtsRNz3CBGK6SEW9jkyHjrnzJdBjqHC2PtrE5i9C91XQOdzMUGkITGh7hZJqEbkL4kQgExn+CGm8NTs1JUA/S7msnoAdwaS7TcqdQNjZIP2bjP7/rSouQUkYMq85Ci9V0bGxSH72ucLiML1alTDC06Q5HShubX0+0qoXlBJEumcRXvGWtxluDQ3Mk1NOlbhrzE0ye/vIV6ZJJBD0M3CKlfBFACPEOjMHPkiTHzAAeEUI4MO4i/UpK+XshxL8DW6SUzwAPAb8QQuwHOoFrbbyPjIk2rYU/rNPF6dRoqIi9zV1RbFHZugNQOj2xXGhGMpLSSEB67LQbzeGkpnQGIx0fSXIhcDiM+eI6xvGDHRDyo7u8dAqMwYTmpkqCFvDhcrqZUTzd6EdUsqxxuqGk3ihP9RZG7G7xpPogtTpOMc7sfw48FVBlbo8/7msnIEPMtHlnJ2JiK0lvvnW/p4qygdG7OG5HEV5HaVbu7HhcDhZML+OF5ja+zqQa7Ew4ZtbK2rKihEGQ0+mg1j3hFy3WWH0BFU84d8cPenpPjCioRwY9Qb9hOHQUQciP1ndyJL82xLSbLBM6NSf1JfWRnNo62BrJqfUl5nuiROfW8IVjfC6eyjnY7PNb16UxTa1QYjWa+GuJ4LARe3oQgoAeAM01el3hcBnXGQIjzoMjPzWHeVzEXadoaIQdUkIINJO/EbN2NIfGjPC1SxQJ9YRmGdsKhV0y+WsOhQc6AFLKl4QQ5ivgR+tsB5aalP9b1OMh4GMZ9COrxKtTJ1QzGa2RNFFPc+0TULdodIASr5y86gHY/iQsuRbW34JeWkfze77LrZvvHFVAXnwXTc9+E62/1WivdiG07UnUYIfPk+lbmMJa04JCSjj8ItQvNj74TDgweBKABpvTW46nqZ0O0++tZkZXc0xZubsqK2t2ABY3lvM/rx2jrW844eJckV3yKq+mg9V2AFZ5MJWCOlr9+7FH4YV7Rp9nmF/t5lSVi9Oj4GI1GcFhaN09anUNx2NZAzx0xWjZ9b+B4FDGn/tBPci+rn1KDa0oODLJeM8LIX4shFglhHinEOIBYKMQYpkQwnrSf54Tr06dUM1ktEbSRD3Nk58w6sTXDb++/ha4+MuR4zov/WpkoAMjCsjNd9J56VdjdddmGuxBe/awqaw1LSi6j0DfSZh+jmWV8GBnRpH9PXYqPAGKnOlNle33VOH19+KKWjg7zVVJ+1BLkqPSZ/FMwyy4ab+ysuWavMqr6WC1HYBVHkyloI5W//7vjbHPM8yvdnOqysXpUXCxmoz+1tGBDozGY2gotqzroK3PfbNtK5QaWlEIZDIUD7tp74wrX4qxd85lWenROBOvToUJ1ExGaySt1KZBf2Ld6NfD6lPMdacxKsnuo8Yc3mTnyZDJpDWd1BwdcYvUJRnsDJyixjUNj01L07EeR1pygjB9I4tcywZO0VlhTK0rd1dzoG8nugyhibHZkOZWl1DmcfJCcxsfWjpzTG0pkpNXeTUdrPKpVR5MpaCOVv+aPc8gv9rNqSoXp0fBxWoyorevCNN9NGE6vKWOOkVcBkIBpYZWFCSZ2NhWJ/m/IAc6kKhOhQnUTEZrJK3UpuGFgFbKybD6FHPdaYxKsmK2MX832XkyZDJpTSc1R142dNOVcyyrHBg8yQybU9h0CS19zrS002H6vcZgZ9rIHSWASnctIRmkY/i0rX5Eo2mCc2eW8/zeNnR9bBuVKpKTV3k1HazyqVUeTKWgjlb/mj3PIL/azakqF6dHwcVqMqK3rwhTMdv4EjQaKx11irh0OVxKDa0oSNIe7AghpgshHhJCPDvyfJEQ4tO569r4EK9OnVDNZLRG0kQ9zbVPGHXi64Zfv+oB2Hx/5LiqF37AuovvilVAXnwXVS/8IFZ3babBLra3L8xU1poWFEc2Qd3Zlut1QlLnsO8UDTansLUPOvCHtMwGOyMxMq0/arBTZMThad8xW/2IZ+msCjoG/Oxs6c1Kewpz8iqvpoPVdgBWeTCVgjpa/fuxR2OfZ5hf7eZUlYvTo+BiNRmldaPbV8BoPDo8sWWV82x97pttW6HU0IpCIBP19LMY9rVvSinPG9kT5w0p5eJcdtCMbGt8461BE6mZ1EPBGLV0lS7RgkPoTg+dTid+PTBq1ZGM2oPCppXgELq7hE5C+PUgHocHPyECoQAuhwsHGkOhYVwjumnncJ+huNZDEIqyEEF6ZiKz95CBWnqCFahmTLjKN+cMtMM9Z8KyT8JiczfIUV8r73vt23yq8UouqbKe6mbFGyfdfO+FKr6w7ARzK4fSPu79r93DienL2XT+l4yuBvt4cM+/cc28L3N5w9UZ9yOeHl+Amx/byu1XLuDWy5vG3F4ekLfxmk95NS1S2dii7WsyZOh8hQYOt7EwPKKgroOB1hH9tMPIrwEfusNFp5AM6UE0zYHH4aHMXUbHUEckP1tpdhO2CvDW4HIkfpsen09TqagnKP/mXcwWXKxGEx+3nnJjD7WIUroWhDNBPa1LaVxryCBuMbJdhTP1HRqzWIxXSJe7yxPiWhNaQqwBKeMvT64RCiQYFFZksmanRkr5KyHE1wGklEEhRAFOak3ETJ06EehSp7nnQKw95+K7OHPLLziw4oZYq1rYqlM63dib5/Rb8KsbYgxsNd4ablt2G3dsuiNy3N0r7+a+1++j3dfO2lX3smDTAzgPvZDa9JaBQSisnUxlA1K2oAni6GbjZ1I5gWFAG7OJLYM7O2Dc3Ym+s1PsKKVI82Ttzk6518WZtSVs3Ns6WQY7eUu+5NW0sdoOAJLb15LlzrPeB5d+Df2tX9O8/Fpu3Xh7JNfd+857Oa2d5rYNtyU1WwX1IM3dzSkNWJnmU5V/Rym4WA1jFm/v/Oc4G9tj4C6Gxz4SKdOv/w3Nws+tUTG1btVamioXJN2fT5c6B3sOxsTMg1c+iD/kjylbu3otD257kA3HN9BQ0sADVzxAUA+mPC4+/lSMKrJFJtEyIISoxpARIIS4CGNjUUWWMLXnbL6T9kvWJFrVoq06/aciyS3awHbT4psiA53wcXdsuoObFt9kWFQ23k77JWvSM73ZMLSlsgEpW9AEcfQV49u9auuL/QMDI9rpMZjYSlwhStx6Rsf1e6pi1uwIIagsqqPVd8JWP8w4b1YF245101WItiXFxJDMvpYsd57/CfjfG+m84MbIQAeMXNc53BkZ6ITLzMxW6RqwMs2nKv9OAsziLcHGdj10HYop6+w5EhnowMjvfuMaOn3JP+PNYuZ43/GEsjUb1nBV01WR5y39LWkdFx9/KkYV2SKTwc7twDPAmUKITcCjwJdz0qspipU9J+BwJLfqRBnVog1s5e5y0+PK3eUxbQPpmd4yNLSlsgEpW9AEcfglqDnLGPBYcGDwJFWuMrwOe992Hu/NTE4Qpt9bjXe4B2dw1I5U4a7J2p0dgPMbK9AlvNBsT6+umIKksq9Z5c6R+n7NmZDrvE5vWmardA1YmeZTlX8nARbxFkP3UcO+FoW/qMT8d68n3TrRNGas4jh8nWFVx+q46PhTMarIFpkMds4E3gO8HfgT0Exm0+AUKbCy57hCoeRWnSijWrSBrcffY3pcj78npm0gPdNbhoa2VDYgZQuaAIb74dSOpMppMAY7du/qSGnc2Ul3M9FoIpKCgVgjW+fwaQL6sK3+xHNmbSllHifP71WDHUWapLKvWeXOkfpuPZiQ63xBX1pmq3QNWJnmU5V/JwEW8RZDxWzDvhaFe3jA/HefYmNQs5ixiuPwdYZVHavjouNPxagiW2Qy2PmWlLIXqARWAw8AP8pJr6Yopvaci++i5sW1iVa1aKtOaX3EwBJtYHt4x8PcvfLumOPuXnk3D+942JhXu+peal5cm57pzYahLZUNSNmCJoDjrxkLqacvsqyiS52Dg6dsr9fpHdbo92tML8l874U+7+heO2Eqi2qRSNp82dlcVNMES2aWs3GfUlAr0iSZfS1Z7tz2BHzsUapee5R1q+6NyXVVRVXct/q+lGardA1YmeZTlX8nAWbxlmBjewwq58aUVZXPYV1cTK1btZYqb/LPeLOYaSxrTChbu3ot65vXR543lDakdVx8/KkYVWSLTGxsb0gplwoh/g+wQ0r5y3BZbruYSF6ZrVKRyvATXz3exiYFWmCQgKecdn14xIDioMZTbXyzF21jA8P64/LS6XDg1wOGjU33E9ADuDQXGhrDoWHDouLw4hruS24esmFji3k/KUwqeWJaiSbvTEFZZcP34IV74NonjUWrJhzztfPe1/6Nf5p5BZdWn5vxKXa2uvn2hio+c34LC6p9qQ+Iwhka5iOv3M3Whf/IjibDvnbKd5THD6zlCwu/w9KaSzLujxkv7W/nvzbsZ/0XV3LerIqstDlBTO54zQdCwRGTVdDYr8RZNGpfEw7DeKVpozkyOncKYdjaQn50h5sOQgzJIA7hwCVclBWV0TXUFTFbVXkq6Qv0J+TDoB6k3dceyeNW1rZM86myseUx6X4GR+JzxLRWUhNnY5tuWAPjbWwQda0xYmMTWspzmsWirut0DHVE4rjSU0n3cHdMHWVjU0wkmUxDOyGE+DFwJfB9IUQRmd0ZmnpkajXTdbS2PdTE1Q/WNNHccyDRxqOV4Hzk/Qlta5pGDeYmk2gbW1KrSTIzUQaEzWx2X1dkmSObjG/5LAY6AAdHBAH2TWzGOjA7a3aCjiJ87jKmRd/ZcY/stTN03FZ/zFjSWI4ANuxtLfTBjiKXRJkuR+1Wv4AdT0PTFdZWNrPcKXU6o/Lx6sbV3Hz+zQl5PdpiFc7RTs1JfUl9yu5mmk9V/s1T0r120HVo25PcxnbtE+D0wGMfjinT6hZRUzojo3PqUudA94EEq9pQcCjBKthU0ZSgRzeLtVTxp2JUkQ0yGax8HGOtzruklN1AFfC1nPRqspCp1cyifvtQh7mNh2DSts1MJtE2NmU1mWKEAnB8S9IpbGCs1wH7g50TvU6KHDrlRfbM9P2eqpg1O0UOLyXOabRmUVIwzeNifl0pG9W6HUUyokyXwIjd6gZYel1yK5sJ8fn4qqarTPN6tMVK5egpSrrXDunY2J78BHQdzLytNK8pjvcdT8sqqFBMJGkPdqSUg1LKX0spm0een5RS/jl3XZsEZGo1s6gf0EPmNp74u0NxbVuZTKJtbMpqMoU4+SYEh1LLCQZOUukqpdjhsXWa471Oakv8CJs3/vu8NUzrj1VNV7prOeXL3p0dgCWNFbx5rJtOpaA6wZ6FAAAgAElEQVRWWBFluozQfdSYzpahsTI+H6eyZYafqxw9BUn32sGmjS2ttkzqZWJjC6YwuykU44mahpZLMrWaWdR3aQ5zG4+uJ9SNbtvKZBJtY1NWkynEkZeNn0k2E4WxmdggbGLLXE4Qptdbh9ffS9HwqM2noii7+mmA82dVIIEX9qm7OwoLokyXESpmgx7K2FgZn49T2TLDz1WOnoKke+1g08aWVlsm9TKxsZmtKVMoJgo12MklmVrNLOrXeKrNbTw4k7ZtZjKJtrEpq8kU4+jLMK3B+PbPgrCJbYbNuBgMCDp9DlvrdcL0jMRwRf/onZxKdy19gS58wQHb7cYzr7aEaV4nG/a2Zq1NxSQjynQJjK7ZeePx5FY2E+Lz8frm9aZ5PdpipXL0FCXda4d0bGzXPgGV8zJvK81risayxrSsggrFRJK2jS2fKCjrSjpGleg6Lq/xrWHIb1h8NAcEfAQ902gPDhDQQ7hGbGzOKBub7vLSKcCvxxpLYkwmmhtN0xgKDuWL+SxfTCvRTE5TkK7Df8yFxgtg5Vcsq50Y6uDdr36LG2dezqrqxRmfprnDxTf+Ws2NS05ybu1g6gNMKB7u5v1bfsDmxZ9j7xnvMtrt3c4zR3/ON857kDPKFtpq14wfbdzPtmPdbP3WlbgcBfndz+SM1/EkVY4O267A2EQKaditRixrSGkc560GX0fSXB+f78pd0+gY6iCgB3FpTqo91fQEehPy4XjlyXE6j4rZdIi3rJXWg8PkbkkwYNSLtq8NdcXEoS71RPOaWVtpXK+YxUhID9Hua4/Y2Gq8NQlygnTJw2sCUDa2gkfdZ8w1qaxmVgaU2oWjlpXSOpyXf5v69bckWlJKp5ta16JNa/n6DUuqfiuySPs+GOpOawobwExPta3TnAib2MYwjW3QXU7AUUR51J2d6iLDRNUyeCirg50VZ1TxQnM7rx7qZOX8/Pw7UeSQdKxXDieUNSSvl6Y9yywf10cbsYAaZ+zr45UnVT7OI+Ita1Y2tlAQWncm2gKnnxsZGOlSpznOoGb5e03DwmoWw5pDY0ZcHNt62yoGFTlCRc9EY2VA6T81Wr7yNggPdKLrjFhSzAwphWDxKdR+FyRHR9br1CU3se0f+V3YXbNzrNeJQ0iqvfYHOwhBr7eGir7RNToV7hqcwsWJgUP22zVhSWM5RU6NP+08lbqyYvJh13oVXy9T82YGjFeeVPk4j0g3nqxsgf2j+ayQfq+F1FdFYaEGOxONlQEl2gJkZVgZsaRYWdfy3eJTqP0uSI5sBm8VlCX/9m1v/wmqXWWUOO2Z2I71uJhe4mesM8J6i+uo6Bu9s6MJjeqi6ZwYzO5gp8jpYEljOX/aeYpCnNKrGCN2rVfx9TI1b2bAeOVJlY/ziHTjycoWGBr9sqmQfq+F1FdFYaEGOxONlQEl2gJkZVgZsaRYWdfy3eJTqP0uSI68bNzVSeGD3jdwnEaP/elcR7sdTC8d+wdTr7eW4uEu3P7+SFm1p54TgwfH3HY8y+dUcbp3mB0nelJXVkwu7Fqv4utlat7MgPHKkyof5xHpxpOVLTBqvUwh/V4Lqa+KwkINdiYaKwNKaf1o+ab74KoHLC0pZoaUQrD4FGq/C47uY9B7POV6Hb8e4JDvNI0213gNBgQdPifTx2BiC9NbXAfEGtlqPA30+DsYCPSOuf1ols2uQBOoqWxTEbvWq/h6mZo3M2C88qTKx3lEuvFkZQssrY9UKaTfayH1VVFY5FRQIISYBTwKTAck8BMp5X/G1VkFrAfC81N+LaX891z2K5uYmkMkqQ1sYTTNkBF86tlR64qrGPpOok9roPMzfx4xqLio+uwGtIAvoU1NaDRVNvH4+x5PtPiEgulZWKzeSw4XBSbrtyKLHN1s/Ewx2Dk0eJqQ1Jll887OsR4jruqzMNjpKTYWyVb2HqG1yhAS1IxICk4MHmJB+XljPkeYMo+Ls2dM4887T/O1d2VPfqCYYNIxYWqaccfzM3+1rhdup7QO/umPhvUqbMcK14tqR9d1OjXwI3EPd8bmtJG2ggjaCRkWNoeLGm+N5b4kZnmyoqgi67la5eM8wioupQ49LbGGtunnxl4/lEyPsQJqxbXmv1cJDJzO2LwGpCyrKKqge7g74zhSMajIFbm2sQWBr0opXxdClAFbhRB/kVLuiqv3opTy/TnuS9axNIfoDrTHPpzcohJpxMS6ctUD6NufpHnFDdy6+c60rCRmhhQ9FKS5ax+3blwz2saqtTRVLkgY8EyUBSWfbXGThqObwVUCFXOSVts7YNxFmeWx9210ZLCThWlsg0Xl+J1eqnoPR8pqPMZ6o5YsD3YAVsyp5JHNRzjY1s+82tKstq2YANK0owHJDVThdjZ8D972eXjmS9btaRp6Sa11HpVA6y6C23/FvmXXsGbj7ZE6a1evZUHlgqQDnnCezGWuVvk4j4iPy1AQTr9lbl4rbzTqWMS9Vrco9veaxt+HWZw9eOWD+EP+lGVrV6/lwW0PsuH4hozjU8WgIhfkdLgspTwppXx95HEfsBuYmctzjieW5pCeI+lbecysK+tvofOS2yIDnZi2M7CSdPraIgOdSBsb19DpS+yLsqBMYo68DHULjT2bkrBv4AQu4aSuqMLWaY71OHE7dCo8QVvHxyAE3cXTqYwa7JQ6y/E4irNuZANDQQ3wl12ns962YgLIlh0t3M75nxgd6CRpL2keHWmrfcWNkYFOuM6aDWto97Wn1SWVq6coaZjXsmYXxDzOjvcdT6tszYY1XNV0VeS5ik/FRDNu9waFEGcAS4G/m7x8sRDiTSHEs0II07k2QojPCSG2CCG2tLWNXeeZDSzNIUUlsRWTWXksrCt+zTlmK4lfD5q3oSdejCoLSvbJi5gd7DTuHNYln8IGsK//BDM91Thsfjt8rNdYr6Nlafu17pIZVPYeQcgQAEIIqovqOTbQnJ0TRFFTWsTcmhKefWvqrtvJi3jNFtmyo4XbSWHEDJM0j460FXA4TOsE9PR07SpXjzKpYjYVaZjXsmYXxDzOvE5vWmUtAy2Uu8tjnk/F+FTkD+My2BFClAJPA7dJKeNXF78OzJFSngfcD/zWrA0p5U+klCuklCtqa8e+6DMbWJpDhgdiKyaz8lhYV9x6cMxWErfmNG/DZKqEsqBkn7yI2aOvGD+nJ99fB4w7O402NxMF485ONuQEYbpL6nGFhikbGL3bUu+dxbH+/QRNBuxj5aJ51Ww71s2RjoHUlScheRGv2SJbdrRwOymMmGGS5tGRtlyhkGkdl5bejvMqV48yqWI2FWmY17JmF8Q8znxBX1plDSUN9Ph7Yp5PxfhU5A85H+wIIVwYA53HpZS/jn9dStkrpewfefxHwCWEKIgJm5bmkPI56Vt5zKwrVz1A1Yv3se7iu8ZkJany1rJu1drYNlatpcqb2BdlQZmkHH4RHG6oOStptXZ/Lx2BPmaZxEY69A4LuoccWVmvE6a7xBASVPWOTlur984mIP20ZHm/HYCVZ1YjgPXbWlLWVeQ52bKjhdvZ9gR88Icp20uaR0faqtnyKGtX3RtTZ+3qtWmvU1C5eoqShnkta3ZBzOOssawxrbK1q9eyvnl95LmKT8VEI3K5kZ4QQgCPAJ1Sytss6tQDp6WUUghxIfAUxp0ey46tWLFCbtmyJSd9zpQx29gg1hrkcBtrKwI+dJeXTgF+3b6VJJ9tbHlKliZhxTJhMfujlUY8/cN3k1Z7uWs3n99xP1+bdzVnl87K+DQ7W118e0M1nz6/hbOqfXZ7G4OmB/nIK99hx5kf4o2zrwOge7idh5q/y/Xzv8ql9R/Iynmi+c7vdzIU0Hnuq+9EpNiTKE+YXPGaTdKxsWXSjq6DDIGUSdtLmkfjbWwyiEtLbmMz7VJh52oVs3YJBY01OtE2tvjP83TjPo16421jy2MK4sNAYU2ubWwrgRuAHUKIbSNl3wBmA0gpHwQ+CnxBCBEEfMC1yQY6+YYmoSYUgmAICBmC7WR2HzOkbiQvPWhs+lhcDSU1aMBYb3FpDic1pTPSq6ssKJOLwU7D3nP+9SmrNg+cABiDdtqYSpEN7XQYXXPSW1xHddSdnXJ3NV5HCYf7dudksPP2+TX87MVDvHWil8WN5akPUOQvVnk41UVe9OtCgHCMtpXGYClpHh1pxwnUm9dIC5WrpwBmAxuhGY+lNH6aDSDSvf5Io55VnKVTpuJTkU/kdLAjpXyJFCNiKeUPgR/msh85IxO9qRXJdJIWd2AUirQ4ssn4OWNJyqp7+49T6Sql1Om1daqDXU5K3SGmFYVsHW9FZ0kDDV37jA93IRBCMN07i0N9u7N6njBvO6Oa/950mN9uO6EGO5ORVDnb7PUP/hD+/mNY/Y3McrtCYRez64JrHjO2EEh3WwuFQhFB/YWMhWzoTdPRSSoUdjj0Ijg9UN2UsupbfUeY462zfaoDnS4ay4bI9syvzrJGPIF+SgejJQWzaRk8wnAoO9Ploin1OFk6u4LfvnECf1DPevuKCSZVzjZ7/ZkvGeppO+pqhcIOZtcF/3M9dB0cu05doZiCqMHOWMiG3jQdnaRCYYfDL0Dd2bG2HhP6gj4O+U4z12tvYs1wEI73OplZNmzr+GR0lhqb5dV2j+qm672zkegc7tuT9fMBXLZwOh0Dfv68S33hMOlIlbOtXg+rpzNVVysUdrC6LnAVJ5apmFQoUqIGO2MhG3rTdHSSCkWmDLRD626oX5yy6s6+IwDMK85gnVkUh7td6FLQOC37g52ekjqCmpvartHBzsySeQg0dvdszfr5AJY0llNXVsTjrxxNXVlRWKTK2Vavh9XTmaqrFQo7WF0XBAYTy1RMKhQpUYOdsZANvWk6OkmFIlMOv2j8rE+9XmdH32EAzvDaG+wc7DIG5rm4syOFg67SBmqi7ux4HF5mFM9hV1duzEuaEFy2sI7NBzvY39qfk3MoJohUOdvs9Q/+0FBP21FXKxR2MLsuuOYxqJw3dp26QjEFUSvg49B1SceAH38whNvpoLrEjWa1JbymGYsDP/NX41ayywt6CHpPpK86dTiN3e3/6Y+GjU1zmusk7b+h7OhXFYXFwY3GlIfq+Smr7ug7TH1RJSVOj61THeh0UeYOUp5lOUGYjtKZNJ16FU0PoI9svDindAGvtP6ZgUAvJa5pWT/nOxfU8tTW4/zy70f5tw+k3pBVMUpGOXS80TSoXQifejbWchXOifE5PWxj+8Ba8Fanb3FTubagmfAYtrou0ByjsWkVY+noqRWKKYb6C4hC1yV7T/fx2Ue3cLzLR2Oll5/euIKzppclH/CUTrdvZtN1aN87NqNbsrbHaotTFB5Swr4/w4zzjQ/JpFUl23sPs6Bkpu3THewy1uvkaluazrJGHC2bqOo5RHvlAgDOKF3I5tY/sbvndVbUrMr6OSuK3Vwwt4pfbTnGV65ootyrppWmg60cOr4dhLY9yXOimZLXjsVN5dqCJC9iONl1QTJdtLK7KhSmqCwcRceAP5LgAI53+fjso1voGEhjAaBdM1s2jG4T0bYif2ndBX0t0LgiZdXTw110BHptr9cZCgqO9zhzsl4nTNu0MwCo79gVKav3zqJI87Kr69WcnfcDSxroHw7y2CtHcnaOycaYcuh4kKs8rXLtpCEvYthuPCm7q0JhihrsROEPhiIJLszxLh/+YBrTc+ya2bJhdJuIthX5S/NfjJ8zl6es+lqPsRZmfkmDrVMd7nIiETlZrxNm2F1Kj7eW+o63ImWacDCndAFvdr5MSAZzct65NSUsnVXBz148iM+fmyl6k40x5dDxIFd5WuXaSUNexLDdeFJ2V4XCFDXYicLtdNBYGbupYmOlF7fTkfpgu2a2bBjdJqJtRf7S/GeoOhOKq1NWfaV7L6UOL7M89ha5HhiREzTmcLAD0FY+l+kduxH66AXHwopl9AW62dP9es7Oe9X5M+kaDPDEq8rMlg5jyqHjQa7ytMq1k4a8iGG78aTsrgqFKWqwE0V1iZuf3rgikujCc3WrS9L4wLJrZsuG0W0i2lbkJ4OdcPSVtO7qSCn5e9ceFpY2otlccLOnzU2lJ0C5J7fferaWn4ErNER1z8FI2dzSRXgcxbx46vc5O+9Z9WUsmlHGj54/wKA/N3eQJhNjyqHjQa7ytMq1k4a8iGG78aTsrgqFKWrFWhSaJjhrehm/uWVl5haWeItPujYeu8cBeihIp68Nvx7ErTmp8taiOZyxVqCyevj0XyGkDEFTgj2/BxmCOW9PWfWwr5XT/m7eVbvM1qmkhN1tLs6sHExdeYy0TZsLQH3HTtormwBwak4WV17ElvaNtA+dpMYzIyfnvuaC2dz5zE4efP4gt1+5ICfnmCyMKYeOB2PJ03YsboNtpu3rUqdzqBN/yI/b4abKU4UmVF7OB/IihlPFmxUOpyEjiD/ORE5gef2gUExCVGTHoWmC2rIiuwcnN6Vk8Tg9FKS5ax+3blxDy0ALDSUNrFu1lqaKJrRc2d0U+c+u9VA2w5jGloK/d+8BYFHp7BQ1zTnZ76Bn2MEZFb7UlcfIsLuUnuI6Gtre5K35H4qUL62+hDc6XuS3Rx7iM2fdkZNzL5hexsXzqvnJCwe49oJZNFR4Ux80hRlTDh0P7OTpdC1uxbUprWy61GnuaubWv906mrsvW0dTZZMa8OQJEx7D6cSbFQ4nlDcmb97q+qFygRrwKCYlKrMWKJ2+tkiiAmgZaOHWjWvo9Ckr0JRlsNPYX2fO20nHA725aw81rmnUusttnW53mzGtY17FkK3jM+VURRPTO3biDI4OrspcFSyvWcWrbX/ljY4Xc3buT1w4m5Au+f7/25OzcyjymHTtWGnU6xzqjAx0YCR3/+1WOoc6x+OdKAqBHNv9kl4/KBSTEDXYKVD8ejCSqMK0DLTgl0FlBZqq7PmDsQHdnHekrDoYGmZT1y6WTJuLsLleZ/spN2XuILXF42P6aak6C4cM0dC2Pab8otorqffO5md7vsNLp/5AQM9+rNeWFfGBJQ2s39bChj2tWW9fkeeka8dKo54/5DfP3SGVoxUj5NjuZ3n9oKt1iYrJiRrsFChuzUlDnC64oaQBt3AqK9BU5Y1fGNMXquenrLqpcxfDeoDl5anrmhHSYfvpIhZUD+ZsM9F42stm43d4aDy9Jabcqbn48JzPMt07i0f338OXX343t25+L7dufi9f3vwe/v2NT/PHY48xHBrbdLsPLZ3JrEov//L0dnoGlcp1SpGuHSuNem6H2zx3O1SOVoyQY7uf5fVDik2oFYpCRQ12CpQqby3rVq2NJKzwnNsqr7ICTUna9sGxv8P8K9KawvbM6VcodxazoGSmrdMd7HLR79dYUJX79TphpObgVMV8GltfB6nHvFbsLOXjc2/hI3M+xwW1l3N2xXIWVazgnIoLAPjtkZ9x97bP0TbUYtZ0WrgcGje/80za+4e56/c7x/ReFAVGunasNOpVeapYd9m62Nx92TqqPFXj8U4UhUCO7X5Jrx8UikmIGsYXKJrDSVPlAh5/938n2lRs2t0UBcwbvzDsT2denrJqu7+HFzt38g+1S3HYXBD9xskiBJIFVbk3sUXTUnUWszveoq5rH61VC2NeE0JjbtnZzC07O+G4o/3N/O7YI/xg+218c+lPKHNV2Dr/vNpSPrR0Jr9+/QQXz6vmYytm2WpHUWCka3FLo54mNJoqm3j8fY8rG5vCnDFYWtNqPtn1g0IxCVGRXcBoDic1pSa6XbtWOEVhEhiCbY9D4wXgrUxZ/YmWF9DRubTqXNunfOVYEXMrhihx66krZ5GWqoUENSdzT7yYMNhJxuzSJq4+43M8efB+Ht77Pb58zv+1fXH5kaWN7DvVxx2/fYtFDdM4p8Ge4EFRYKSbV9OopwmNGm9NljqmmJTk+HPc8vpBoZiEqK+SFIpC580nYLADzv5gyqp9QR9PtjzP+dPOZHpR6oGRGSd6HRzrdbG4rt/W8WMh6PTQUrmQuS2bEBkupq33zmZV/YfY2f0qz7U8ZbsPDk3wpcuaKCly8vlfbKWjf9h2WwqFQqFQKHKLGuwoFIWMrsPL90N1E9QvTln9oWN/ojc4yAfqLrR9yk1HjX1mzq0dsN3GWDhauwSPv4+ZbW9mfOx5VW9nXtki1h95mK5h+1a1cq+LNVc00do7zGce2cJQIGS7LYVCoVAoFLlDDXYUikJm9zPQeQDO+XBKMUHzQAuPHH+OiyvO5oxie9MjQjr87aCXpqpByj0Tc4F/qrKJYWcx848+l/GxQghWz/gwugzxq0MPjKkf8+vK+OLq+Ww71s1XnnyDYGh8p/QpFAqFQqFIjRrsKBSFSigAz/27YeqZszJp1b6gj9t3/YRiRxHXNlxq+5TbThXR4XNw0cxe222MFV1zcnD6cmafepWSwczvzlS4a7iw9nK2tm9kT/cbY+rLhXOruOHiOfxp52m+9tR2QrocU3sKhUKhUCiyS04HO0KIWUKIDUKIXUKInUKIr5jUEUKIdUKI/UKI7UKIZbnsU07Rdeg/Dd3HjJ+6+qZXkUNef8S4q7Psk6A5LKsFZYiv7/lvjvrauHn2eyhzem2dTkr47e4SyouCLKqZmClsYfbPeBsIwdmHn7V1/Iqa1ZS7qnny4DpCcmwb6b3n3Blcs2IWv3njBP/69HZ0NeApLFTeVuQjKi4ViqyR6zs7QeCrUspFwEXAF4UQi+LqvAdoGvn/c8CPctyn3KDr0LoLfnYF3Heu8bN1l0pQitzQ3wZ/+y5MPxcardffBGWIb+x5hOc7d/CJhlUsLLWvSt5+2s2edjer53ThmOB7wr6ico5XL2LBkb/iDmQ+8HJpbi6t/wAtg4d44eTvxtyfDy2dydXLZvK/W4/zzd++pQY8hYLK24p8RMWlQpFVcnrJIqU8KaV8feRxH7AbiN/F8CrgUWnwClAhhCg8H+JgGzz5Ceg+ajzvPmo8H2yb2H4pJifP/jMM98FFt1iu1QkPdJ5t28LH6t/B5TXn2T7dcBAe2jqNSk+ACydwCls0e2ZeiivoY/H+X9s6vmnaEmaVNLH+6EP0B3rG3J+rlzXyofMbeOLVo3z5yTcYDippQd6j8rYiH1FxqVBklXH7flYIcQawFPh73EszgWNRz4+TOCBCCPE5IcQWIcSWtrY8/IMP+kcTU5juo0a5YkqSs5jd/r+w89ew5JrRHbbjCE9dCw903lO3wvbppISfvzGNk/1Orl7YhjNPVvp1l87gSO15LDr4B1trdwxZwYfwBQd45ujPx9wfIQTXXDCb6942mz9sP8knH36V3qHAmNsdL/I+x+YClbcLmkkbsyouFYqsMi6XLUKIUuBp4DYppa2vhaWUP5FSrpBSrqitrc1uB7OB05144Vkx2yhXTElyErOn3oJnvgR158Dij5pWCehB/mX3z/l/bVsTBjpSwoBfcLTbyYFOJyf7HAwGrC1ugRD8/I0ynjtYzOo5XSyo9mXnfWSJt+ZcjkRy0Y6fGm8uQ2o9DZxXtZLnTz7D/t4dWenT+5c08MXV89lyuIuP/WgzRzomdn1TuuR9js0FKm8XNJM2ZlVcKvIAIcQfhRAVE92PbODM9QmEEC6Mgc7jUkqz+SYngOiFBI0jZYVFcS1c+8ToreeK2cbz4kmUgBUTS9dhePxj4C6BVf8KWuKf70BwiNt2/YRXuvfw8RmX8O7a5QRC8NoJD1taithx2k33UKLMoMITYkZZiBmlQRrKQpQV6ZwecPDSEQ+tA07eMaubd5/ZOQ5vMjMGiyrYfsa7WHbwDyw8/Cx75r434zbeMf19HO7fw8/23s2/LX2IYmfpmPv1jvk1lHtdrHuumfff/xL/38fO413n1I+5XUWWUXlbkY+ouFTkAVLKzD9Q8xQhbXwbmnbjQgjgEaBTSnmbRZ33AV8C3gu8DVgnpUy64+GKFSvkli1bst3dsaPrxpzaoN/4Bqa4FrQ8mfOjSJfkm9XYZMwx27YPHvsIDHXDP3wXquYlVDk93M2tOx9kT/8x/qnxCua7l/DXA142HPLSO+yg1BXizKpBGsuGKfcEcWmSoaBG77CT9kEX7T4X7YNu+vzGYEgTkjnlQ6ye083CmkH7fc81UvKO3Y8zvXs/Gy74Z45Pz3zKXsvgYZ48eD/nVF7ALWffjVNzZaVrbX1D/OdzzRxoG+Ajy2byrfctorIkq9/O5me8FhIqb483KmbTQcVlPpGTmM0GQogS4FcYNwocwHeA74+UvQfwAf8opdwvhKgFHgTCtw1vk1JuGpl9dT+wApDAXVLKp4UQh4EVUsp2IcT1wK2AG2M5yi0jbTwUddzDUsq1uX7Pdsj1nZ2VwA3ADiHEtpGybzDyDy2lfBD4I8ZAZz8wCHwqx33KHZoGpfY2a1QoTJESdv4GnvmycSfnyu+YDnQ2de7i63v/m8HQMFeUfIQN25fwn6eL0ITk7JoBLprZS1OVDy2NlO0LavgCGqXuEG5HAVjFhODvCz7KpTsfYfVr9/DS0i9xaOYlGTXRUHwGlzdczV9b/peH932PTy34Oi5t7IOS2jIPd37gHH77xgnWb2thw55Wblk1n+svmoPXba0LV4wjKm8r8hEVl4r0eDfQIqV8H4AQohxjsNMjpVwshLgRuA94P/CfwFop5UtCiNnAn4CzgW+F64+0URl9AiHE2cA1wEopZUAI8QBwHbATmCmlPHekXt5OecvpYEdK+RIpRsTSuLX0xVz2Q6EoOKSEo5vhhXvgwN+gpglWfQNKYqcx7B9o4YEjz/KX9q149RoGj32GpwfrKS8KcuXcTi5s6KXck5kVzOvU8ToLS3EacHp44ZwbuWTXY7zz9ftoPL2VbWddS19J+lPHzqt6O/7QEC+c/h2nfcf4p6Z/ZVbp/DH3zeXQ+NiKWbxtXjWPvXKE7/5xNw8+f4Crlzfy4aUzWVhfhrAw6ikUCoVCkYQdwA+EEN8Hfi+lfHHk8+SJkdefAJ0SZkUAACAASURBVMJ3W64AFkV93kwbuatzBXBtuFBK2RV3jsuB5cBrI8d6gVbgd8A8IcT9wB+AP2f3rWWPnK/ZUSgUFkgJAZ+hkB7ug8EOaN8Lp3bA/r9C50Gkuwz/sk9zfPY76R4KcKrrCAf6O9nbf5ydg7vo4AhSd+LvuILh7ks4t8bPsgUtzE/zLs5kIuD0suHcm1h0bCMLT7zE3BMvcbr6HFpqz6O7bBa9JTPwu0oIOL0EHUWmyu4Lai+jqqiOP514ku9s+wxnly9ncdVFzCqZT5VnOlVFdTiEvbQ5u6qYb7z3bPac6uUP20/y0EuH+MkLB6krK+KiedWcVV/GmbUl1JYVUVVSRGWxiyKnA7dTwzHVfpkKhUKhSImUcp8QYhnGDKm7hRDPhV+KrjbyUwMuklIORbeRxpdtAnhESvn1hBeEOA94F3Az8HHgpozfxDigBjsKxUTRuht+dHFiudMDM5bgP+tDLNmwmKHNgrKuOxKq6UMzKfFfwQLX+Zw710VTdS8eJ4Br5P+pyeGFH+bk3MuYc/wl6lu3s3zP4wl1Nlx2L53VZ5sef77nQhZUncOWtufZ1r6Z3Ye2Rl67a8VPmDfN/Lh0WTq7kqWzK+ke9PP3Q528daKHlw+088ybLZbH/PiG5UpwoFAoFIoYhBANGOviHxNCdAOfGXnpGuD/jvzcPFL2Z+DLwD0jx54vpdwG/AVjhtVtI+WVcXd3ngPWCyHWSilbhRBVQBkwAPhH1vfsBR7L5XsdCzkVFOQKIUQbcCTN6jVAew67kw9MhfcI4/M+26WU7852oxnGrF3yPQ5U/8aGWf8KOV6tKMTfQ76Qz30Do397JiBm8/3fJRmq7xNHuP85ybPZQAjxLozBiw4EgC8ATwH/gyEoGAY+MSIoqAH+C2OdjhN4QUp588hUtv/CmKoWwhAU/DpOUHAN8HWMu0MBjMGRD/g5o9vYfF1K+ew4vO2MKcjBTiYIIbZIKe3vqFgATIX3CFPnfdol3/99VP/GRr73L1vk+/vM5/7lc99g4vqX7/8uyVB9nzgKtf/Rg5SJ7ku+oDyGCoVCoVAoFAqFYlKi1uwoFAqFQqFQKBSTACnlGRPdh3xjKtzZ+clEd2AcmArvEabO+7RLvv/7qP6NjXzvX7bI9/eZz/37/9k78/ioqrPxf587SxISJISAwIu4FVyq1ip1462Ky4stVuyiVtzAWrX0rcX6k/a1VGqLtkoRpJWiooKKlqp1qdZStagtWhU3sLZq1RYRWQIESTLJbOf3x507meXOkmQmM5M8389nPsnce+65J5lnzj3PebZyHhuUbnzl/n/Jho69dFT6+JUYfT5mR1EURVEURVGU/kl/sOwoiqIoiqIoitIPUWVHURRFURRFUZQ+iSo7iqIoiqIoilJBiEhLlnPPF/G+VxWr72KhMTuKoiiKoiiKUkGISIsxpi7lmNcYE+7t+5Y7FWnZOeWUUwygL30V41UUVGb1VaRXUVB51VcRX0VBZVZfRXz1mI5w5OiPdgSe/8+21g8+2hF4viMcOboQ/QKIyPEi8hcReRR4K3asJfZzhIg8JyKvi8ibIvJ5l+s/LSIvxdqsFZExsePnJhy/RUQ8IvJzoCZ2bHms3fdifb8pIjNix2pF5HEReSN2/KzY8atF5OXYsVtFRAr1f8hGWdTZEZH9gBUJh/YBrjbGLHBr39SkRWGVykJlVqkkVF6VSkNlVilXOsKRo9/Z3PLot+55pXHDjgCjBtfs9etzD3907O51p1V5PS8U6DaHAQcZYz5IOT4FWGmMuVZEPMAAl2svBW4yxiwXET/gEZEDgLOA8caYkIgsAs4xxvxARP7XGHMogIgcDkwDjgQEeFFEnsVex280xkyKtRsUu9evjDE/iR27GzgV+H2B/gcZKQvLjjHmbWPMobF/3uFAG/BQiYelKIqiKIqiKN2maVdwnqPoAGzYEeBb97zS2LQrOK+At3nJRdEBeBmYJiI/Bg42xuxyafMCcJWIfB/Y0xgTAE7EXo+/LCKvx97v43LtfwMPGWNajTEtwO+AzwPrgJNF5HoR+bwxZmes/QQReVFE1gEnAJ/u9l/cBcpC2UnhROA9Y8x/Sj0QRVEURVEUReku4Wh0hKPoOGzYESAcjY4o4G1a3Q4aY54DjgU+ApaKyPki8uWYG9rrIjLOGHMvcBoQAP4gIidgW2mWOYYIY8x+xpgf5zsYY8w72NamdcCcmPtaNbAI+Jox5mDgNqC6+39y/pSjsvN14L5SD0LpHaImSlOgiY0tG2kKNBE10VIPSUlBPyNFURSlN+iLzxuvZX08anBN0rFRg2vwWtbHxb63iOwJbDbG3AYsAQ4zxjyUoMSsEZF9gPeNMQuBR4BDgKeBr4nIsFg/DbG+AEIi4ov9/hfgdBEZICK1wJeBv4jISKDNGHMPMBdb8XEUmyYRqQO+Vuy/36GslJ2Yr+BpwP0u5y4WkTUismbr1q29Pzil4ERNlHd3vMs5j5/DxAcncs7j5/Dujnf7xOQGfUNm+/pnpHTSF+RV6V+ozPYt+urzpnGg/4pfn3t4k6PwjBpcw6/PPbypcaD/il64/fHAGyLyGnYMzk0ubc4E3oy5qx0E3GWMeQuYBfxJRNYCTwKOJepWYK2ILDfGvAosBV4CXgSWGGNeAw4GXor1ORuYY4xpxrbmvAmsxHax6xXKKvW0iEwGvm2M+Z9s7caNG2fWrFnTS6NSikVToIlzHj+Hja0b48dG1o5k+aTlNNY0lmpYRckMUqkyW6afkdJJ2curMYa2YITaqrLIh6OUnrKXWaU0lPHzpscy2xGOHN20KzgvHI2O8FrWx40D/VcUMDmBkoNye/qcjbqw9RuCkWDSpAawsXUjwUiwRCNSUtHPSOkpi555j7kr3+av35/AqMFuiYAURVH69vOmyut54b8G1xxT6nH0V8rGjS3m63cydiYHpR/g9/gZWTsy6djI2pH4Pf4SjUhJRT8jpafc++J6AF75z44Sj0RRlHJGnzdKsSgby44xphUYUupxKL1HQ3UDi09ezIZdG6jx1hAIBxg1cBQN1Q2lHpoSo6G6gYUnLOSyP1/GxtaNjKwdycITFiZ9RlETZXv7doKRIH6Pn4bqBiwpm32Ush9fX6c5YO/Kbt3VUeKRKIrSG+Q756a2q6+qz/m8UZTuUDbKjtL/iJoo7eF25vxtTnxiWzBhAVET1cVomWCJxZjBY1g+abnrg8sJKE19OI0ZPKYsPsNyH19fJxo1BIIRALa1Vr4riqIo2cl3zs3Ubt/6fTM+bxSlu6gEKSWjKdDEjFUz4j66G1s3MmPVDJoCWgm7nLDEorGmkZF1I2msaUx68Gxv3x5/WIH9GV7258vY3r69VMNNotzH19f5pD1ENJYDZ1uLWnYUpa+T75ybqV1zR3PG542idBe17Ci9jmO6DkVCrsGIoWioRCNTuko+AaWldCPrywGvlcD2BGvOdrXsKEqfJ9uc2xRoij8HdG5WehNVmZVeJTGPfjAadA1G9Fm+DFcr5UaugNJS103QgNfSsqMtmPC7bmIoSl8n05wbMZGk50DERHRu7iEi0pLl3PO9ORaX+48UkQe6ee0zIjKukONRZUfpVRJN18veXMaNx98Yn/BG1o5k/oT5pc6nr3QBJ4FB4meYGFBaajeyXONTisv2VlvBaaj109oRLvFoFEUpNm5z7k0TbmLuS3OTngNzX5rLTRNu0rm5wIiIF8AY0ytprp37pWKM2WiM+VovjcGTq426sSm9SqLp+qH3HgJg0UmL8Ft+fB4fjTWNeC0Vy0ohVwKDUrsq5BqfUlx2xFzXhtZVqbKjKP0Atzk3Go2yasOqpHarNqxi1lGz+s/cHO44mpYt84iGR2B5P6Zu2BV4qwpSVFREjgd+CuwA9gfGikiLMaZOREYAK4DdsNf83zLG/CXh2kHAWmBvY0w0Vgbmn8A+wGjgZmAo0AZ80xjzTxFZCrQDnwVWi8gjwE2xLg1wLHZ25ceMMQfFlJHrgVOAKHCbMeaXInIi8IvYuF6OjS0puFNEzgauwi7s+rgx5vux4y3ALcBJwLeBv2b7H+mqUulVHBN3osLz4qYXy6FCstJNnAQGbqR+3tD7rgrZxqcUl+0xN7bGgVX882NNUKAo/YHUObcp0OT6HLCsfjI3hzuOZss/HuW35zXSvB7qR+/FmXc/yrADTiuUwgMcBhxkjPkg5fgUYKUx5tqY0pFU2dkYs1NEXgeOA1YBp8bah0TkVuBSY8y7InIksAg4IXbpKOAYY0xERH4PfNsYs1pE6rAVoUQuBvYCDjXGhEWkQUSqgaXAicaYd0TkLuBbwALnIhEZia0kHY6tyP1JRE43xjwM1AIvGmOuyOef00dVaKVcydetKGqiNAWa2NiykaZAU6/FeCiFpRzcyFSWSseOtiBeSxhU46MtqJYdRekPpM65Tv2cfuuy1rJlXlzRAWheD789r5GWLfMKeJeXXBQdsC0m00Tkx8DBxphdLm1WAGfFfv86sCKmtBwD3B9Thm4BRiRcc78xJhL7fTVwo4hcBtQbY1In+5OAW5zjxpjtwH7AB8aYd2JtlmFbhBL5HPCMMWZr7NrlCW0iwINu/wg31LKj9Cr5uBVpbZS+Q6ndyFSWSktrR5gBfg81PotAKEokavBYUuphKYpSJLR+jgvR8Ii4ouPQvN4+Xjha3Q4aY54TkWOBScBSEbkR2AXMjjW5CHgUuE5EGrCtKH/Gtpw0G2MOzXU/Y8zPReRx4IvYbm0TSbfuFJr2BGUrJ/1E0pTeJNdOera6LVD6oHalZ6R+/kDJ6iaoLJWW1o4I1T4P1T47fjQQyvvZpChKGZLr+a71c1ywvB9TPzr5WP1o+3iREZE9gc3GmNuAJcBhxpiHjDGHxl5rjDEt2Bagm7DjbCLGmE+AD0TkjFg/IiKfyXCPfY0x64wx18f62T+lyZPAJU4yg5hS9Tawl4h8KtbmPODZlOteAo4TkcaYC97ZLm3yoh9Jm9IbFCLVcKmD2pXuU+pU06moLJWWlo4w1T4PVV5b2VFXNkWpXPKZ33XOdaFu2BWceXdTXOGpHw1n3t1E3bC84k16yPHAGyLyGrar2k0Z2q0Azo39dDgH+IaIvAH8HZic4doZIvKmiKwFQsATKeeXAOuBtbG+phhj2oFp2G5y67ATFyxOvMgY8zHwA+xYojeAV4wxj+T+k9NRNzaloGTa1elKAoJyCGpXukchPv9CorJUWto6wlT7LKp9Vux9BAaWeFCKonSLfOZ3nXNd8Fa9wLADTmPqHwqajc0YUxf7+QzwTIZzy7DjYXL19QB2xrPEYx9gZ1BLbTs15f13XLr8N3BQ7HwY+F7slXjd09gZ3VL7Pz7h9/uA+1za1GX4U1xRy45SUAqxq1MOQe1K9yi3XT2VpdKSbtlRNzZFqVTymd91zs2At+oF6vc4hoa996Z+j2MKmIVNyQO17CgFpRC7OqUOale6T7nt6qkslZbWYISGWj8+j71h2BFWZUdRKpV85nedc5VyRKVPKQhO0GIwEmTJxCVMGDUBgAmjJrBk4hKCkaBrMGOmYMdcSQyU8iHxM8TA4pMXJ+3qLT55MRiKlvq5pwkxlOLR2hGmxufB57H/5x1hTfutKJVKJqtNfVV9wZLS5FMqQMsJKF1FLTtKj3FLNXnThJuYffRstga2ctHKi1zT/mpa4Mon02d436n30R5up9pbzda2rZzz5DlF+YxVhsqb1pgbm9+ryo6iVDpuVpv6qnrea36vIHNwPvO5zvlKdygLyRCRehF5QET+KSL/EJGjSz0mJX/cgha/u+q7hEyI7676bsa0v5oWuPLJ9BlGTZSRdSOJmmhRP2OVofLFGENbMEK1z4pbdto19bSiVDSplvLmjuaCzcH5zOc65yvdoVwsOzcBfzTGfE1E/MCAUg+orxA1Uba3by+q72wwEqSxppGZR8xkkH8QO4M7uWPdHYQioazBjOUWzK50nVyfYT6fcaqM1lfV09zRnJfMqgyVLx3hKOGosS076samKH2C1Pm6kHNwPn3pnK90h5JbdkRkEHAscDuAMSZojGku7aj6Br1V86TaW82Mw2Zww0s3MG3lNG546QZmHDYDn8cX9+11SAxmdIIdM51Xyp9cn2Gu824y+s6Od/jJ8z/JS2ZVhsoXJ/Najc+D3xtLUKCWHUWpWNzm64iJFGwOzmc+91sZ2lj9b84XkZYs554vQP8/EZGTunjNaSLygxxtRorIAz0bXdcoubID7A1sBe4UkddEZImI1JZ6UH2Bnph7uxIk2BHuYNbqWUn3mbV6Fn7LnzUFpaaorHxyfYa5zm9v387Nr93MzCNmcufEO5l5xEwWv76YyWPs2mW5ZFZlqHxp7bALiCa5sallR1EqFrc1xdyX5nLThJtyzsH5rCkaqhtYfPJiFp24iDsn3smiExex+OTFSX1ZlsWc8XOS7jdn/BwsqxyWs6VHRLwAxphjetqXMeZqY8xTLvfwZLnmUWPMz3P0u9EY87Wejq8rlIMbmxc4DPiOMeZFEbkJu2LqjxIbicjFwMUAo0eP7vVBViLdNfd2NUjw2v++1vU+7ZH2rCko+3qKyv4gs/l8hn6Pn1lHzaLGW0MgHEjapYtGo0w5cAqzV8+Oy9o1469hoK+z8mQ2me3rMtSbFFpenfgcvychQYFadpQC0h/m2HLCbU2xasMqZh01K+sc3JWkAsFIkDl/m5PULpH2cDsLXl2Q5Da/4NUFzD1ubvH+8AIQjASP3hbYNi9swiO84v14SM2QK/wef0Fq7YjI8cBPgR3A/sBYEWkxxtSJyAhgBbAb9nr7W8aYvyRcOwhYC+xtjInGjA3/BPYBbgMeM8Y8ICL/jvVzMnCDiHwC3Ai0AquBfYwxp4rIVGCcMeZ/RWQp8AkwDhgOzIz1tVes34NiitP12MVLo8BtxphfisjVwJeAGuB54BJjjOnu/6gclJ0NwAZjzIux9w9gKztJGGNuBW4FGDduXLf/4P5Ed2ue5FMlObHNzuDOjPdxghkzket8JdNfZDbbZ7i9fTuXPnlpmmw4shQlGld0wJa12atns+ikRUnts8lsX5ah3qTQ8toesndu/V5LU08rRaG/zLHlQqY1hWVln4PzWVPk287v8dMUaGLGqhlJYyhn1+VgJHj0v5r/9ejlqy5vjClxe82fMP/RT9V/6rRCKTzYRoODjDEfpByfAqw0xlwbUyySYuKNMTtF5HXgOGAVcGqsfUhEUu+xzRhzmIhUA+8CxxpjPhCR+7KMawTw39hK2KPYa/xELgb2Ag41xoRFxDHj/coY8xMAEbk7Nq7fZ/8XZKbkyo4xZpOIfCgi+xlj3gZOBN4q9bj6Ao6LT+puSkN1g2viArAnm0Ao0KUgwTvW3cHc4+ays2NnfPd+1MBR6kpUoRQyqUUwEuTI4UdywUEX4BEPERNh2ZvL4rIUNVFXWQuEA4C6pVUy7bECon6vhdcSBLXsKEo5k2vuz7amyEYmL5NotLM+X77JDro7hlKyLbBtnqPogP03Xb7q8salpyydN6JuRI/dzWK85KLoALwM3CEiPuBhY8zrLm1WAGdhKztfBxa5tHHaga24vJ9wv/uIWVhdeNgYEwXeEpHdXc6fBCw2xoQBjDGOz/oEEZmJrZw1AH+nkpWdGN8Blscysb0PTCvxePoEmVx8gDST8uKTFxOMBLnsz5cx84iZOS1CqTs8uUzPSmVQ6BoGA7wDOGv/s5j+1PR4fzcefyMDvPbmUqadwsaaRlZ+daW6pVUwgaDjxmYhIvi8llp2FKVMyXfuz+aWnAm3eX7CqAlsb98eL08xsnYkSyYuybn2qETX5bAJj3BT4sImPKKAt2l1O2iMeU5EjgUmAUtF5EZgFzA71uQibIvLdTGryuHAn7tyjxx0JPyeZipyI2Y5WoTtDvehiPwYqO7GveOUhXQYY143xowzxhxijDndGLOj1GPqK7hVj3czFW/YtSF+7I51d3DN+GuyBhwmBoZfePCF/PCvP9S8932AQtcwaA+3871nvpfU3/ee+R7t4XYgc4KBYQOGdav6tlI+xGN2YvE6fo8qO4pSruRb4+bSJy9l+tPTmbZyGtOfns6lT16a8/ngNs9fecSVaXX48k124LauKWe84v3YLYOcV7wfF/veIrInsNkYcxuwBDjMGPOQMebQ2GuNMaYF2wJ0E3YsTS4T/NvAPrHYG7CtQt3lSeASJ7FCTOFyFJsmEakDepzMoFwsO0ov4mYqrvHWJLkbAdxw7A001DRQ46nBsiw2tW5K2kVxdlfycXtTKoNC1zAIRd1rLYVMCLAfWvvW78uyLywjFA3hs3w0VDWwpW0LoUgIn8dHY00jXkunqkrDybzm1NjxeUSLiipKmVLMGjdJ83xsXrewXJMdzD56dtLzwE2Z6Y36gYVkSM2QK+ZPmJ8Ys8P8CfObhtQMuaIXbn88cKWIhIAW4PwM7VYA98faZ8UYExCR6cAfRaQVW1HqLkuAscDa2BhvM8b8SkRuA94ENvWwf0CVnX6Jm0m51lfr6m60m283NrVtymjabqxppImmbiVCUMqP7ia1yIRTaym1P5/lA+yH1nvN78Xla+oBU/nCvl/g8lWXk/BQYOzgsarwVBidlh2J/VTLjqKUK/nM/d19PqTO8yNrR3LThJuYMGoCqzasirebMGoCWwNbk1zbsmWCLYSrdW/g9/hf+FT9p05besrSgmZjM8bUxX4+AzyT4dwyYFkefT1AipuZMWZqwu97pVyyyhizv9hZDG4G1sTaLQWWpl6fMqZ/AwfFfg8D34u9EtvOAmblGne+lKdkKF0mUw77cDTMptZNfPjJh2xq3cTW1q0EQgHuPOVOph4wFbAnq0FVg1zdjdrCbTlN21rrpO/g9lkuPnkxGDLWR0iVsVAkFJfFak818yfMT+pv0UmL8IiHjS0b2dK2hZtfuzkuX6ePPT2u6EA8kJOtbVuz1mdQyg8nGYEvbtmx6AirZUdRept8a9zkeo5391nvVk9t0euL+L+j/i+pps7/HfV/aa5tj/3rMTa3bo4/X7YFthXU1bq38Hv8L4yoG3HMHgP32HtE3YhjCpiFrVR8M5bF7e/AIOCWEo8nK7pV2gfItNOxz6B9eLf53aRd8jnj57Dg1QU0BZqYP2E+Uw+aihFDe7jd1TwdNuGcZutKDBhU3En9LKu91Wxt28o5T57juosWjoZ5Z8c7aZaYJ957gqX/WMrI2pHcPvF2lp2yjJAJUe2pZnv7ds79w7lJdXW2tW9jbdNaWwlykbdNrZs4/4/nV8QunmKTmHoaYspOSBVVRelN8rWE5Psc706CArd6anOPm8vO9p1JiY0WTFhAY01j/Bnw5X2/zCn7nMLUP06Nt7n1f25Vt/kywBgzH5hf6nHki64W+gCZAgubAk1pu+SzVs/iwoMvjO+Yh0yIxppGPOKJ79Y4xALoXI+nTnCVFjCoZCbxs4yaaNZdNDcZu3zV5Zw+9vT4+2+s/AZej5c9Bu6BiKT1N3v1bC48+EIAIibiKm/bO7a73l8pX9wSFLSrZUdRepWuJJ3J9RzvboICt3pqOzt2pllxZqyawaWfuTR+3QUHXZDmcbL+k/V5rUkUJRFdkfYBMgUNhqPJVplDGg9h5hEz2XfQvvEdFMflyCOetAxs14y/BhHJKzuK0jfJFZAairgnIHCSXAAcOfxIgpEgH37yIYGwezKLhipbnh5+5+E0t7drxl/DHevucL2/Ur60hyNYAl4rMUGBWnYUpTcpZNIZp27aw5Mf5ven/56HJz8cn9+z4VZPrcZb4zquPXfbMz7/u1n6F7+xmAUTFuiaROkS6sbWB8gUNOi1vPHjhzQewncO+06SGXnO+DkAnPP4OSyZuIR737qXmUfMZJB/EDuDO7n3rXuZPGYyj7z7CEsmLsEjHnVR62fkCkjNlIAgEstc+eV9v8xZ+5/FtD9Os9OaT7zDtf2IuhHxujqD/IPi2Xi84uVnL/6MtU1rXe+vlC/toWjcqgOOG5tadhSlNylk0plcddMyjsFKH0MgHHAd1wDvgLgrnSBpbZoCTTTWNKrbvNIlVDr6AKlBgxNGTWDJxCUYDEv+ZwkTRk3gwoMvTDMjz1o9yw5ajOW3n37odG546QamrZzGDS/dwJQDp3DHujvY1r6Nf+/8twaG90NyBaQ21jSmWWLmT5jPw+88DMDUg6YmuSEYY/jZ53+W1P5nn/+Z/UCOuU74PD6G1w5nj4F7sHvt7lw+7vKkINbFJy/WXbwKoD0USVJ2/F4rno5aUZTeoZAJhHLVTcuEZVnceNyNSfP4iAEjXL1G6qvr49d5LS+LTlqU9nwZXD1Y3eaVLqGWnT5AYmBhNGrnn79o5UVJQX/1VfWuJmNnB37VhlXMOmqWXTcnHOCdHe/wy1d/CZBmEdIA8f5DrqBVr+Vl7OCxSXURhlQPYdjBwzj7wLOJRCNpKc4jJpIU4Oq1vIQj4YxjCEaCSUGsC09YWPS/W+k57aFovMYOqGVHUUpBIRMI5aqblolwJEwgEkiax6/972vZe7e9k8ZVX1WflqJ64QkLueeL99AeaY/X3dEyBEpX0dVqH8EJLLQsyzXoT0Rcg/p2BnfGf7csu48abw03vHQDa5vWulqENEC8f5EraNVreeOWmOG1w+OFQEfWjYy7uTlUe6u58tkrkwJcr3z2SqK47/h3JbhWKS/aw5F0Nza17ChKr1OoBEKp8zkk103LRJQoP/zrD5Pm8R/+9YfxBEnOuJo7ml3nexGJP19U0VG6gyo7fYxMwYhRE00zZc8ZP4c71t2RZtZONHsP8g/SNI9KEvnUbHBIdXPLlKAgHA279lfI4Fqld+kIRZIsO7Ybm1p2FKVSyeS2PKR6SNZngluCAmddkojO90qxUBW5j5EpGNFn+fBa3rj7kMEwqm4Uc4+bm2bWTjV7Fyq4Ual8ulq9OtXNzUllnipPH+z8gOlPT0/rpSGUyAAAIABJREFUr5DBtUrvEgilWnZE6+woSgWTyW35/Z3vZ30m5DuPuyUyGFk7Er+l873SM9Sy08fIFIwIMP2p6XH3oQWvLOBfzf+Kx+xA8o799vbtNFQ3MLx2eMGCG5XKp0duZQYE4faJt6dZGBe/sdi1v0IG1yq9S2rMjt9ru7EZY0o4KkVRekKq2/LO4M60Z8LNr93MlrYtcUtPfVV9XvO4ZVmuCWwsS5eqSs9Qy04fI1Mw4ke7PopPRm5pqBeesBC/x8+lT16atjtTqOBGpfLpqptBOBrmnR3vxAuPOm4PK05dQVu4DYArn70yKbV0Yn+FDK5VepdAMEK1LzlmByAYiVLl9WS6TFGUCiL1mXBI4yFMOXAKFzxxQdJaYt/6fXPO4+FIOMkDJZ8ENoqSD6rs9EGcYMREEuuhZEo6MOuoWWnHlk9aTmNNY1p/Sv+kq25lTYGmuKIDtlxdvupyln1hGSPrRtIUaKIp0JR0TWp/bvKslD/toQgDqzsfMY6Vpz2kyo6i9BVSnwmZ1hfOWiIbUaJc+eyVac+XZV9YVrw/QOkXqLJTIYQioaRFocHgt/w0GLBCAfD6YcBQSDH3Ro2dijpqoiz5nyXMfXluxqQDNd6atGPBSDDeh+6sKw3VDSw+eTEbdm2I77yNGTyGcCTMh598GM/E5mTMCUXsbDuJxWrvWHcHoUiIjS0b8Vt+Fp+8OM2iqG5qlY9bNjaAjnAEyJ69SVGU4uL6XDdA21YIB3OuKRLTRSc+E4bUDOl2koFMiQycBDa6/lC6S9koOyLyb2AXEAHCxphxpR1R+RCKhHi3+V0Wv76YKQdOSXY/O/oaxjzxQ6yWLfD1+2DYgfHJyS2Y3Km547Y7HwgHku7rBAZ2JSBd6dtEIxHaw+1J9RLmT5jP4tcXs2rDqvj7sYPH4rW8VHurmXHYDGatnhVvP2f8HAAmPjgxLpM/+/zPiJoogXBAkw/0EdxidgBNUqAoJSZjopmoB+ueL0PzeqgfndeaYuEJC/FZvvgzYdGJi7qdZCBTgoKPWz/mwpUX6vpD6TYFlxYRqcrnWAYmGGMOVUXHxkkYsDWwlctXXc7kMZPTzcMvzGbT126n6YvXE33mZ/auTIzt7du5+bWbmXnETO6ceCczj5jJr1//NV7Ly5zxc9KCAEfVjUoLILQsS+ucVDhdSRUNdpzNptZNfPjJh2xq3UQ42ukv3dTexIxVM9Lc0iaPmZz0PtEK6Sg6zvlZq2fFzzt1oHZ07IjX3bn0yUtVvvoAHSnZ2PwesY9r+mlFKSkZE83s/I+t6ID98zdnw66N0PwhtGxme8D9um3t2+LrjCpPVbeTDFiWlbY2mTN+Tjypia4/lO5SDMvOC8BheRxTspC4g3LLybewsXVjRvezje1N/HDtTSz879mMiUbjGmw0Gk2zBF0z/hrCJsyCVxckuRbNWzOP+cfOZflnZxKsqsXf0UpD1MOmcLvmva9gupoqOlNCAcdSE4qGXeVhkH9Q0vtQ1K6onanidmIWQLfrVb4qn45wNNmNzdsZs6MoSunImGhmwODkhs3rYecGuGMi1I8mOO0x1+saaxqZ9ddO6/3c4+ay5H+WsLltc5eSDLSH29PWJgteXcDlh1+ePE59PihdpGDKjogMB/4LqBGRzwISO7UbMCCPLgzwJxExwC3GmFsLNbZKJHHnJWIijKwdyc7gTlcT787gTnvH42/XsPyUpTghgFGiaZag2atns/SUpTQF7B36xH6sLf+g8e6vdA6ifjT+i/6kdU4qmEw7eJmCRbMlFBheOxyf5V4nZ2dwZ9J7p6K2H8mrfep7la/KJhyJEo6aZDe2pJgdRVFKhSWW67xsVQ9Kblg/Glpj3iLN6/E3/cv1ug27NiQ9M6589koWnbSIaSunxdvkk2TA7/G7rk30+aD0lEK6sU0EfgGMAm4E5sVe3wOuyuP6/zbGHAZ8Afi2iBybeFJELhaRNSKyZuvWre499AEcl6NAKMDMI2ZySOMhLHtzGTcefyOPvPsI14y/JsnEe834a7hj3R0c0ngIM4+YSSAajrsqZQr2M5j0nPcTFtCw6rrkwTSvpyGK1jnpJuUgs11NFR2KuFtiHEtNY3UjCyYsSKug/ci7j8TfL5iwIK5INURh4TE/TW4fk+VM16t8lYZCymt72LbeJLuxqWVHKSzlMMdWIhaW61rC8tbYCg7YP0/7FaxeEL+uYdV1LJxwU9J1CyYsiNdJc9jYuhGPeLhz4p3x50Eu92lwr6umzwelEBTMsmOMWQYsE5GvGmMe7Mb1H8V+bhGRh4AjgOcSzt8K3Aowbty4PlmVzs3l6Jrx1/DLV3/Jin+uYMbhM6jx1rD0lKVETIQPdn7AL1/9JYBr3ZyG6oYMgYK+9NolUewkB4nUj7brnEQ9ae5tlqHTdqe4Ug4y29VU0T7L59reJ7alxvJ4qPZWJ9VBGOgbyHmfPo/zP30+gXCAam91p4ucx4c/pf1ulo+rD7+cH+w3BX9HK/VWHVcffTU/iP5As+2UkELKa3vItt4kWnYcNza17CiFohzm2ErEEuHet+5Nche79617ufroH8FFT9nZ2ETgDzNhw5r4ddHR4/F6fEnzeX1VvWv5gA93fcj0p6fH426qvdV5jCu9rlp9VT1XH3M1P4jo80HpPsWQmMdEZIqIXCUiVzuvbBeISK2IDHR+B/4HeLMIYytr3FyOZq+ezYUHX8iLm14kFA2xe81QRmAx0ngYVfdfXPqZS/np+J/GU/w6113258swxjDv+HlpwX6WMfHaJSPrRtJY04hV02BnXknc1TlrOUTDWDvep3HVdYxcMpHGu79iZ2tp0120SsBtpyzbzlij5Wf+8TemWWIaY5l0trdv59InL2X609PjCQXmvjw33l8wGmT+mvnxANLtAvPXLSEYDcbP3/Dar6D5w7g8ef/4AxpD7YwMR2iMRGxFWqloHGXHp5YdRSk90Si0bI4nGmgw8O39p3DDSzcwbeU0bnjpBr69/xQaIhGo2x3q94CBI2HCVUlrgqajL+GmV25Kms/vfeveNGv/nPFz4tYeJylNNJrf9z51beK1vMlrFVV0lG5QjAQFjwA7gVeAjjyv2R14SEScMd1rjPljEcZW1mRyORo7eCzLJy2nwV+PtfWfdoaUumEEJ12flALYsQKtbVrLxtaNrN+1nrv+fhe3nHwLO4M7aQo0seDVBcz9/M/Tb25ZdopJZ1cnGoaVP4S3H+80Z//5J/YuT/N6u41S9rjtlGXbGfMGWxm7ehHLTriZkMeDLxKh8S/z8R73AxjQkLFa9vSnpifJofNgixr3JBlRKxbGN2ocHHkJ3PmFjOlOlcrDUWjcUk87ipCiKL1ANApb3rLXDbE51jrrHsasuZvlh3yX4IAG/G3baXjih1hfWdJ5XeqawOsHMa7zeWNNY/wZA3Dls1eytmltvKuNrRvjCpKilIJirCZGGWPOMsbcYIyZ57yyXWCMed8Y85nY69PGmGuLMK6yx3E5SmRk7UhqvDX2jkZgW3zC2n7sFVy2+oeuViDnup3BnazasIpLnrwkHvTXFGjCb2XQcS3L3tXx+uGu02xFB+wJ8tH/hfGxoMH60XYbpSJIs+Jl2xnz+vF+8BzDFx7OHvMPZfjCw/F+8Fz8806VUbdq2bNXzyYay7aWKUlG1BdTdsbPsGUrNd2pWg4rmrgbm2tRUbXsKEqv0ba1U9EB++eKc7EOnEzjvWfbFvZ7z7bd2D0pxX6dNUH9HlC3O8YY1/k8HA3HnzFOkoFENKmAUmqKoew8LyIHF6HfPk9Ol6NwMD5hBQc0ZEwBnJi4IPX4wuPn01Az1H0Ajqk72AYTr7N33R2a10PN4AT3tljbPE3TSoUwYGi6O+PX77OPky6jDVXuchiNhqH5Q6IZUlVHLQ9MfdzeOawbljyG5vUQCsRdLlTGKg8nLsctG5tadhSlgKS4qKXNlwnrhjjN62HoAXDO/fY8fM79MOV+qN09a1+RDPN5xHSmle6q67Si9AaFTD29Djt9tBeYJiLvY7uxCWCMMYcU6l59lZwuR16/vfhsXo+/bbtrIPnI2uHMOmpW3J2t8/gIlp9wMw3id9/ZdzF1J7mu1Y+G+j3h/EeT3dvU5ahv4ea6MGBo/PNNlVErQ2ppfyQMNx2K/7zfuZ/f9j7c/RVbhiYvgqd/3BkIWz8amt6B5WeojFUocTe2pDo7TlFRVV4VpSC4PbdT50uPL75uiLPfJAjsgMevSLjuXmj+D9zz5Yx9eTOUHvBK51Kyq67TitIbFFL6TgW+hJ06+lPYSQa+lHBcyYOsLkcJu+4Nz81j4dHJqSMXHvcLhgVaGJaQHWVk7UgWHjWb4SvOp3Hh4VjLJtlm7cTdoNYmu0pyqqnbcV1zJj1fTbp7m7oc9T1SXBdSlYxEGR1mVbEwJQnGwuPn0dC2A4ilKh1/bfL58dd2pjlvXg+PTIfjvm+/d5SfZ6/vPK8yVnFkc2NTy46iFAg3F7XfnG0/zx3rDAJnLEu24pzyc1hxTsp1U2DH+1ldihurG5k/YX5aaujG6uSabV1ynVaUXqCQqaf/AyAibrbKXYW6T78mYdfdCgcZ46th+ReXEwwH8EdCNKz8EdY/H2PM/qeyfOKvCXp8+Deto+GPP8Jyds2d5ALOblDdMDjxx/buj5upe/eD7F3+AUPhk4/c22iygn6LFWxlzOpfs/yEXxG0vPijYRr+sgDrM2fb54ExEUlOXR6R5F2W5vW21XDq4zBwODx0SVK6U5WxyiPgknraEsFriVp2FKVQZHJR27kB7phobx5N/YPtFpxoxTnzLvvZn3ht83rwDUjvK2Hu9Xp9jB00hmWnLCUUDeOzvDRWN+L1psT6KEqZUYxsbK8CewA7sF3Y6oFNIrIZ+KYx5pUi3LP/4Oy6Yy8kG8HewVk2OT5xWf98jMZNa+1J7g/fT57Q6kfb+fOd3aCJ19k76xOvSzd1148G/4D4/RLd6JLaaLKC/osI1gfP0fjaPZ3H6kfD2FPs38fPwHpgKo2pMjPxOlhxbuf7pnfs92fdAy71nlTGKgs3NzaAKq+llh1FKRSZnsmtMWuME//48KXJFpvfng+T5tmuwonXhdqS+3eZe71eH8PrRhThj1GU4lEM2+KTwBeNMY3GmCHYbm2PAdOBRUW4X0mJmihNgSY2tmykKdCUV5Vg+8IMQYWJx3dttl3MMgUeRsL2Dk407L67YyJ2jM1+k+xjTnIB5zzYSQea19tVkk/7VcbAdCBn8LpSpuQKYE1t3hWZFg+cvjhZJqbcb8vV1MehcT932awd2tn+rHugttFuX9tov0/s79yH7GhATVhQMbi5sYFdd0ctO4pSINyeyaf9yn6eO3TsdJ+DG/ZNvu6MZTBkbJK7W/SCx2myPF1f3yhKmVEMy85RxphvOm+MMX8SkV8YYy4Rkaoi3K9kRE2Ud3e8Gy8E6mQdGTN4THYf1UxBhUP3B6eOjnPcCd5u2ZIcLBgJw+Y34bfnZbbKbFoHK6+yFZxJ8yDcbicXOPTszvaBHfbvG9bYyQgmXmcvRAeNsouKJcZr5AheV8qQfAJYE5t3VaY9PjuWa9I82wVit1HQuhke/pZ9v6mPu8vmoFEw403w+KFjl23VccZ37kPwjacgErT73rUpa9CsUn60u7ixOe/VsqMoBSL1mSwCf5iZ7AbsPONT52DL0zlvh9rsDaqOXXF3t+j+p/LuST/ksj98s2vrG0UpQ4ohsR+LyPdFZM/YayawWUQ8QJ/aFtjevj2+KAQ7BeNlf74sXj0+I6lBhXXD7IDCnR/aP51UvE7w9vgZ6cGCLZtsRSeTVcbZ3akbZrcNt8O2d+2FaGL71Qs6gxdP+bm9E7/bf4GVwQc3R/C6Uma4BbCuui45gDXBUpJRplszWIaiEbj/AtsdYukk2PEBPHhR5/1M1FbYE2Vz8iLbIgQQDcGTVyeP754vxxxg97Ctk24BuJqwoKxxrDeplh2/16Ij1KceA4pSRgic+KPkZASD9oCz7k2eg89aDn+8qnPeXn6GvT5YMSU+124/7Bwue+byrq9vFKUMKYZlZwowG3g49n517JgHOLMI9ysZqdXkIVYpOJIjmDoxqHDUODjh6s7Ciqkpn536NpAcLBgJdfaRaJXZ/SDb4vPnn9jnMvXttB/5WWjbnhy8eNqv4MVbYMJVuoNe6aQGsI4aB0deAnd+wdVSklGmP/kIbj0x3bISSek/NdGFWPDUbFvWagbbu4xP/xi+fCv88rBOeWvd3LkbmSjnmQJwNWFBWdMeiiCA15Kk4z6PFa/BoyhKD3Gz3J95F6xZ2lke4qu3294aiVacAYM7s6o6+AYkzbWZavnlXN8oShlS8FWsMabJGPMdY8xnY6//NcZsNcYEjTH/KvT9SklqNXnIs1KwE1QI7hXknZTPYLcL7Oj83QkWdHLnO2xYY7usWV7754Y12ft22kN6CspH/9d2ddMd9MonUdbAXSYSPueMMu0kDUi1rKT277hMJL5v2WK7qS2dZP9s2WK7UDj9Jco7JMt5av+p55WypD0Uwe+1EElWdmw3NrXsKEpBcLPc//Z8+/ntvH/wG7bVJtGKEwykz6uhtqRjTi2/RPJa3yhKGVJwZUdExorIrSLyJxH5s/Mq9H3KgW5XCk4MKnQSBCTiWHMcl5/VC9KTAdQNhzPvTjZNn3m3ffzch2zz9dD97R31UeOS+x66v33+3IfAmMz31x30yic1gLV2aFZLiatMHzWbhufmubZP6//1++CcBzrdKGoG2wkLUl0sg63J/SUmLEiUc02KUZG0h6JpLmwAXo+oZUdRCoWb5btuGAw7wJ5/z7rHfp+aUvr5m9LXD4NihUVjxxpeXc7C4+d3fX2jKGVIMdzY7gcWA0uAPv1U63al4NSgQrfgwfo97SBtywNfW5qeDMDjtV3Wpj1hu7R5fLaiI5Ydn5Pqlua4xdWPtpMgrLzKXjRWDXS/v7NDrzvolY1bAGuW9OFpMo3Q8Nj/66zTlNI+rX9/rR17lih/X10Ck2+2ZTOww3aRdHYenf6chAWpcq5JMSqS9lAkLTkBaIICRSkoqamnR42z6+bdnZDQZfKiTku6wwfPwUnX2OUpomHbI6RumB2rG5trLa+fMTVDur6+UZQypBjKTtgY8+si9FuWOJWCu35hLNA/GrWVDscUvd8kmHit3cbjs03L0TBExA72jmKbrp2FX2rGtJbN6WbtR//XtvCsvKpT8XHckb65Kvn+iTE7uoPeN0iozZQmby6WkiSZjkbh+P+DTWuTs6U5qaAd5cPpf+eGzsxqEHOjuAjOeRBat4C3Ck68Gp6OxZQ590+V40zjVyqC9rC7ZcfntWhpDZdgRIrSBxkw1J6Pd7xvW29qh8HyrybPv49Mhwse61SKHC+QqkF2Apgw9jxu+dLmWgu6t75RlDKjGMrO70VkOvAQ0OEcNMZoCg83Eneuo1G7GNhdp8Hex8LnLrL9bxMnqJoGWDYpcxreTAHdux9kKzyOhcc5Hgqk7/yLB740X3fQ+yJdtZSkts+VCjqSoeZTsMX2F3fq6py2EELXq6Wmj5LVsqNubIpSOBI9Ob65yn3+hWQvkNrdoentvEsSKEqlUwypvgC4EngeeCX2WpP1iv6Os5tiWZ3JAo7+TqeiA7HAw/PsiS1bGt5MAd0eX2figsTjXn9yOulBo2C3EZpWui/T1fThie1zpYK2PO7y5/iMN6+3LT/hdk1f3odpD0XwuVl2PJp6WlEKRmqCAt8A9/lXLPvZ3rC3/bN9u6b0V/oVBbfsGGP2LnSf/YZEq4zlsX1oE1P2rl5gx+qcdY/9u5OaOjGJgBPQnbpjUzc8p/uSouQkU0BsOGi7tSF2jNkDU5PdIsOBzvbN6+0dRqXPktGy47XiNXgURclCNJrssu5mAU+dj8MBe/4NbEsoFjrE9tjIdh1oQiKlT1NwZUdEBgDfA0YbYy4WkTHAfsaYx3Jc58G2AH1kjDm10OOqCDwJwYaW1w40fGR6cqAhkhx707IlOYlANjclDfRWeoonQ0Ds0i92yumXb82dkEDTl/ZpMmVj83lEExQoSi7c6ue4uZmlJijo2GXH3iQmiPnyrfZ6IpHU60ATEil9mmKsdO8EgsAxsfcfAXPyuO67wD+KMJ7yJxK2A7ujIThjmT3ptGzuVHSgM9BQxLb2vHiLnU3l/Edt5aW1CXbFqty3bbUVmVQ3oa66Lyl9g2jUlqfmD+2fkXDy+2g0//YY++HpuEoc9/10OX3oYttNbekkWzE/7ko7JTV0Ku3eql7785XeJ5dlxxhTglEpSoXgVj/Hzc0sNTW/WPb8mzofRyPZr1NPD6WPU4wEBfsaY84SkbMBjDFtklpZLgURGQVMAq7Ftgr1HyJh2PymHY/jZGM793e2UuNmZt65odOyU7+nvaNeNyzdCqTBhgpkqLB9Nzx7Q2eF7URZSW2/3yQ4bmanfNaPtt0kvrQwlu58mLucNo61U0kDPPtz27Jz9HTb0vP0j+0+lD5LIBRhmItlx1GAOsJRqn2etPOKotA1NzNvNUya15mNze26aIrbsHp6KP2MYkh2UERqsJPTIiL7kpCVLQMLgJnYiZX7Fy2bOheSYC9A7/mKrQS5BRoGdnSmkw4F7N/Hz0jfXddgQwUyVNg+L7nCdqKspLY/9Oxk+Wxeb8fjONnVolF3ObW8tgXR67drOqw4126/4tx010ulz9EeirpadnwJyo6iKBlw6qElUj86PfambaudGXP5Gfb82vR2hutcNhbU00PpRxRDumcDfwT2EJHlwNPYiowrInIqsMUY80q2TkXkYhFZIyJrtm6toEV8qktQqstQJJRhBydgu/ukVp5fvaCzTcdO+/eawRpsWIaUhcxm2iGsGZz83pGV1PaZZGvofnaFbn8tnL44XU6dh6u6S1QMhZTXjnDENWbHOdahcTtKASiLObYYiCfmvZEyr3r8yeuJ1Pl69QL360Syr0MUpY9TjGxsT4rIq8BRgADfNcY0ZblkPHCaiHwRqAZ2E5F7jDHnpvR7K3ArwLhx4yrD4TufIEOPzz1QsGqgHfdwwWOAgaZ3kmvk1I+GcMxgFtihwYZlSFnIbKZA1MCO5PeOrKQmIMgkWzv+be8m1o+248wctzYnIcGX5ttt1V2iYiikvNoxO+ney2rZUQpJWcyxxcCy7Hk0MRvru0/ZvzvlKepHJxcLBXt98OItMOV+aGvqnI+/8HNYcpK6uSv9loJJuogc5ryAPYGPgY3A6NgxV4wx/2eMGWWM2Qv4OvDnVEWnYsknyLBuuJ1KOnEn5vTF8NAlcMcpsOxU2yfXW2O7/zhtJi+CwXvZv69ekG4F0t1zBdwtK2fenZww4NyHbKdTtwQEr98HZ96VfP3kRfDs9fb75vVw/wW2T7iTkGDCVcmyp+4S/YpI1BCKGHfLTkzZCahlR1EyM2CoPY+uvKpzXv3chfDMz20FaOrj9s/A9vRn/3FXwtM/SUgQMxNevE3d3JV+TSEtO/OynDPACQW8V2WQT5ChWOCr7QwwDLXZyk1i+1DADupO3OVxgrwTK9t/4ymI6O65koCbZaVmiG15+cL1ttzs2mT7fbslIAjsgJeXwNQ/xDo08MC05OK0zethyBg7IYHKXr+nI2wrMm4xO1W+mLITVGVHUTLiNm9Ho3DkJXa8rjNXn/s7+MP/S14bvLzEtuRMvNaOnfRWwQsLk/tXN3eln1EwZccYMyGfdiJysjHmyQx9PAM8U6gxlZx8ctk7AYapbSZeZwdz14+2F50tW+z3qf3U7V78v0OpbBzLSiLO+5bN6dbHB6Z2yh/Ysnbi1fY1Ozd0WhgdHBkdNKqof4ZSGbSHbBc1N8tOdexYmyo7ipKd1Hl754ZORQfsnzs+cF8bOPM12HO8urkr/ZxSbL9eX4J7loZ8grPDQTt971n32Kbps+6x39cM7mxfN1yDvJXikEn+amOylSprdcNtN7hUt7i64aUZv1J2OEVDfS7KTlUs3XQgFO7VMSlKxZGa3Eg86Z4iz14PZy3PvjbQJDGKUpQ6O7nIWnOnT5FPcLavJr1GzuRF0LCvfZ3TXoO8lWKQSf4G7+Xulubxwu4HwbQn7EyCHp+t6HhKMZUo5Yij7Li6scUUoNYOtewoSkbckhudtdyue/b2453tWrbAbiOzrw10/aAoJVF2+k7GlHxwcyFKJBpJr5HzyHSY+kT6hKUua0qhySR/33jKTijghseb3WUtGrXdM/XB2i/J5sZW5Y1ZdtSNTVEy45bcaMU5cP6jsHldcla1mobk+dWxCKXOv7p+UPoxuh1baiIZkhg0/xse/pamiFSKSyb5i3QzeDWfdOtKn6Y9jwQFbUF1Y1OUjGRKbmR5s1todP5VFFdKIf3/LsE9yxcniUEiTh0UTRGpFJtM8tfd4NV80q0rfZq4G5trggLbstOmqacVJTPZ5uVsafx1/lUUVwpm2RGRr2Q7b4z5Xexn1nYVTXfcd5zgwcSdmNN+ZRcQHTUOxs+AYJttllZ3IMWNnriNuclfavBqV/rPJ9260qfpcNzYXCw7Po9gibqxKUpWBgyFr98Lv5mSMC/fmzupgM6/iuJKId3YvpTlnAF+V8B7lR/dNR8nBg+GAtD0jq3oAJxwdXJOfTVHK6n01G0hV/BqV/vPJ9260qfJZtkREaq8Hk09rSjZMFGwfMn19yyffTybQ47Ov4riSiHr7EwrVF8VSSbz8UVP5Q4MdIIHo1HoaLEzrEy8Lj2nfr79Kf2HnsidQ7bg1a72n4+lSOnTZIvZAaj2WRqzoyjZaNkE956RrrRMeyJ7chidfxXFlaIkKBCRScCngWrnmDHmJ8W4V9lQCPNx4i57sE3N0Upuiu220NX+Nc1pvydbNjZALTuKkotIKEPimFD263T+VRRXCv4NEJHFwFnAd7Br6pwB7Fno+5QdhQr0dnbZ/QMKGziu9E0KnWCgEP07MpwpiFb/1VkOAAAgAElEQVTp02QrKgp2RjZVdhQlCx6f+7zr8eW+VudfRUmjGJadY4wxh4jIWmPMNSIyD3iiCPcpD5zg7WgUznnQThnt+NgO3qf75mM1Ryv50B05SU04UDMEAtvcdwJVDpUu0p4lQQHYhUU1QYGiZKFuOEy5H3au71xPDBptH1cUpcsUQ9kJxH62ichIYBswogj3KT2Jwdt1w+DkOfD4FcmLwu6i5mglH7oqJ24JB868G569wa7MnZqAQOVQ6SLZEhSA48amMTuKkpVIR/J64qx7Sj0iRalYirFieUxE6oG5wKvYdXV6sOovYxKDt8fPgIcuLmx+ezVHK/nQFTlxSzjw2/Pg0LM736fKrcqh0gXawxG8lmCJuJ6v8qobm6JkpWUTrDg3eZ5eca59XFGULlMMy84NxpgO4EEReQw7SUF7Ee5TehKDt2sGa0IBpfzJlHCgZnDye5VbpZt0hKJUZbDqAFT5PGzZ1dGLI1KUCqO7CQoURXGlGFu0Lzi/GGM6jDE7E4/1KRKDtwM7NKGAUv5kSjgQ2JH8XuVW6SbtoUhGFzaAaq+mnlaUrPQkQYGiKGkUTNkRkeEicjhQIyKfFZHDYq/jgQGFuk9Z4QRv14+G1Qtg8qLOCUoDuZVyJFFmoTNm5/X7Ot+r3Co9IJeyo25sipKDuuH2vJw6T2uCAkXpFoV0Y5sITAVGATcmHP8EuKqA9ykfUoO3fTXwjacgooHcSpnilnCgZgh8aT584XqVW6XHtIei+DJkYgPbjS0QjGCMQTLE9ShKv8bjhd0PsouIRkK2RaduuH1cUZQuU7BvjjFmGbBMRL5qjHmwK9eKSDXwHFAVG9MDxpjZhRpbUclWfV5RyhE3mVUZVgpEezi3ZccAHeEo1T5P7w1MUSoJjxcGjSr1KBSlT1CMbYLVInI7MNIY8wURORA42hhze5ZrOoATjDEtIuID/ioiTxhj/laE8RWf1DomulOuVBIqv0oPaA9FMtbYAeIKTlswosqOonQFnZsVpVsU41tyJ7ASGBl7/w4wI9sFxqYl9tYXe5kijK34OHVMlpwECw6yf255yz6uKOWOyq/SQ3K6scWsPpqkQFG6gM7NitJtiqHsNBpjfgtEAYwxYSBnNKqIeETkdWAL8KQx5sUijK34uNUx6Wm9HUXpLVR+lR6SO0GBbc0JaJICRckfnZsVpdsUQ9lpFZEhxCwzInIUsDPXRcaYiDHmUOwEB0eIyEGJ50XkYhFZIyJrtm4t4y93pjomWrek31ExMpuIym+/pVDymsuNrcpnn2tVZUfpIRU5x3YXnZsVpdsUQ9n5HvAosI+IrAbuAr6T78XGmGZgFXBKyvFbjTHjjDHjhg4t47S4meqYaN2SfkfFyGwiKr/9lkLJa3somrPODqgbm9JzKnKO7S46NytKtymGsvMW8BDwMrAZuA07bicjIjJUROpjv9cAJwP/LMLYio9bHROtW6JUCiq/Sg9pD0dypp4GdWNTlC6hc7OidJtiZGO7C7u2znWx91OAu4EzslwzAjtttQdbAfutMeaxIoyt+LjVMdGMKUqloPKr9JCOHJadzgQFquwoSt7o3Kwo3aYYys5BxpgDE96vEpG3sl1gjFkLfLYIYykNWntHqWRUfpVuYozpQuppdWNTlC6hc7OidItibAm8GktKAICIHAmsKcJ9FEVRlDIiGIliIHvMTkzZaelQy46iKIpSfIph2TkceF5EnLQho4G3RWQddkmdQ4pwT0VRFKXEtIfsmh/ZLDs1jmWnQy07iqIoSvEphrJzSu4miqIoSl+jI2Rba/xeydjGYwl+j0VLX3Rja9sO//g9+AbAAV8CX3WpR6QoitLvKbiyY4z5T6H7VBRFUcqfuGUnixsbQLXPorWvWXb+9RT87mJo22a/H34IXPAo1Awu7bgURVH6OZrGQ1EURSkIbSFbgan2erK2q/F7aO1LMTv/ehruOxuqdoMvzoPjr4Itf4fHryj1yBRFUfo9quwoiqIoBcFJJ13ly2XZ8fQdy87G1+A3U2C3UTDxOhi6H+x5DBz0NXjzQfj4jVKPUFEUpV+jyo6iKIpSENpi1ppclp1qr4fWvhCz07IV7ptiW3RO/glUDew89+mv2O+f+XnpxqcoiqKosqMoiqIUBqd2TpUvu7JT5bNoaa9wZScSgt+eD21NMOEqqKlPPu+vhTET4Z2VsGtzacaoKIqiqLKjKIqiFAbHja06R4KCGp+Hlkp3Y3v2elj/PBz9HRjyKfc2nzoRTATW3d+7Y1MURVHiqLKjKIqiFITOmJ0cbmy+Ck9QsP5v8Jd5sO9JsO+EzO0G7QGNY2Htb3pvbIqiKEoSquwoiqIoBcFxY6vOkaCgxueJt604wh3w8KVQOwyOuDh3+z3Hw6Z10Pxh8cemKIqipKHKjqIoilIQ4padXAkKfBatwQjGmN4YVmF54WbY/gEcNR38A3K33+MI++e7K4s7LkVRFMWVghcVVRRFUfonbcEIPo/gsSRru2qfh0jU0BGOUp3D5a2sCOyw3df2OBL+67D8rtltFAwcAW//ET53Uc7mH7d8zLqmdWxq3cSOjh1YYjHQN5BPN36aw4YdhseqoP+XoihKGaDKjqIoilIQ2oLhnGmnwXZjA2jpCFeWsvPiLRBsgUPPzf8aERg1Dt79k+0C561ybfbSxy/xq9d/xWtbXosfs8QiaqLx96MHjub7R3yfY0cd2+0/QVEUpb+hyo6iKIpSENqCkZzxOkBcwWnriEBdsUdVIMId8OJi26rTsHfXrh1xKPzj9/DhS7D359NO3/nmncx/ZT5DaoZwxtgz2L9hf4bWDKXWVwtAa6iVv2/7O4+//zjffvrbXH745Vx40IWF+KsURVH6PKrsKIqiKAWhLRjOmYkNOpWdiko//c5K241tvy92/drdDwKx4IPn0pSdR997lBtfuZHPDf8cF376QqpcLD91/jqOHHEknx32We548w5bMaoewuRPTe7uX6MoitJvKHmCAhHZQ0RWichbIvJ3EfluqcekKIqidJ22YISqHDV2oDNbW2slZWR74zdQM9i20nQVf61di+eDZ5MOv7/zfa55/hr2b9ifiw++2FXRSerG4+eigy/igIYD+PELP+bt7W93fSyKoij9jJIrO0AYuMIYcyBwFPBtETmwxGNSFEVRukhLezgej5ONmkqz7LRtt2Nu9j4WupsgYMRn4KNXoKMlfugXL/8Cj+XhkkMuyTvxgNfy8q3PfIsB3gFcvfpqwtEK+R8qiqKUiJIrO8aYj40xr8Z+3wX8A/iv0o5KURRF6SrNgRADqnJ7RyfF7FQCf38IoiHY54Tu9zH8MxANw/oXAPjbx3/jLx/9hVP3OZVBVYO61FWdv45zDjiHt7a/xYq3V3R/TIqiKP2Akis7iYjIXsBngRdLO5LsRKOGrbs6+GhHG1t3dRCNmi6dVxQlf3r6fdLvY+/xSSBErT//mJ3WSrHsvPEbGDQaGvbpfh/DDgDLF3dlu33d7QyuGsxJe57Ure7G7T6OAxoOYPEbi9kV3NX9cSl5zRE6jyhK5VI2CQpEpA54EJhhjPnE5fzFwMUAo0eP7uXRdRKNGt7evItv3rWGDTsCjBpcw23nj2O/3QdiWZLzvNJ/KBeZrWR6+n3S72P+FEJed7WHGeDP/VipKDe27R/AhpfgsAvsNNLdxVsFQ/eDf/+Vf+34F3/7+G98dcxX8Vm+bnUnIpwx9gx+8refsPTvS/nOZ7/T/bFVKIWQ2XzmCJ1HFKWyKQvLjoj4sBWd5caY37m1McbcaowZZ4wZN3To0N4dYALbWoPxCQ9gw44A37xrDdtag3mdV/oP5SKzlUxPv0/6fcyfnsprMBwlEIowIB/Ljj+WoKASlJ1199s/9z6u530NPxg+foPlb96Jz/Jx3Kie9bnXoL0Yt/s4lv9jeb+07hRijs1njtB5RFEqm5IrOyIiwO3AP4wxN5Z6PLkIhiPxCc9hw44AwXAkr/OKouRPT79P+n3sPXa1hwCozSNmx2tZ+DxCa7DMPwdj4I37bCWlbljP+xt+MEETZeV//sS44eOo8/e8yNCkfSbRGmrlN//8Tc/H1w/JZ47QeURRKpuSKzvAeOA84AQReT326kYhg97B7/UwanBN0rFRg2vwx6qG5zqvKEr+9PT7pN/H3uOTdttKk49lB2xXtrK37Hz0Kmx/H/aZUJj+hu7P6rqB7Iq0c+TwIwvS5Z677cnBjQdz11t30RZqK0if/Yl85gidRxSlsim5smOM+asxRowxhxhjDo29/lDqcWViSK2f284fF5/4HN/dIbX+vM47hMNRNjYH+M+2VjY2BwiHo2n30oBIpT+SKPceC247L/f3KROu38fzxuGx0O9VgfkkELPs5BGzA3aSgrJXdtb+Bjw+2HN8Yfrz+Hli8DB2M3DgkMJVWDh1n1Np7mjmd++6eoErWchnjhhc43N9rg+u8ekzWlEqgLJJUFApWJaw3+4DeWj6eILhCH6vhyG1/qQgxSqvxU8nH8QAv8e1yF44HOWfm3dx6T2vxIMdF597OPvvPhBvrK0GRCr9ETe5Xzrtc/zijM8g5F+0MpHU76PPK/zgwbX86a0t+r0qIJ/E3NgGVOW3213t85R3goJICNY9AKOOtIuCFoC2SAervBFO/aSFqlA7kaqeu7EBjBk8hv0G78edf7+TM/c7E78nv80AxSafOaKuypPUpq7Kw7tbW/QZrSgVgCo73cCyhKED3Stdb2sNcv4dLyX5944aXMND08fHr9nS0hFXdMD2/b30nlf47SVHM7K+Jt6PW0BkYj+K0tdwk/upd77Mj049kEvufgVI/z7l6s/t+/ijUw/kT29t0e9VAWlu65plp8ZnLxrLlvf+DIHtsG+BXNiA57a/STtRvtjaSt3H69i519EF6/vUfU5l3ivzePS9R/na2K8VrN++Tr5zxE8nH8S0pS/H29w59XP86JE39RmtKBWAKjt5Eo0atrUGCYYjVPstgiFDMBLF57EYVleF12sRCkUIhiOsuOQojIFdgTDrd7Sx+Jn3kgIZQ5Goa7BjONLpyqYBkUpfIfG74/d6GFzjY0cg5PoeYGhdVZLsD62rYuywOlZcfBTNgRBPv7WZYDjCRzvaXC2riQTDEYbWVfGjUw+kvsZHcyDE4mfeo76mM92vfq8Kw5ZdHQDUD8gvlXKV1ypvy84b90HVbjDysIJ1+cSWNdR7azk0GGHbxtcLquwcOORA9t5tb25fdzunf+p0vJY+3vMhGI5w2YR9OWbMUCJRg8cSnn93a9oc0Vjn55bzDo/PI411/rRn9NC6qrznJkVReg+dDfMg0bVmaF0VM0/ZjysfWJvkgjamsZZ3mlr55f9n78zDpKjuhf2e6mVWmIWZYUcBWTW4gCtGQWPcJYlGgxoVExVzDXGJSW4+E6+JSa4aMRdN4pIY1KjRaCJuiUYFF3DDXREExAUQZoYBZp9e6nx/9FRPd0/1Ot3T2+99nn6mq+pU1emp3zndp845bz37EeceNp4fP9y3/YbTZlAWMmnX5TAYU1PW706S09E3PMeaEBmZRiZECvlE5LC0r05vYNHRk/sN4Vzy7EfBISM3nDaD6/+9jrc+38X+Y6v50XFT+HbvndevTm/gkqMmccbtryQ0dKTM7ehXXm84bQam7htbL+UqPTS19eA0FJUJ2NgASt0OWnJV3duxA9Y+AZOPDczZSQNtvi5eavmAI4btQ1etlyFb3knLcS2UUpw08SRufutm/v3JvzlpwklpPX6hMqTMYNroar4VUqf88eyZVJb0fR9/dXoDGvjl42uCaf5w1gF8dXoDT69pBAjWVYnWTYIgDB7S2IlB6B1p68faz06aHvzhBHDYhGGUuhw0dXq4+K9v8LOTpgcbOhC4I3TlQ+/ytwsPwefvDhxXa+6/4BC27e7G1JpOj5+xtWU0VPZ1fVuTJiPHAyc6MVsQcoHIYWmnzhzLY29v5i/nHYjDUPhNzUOrP+PUmWODQ0aufOjd4JCRRUdPYsXa7cH0TofBmXe8EnPoiNfrp7G9B5+pcRqKv6zc1K88/vab+wL2woPInqh4d2eTTV+oNLZ1U1PuRiX44M2ctrG9cz/4PTDp2LQdcvmOd/BoHwdXTaGtrodRa5/G0dOGv2RI2s6xb/2+jKkcwx3v3sEJ40/AUFl3EOUcHo+Ppg5PsH4wlOLmZz8K6/29+dmPuPrkvXngwkPo9PjZq6GSXz7+QViaW55bz1UnTmfNF21s3tnFoqMnhf02kGFtgpA7SGMnCqF3pG/85r7BCqy6zBV8f/rMMZx96B6c95fXgmlCt1ts3tlFe7eXnZ3efneYr//3Oprae7j17Jn98hBPdCAIuU7kcMwJdeWMrCplwdLXw+6Qlrn6Ynvzzi4mNlSy8sdzcTkUwyrdwfQPLTw05vBOr9fP2sZ2Lg7pObru1Bk0tXl46/NdwfSjq8tY+eO5/RonyYpBRCTSR1NbD1UJDmGDHLaxaQ1vLoX6aVCzZ9oO+2TjaupcQ5lQPoLWYZ2M5imGbH2XXePTZHoDDGVwwoQTuP3d21n+2XKO3uPotB27EPB4fKxr6girHx648JB+ozGuO3UGhoIzbn8lZhq30wjKivxay9BzQchR5NdzFELvSO/q8gaVk6HvLzhiAt+7982wNKHbLcbUlFFe4up31+fKh95l4ZyJQUFBY3tP2PnPufM1Fix9nTNuf4UFS1/nnDtfkyc2C3lF5PMpSlzOYJmBQDn43r1vUuLqu+8ypqaMMpeD0TXleP06LP2ODo9t+bJ6Exrbe4I/ZKzj//jhQDkLTe8wFKNryqkfUhLWKEn2SenyZPU+trd2h81ziEe5O2Bjyzld76eroHk9TPpq2g6509vOKzvXcmD1ZJRSdNTsgWm4GLI1vUPZAA4acRDDy4dz+3u3o3WO/W+zTFOHp1/9YGr6jcb48cPvYoVlrDR+U1M/pITRNeWUuZzyLB5ByFGkZwf7YSgen5/DJgzjgiMm4HQo/vqdg/n1k2u4dcVGfn/m/rR0eHE5DX520nRuXbGRW1ds5JYz96fba3LX+Qfx2Y5Oljy7nqb2Hq47dQZ+015KUF3mYv+x1SycMxGv36SprSd4frlLJOQjoeWpzO3g7gUH8WlLJ+VuBwp7AYGCkCEjFWg0W3Z2oiPS37piI9edOqPfHValAs/E8Jn2d1etYWpWekeUThe7chdr0rGU0z62t/YwrrY84fSVJU5MHVBWV5fn0PDclxZDaTWM/3LaDvmfprfwY3Jw9WQAtMNJ27DxVH2+ms/TdpYAhjI4fvzxLP1gKSs+X8HccemzyeU7dvWDzzRtJSZ+UwelKErZ1yv+kIa6DD0XhNyl6Bs70YahjKou4exD9wgbbnPr2TNpGFpCU2tPUDlp/Xha9tYWerwmP/z7O32THM86gPYeH39ZuYkrj51qKxzw+k1+eOyUsB9vd5wzi+FDS0RQIOQdkeXpoi/vycn7jQkrL3YCgvm983CCAoM7XrVN/9bnu7hr1Sbuv+AQtu4K9KjetWoT5x8+gW/d/gov/miubblpGFoa/OFy16pN/OrrM2zzHykGiTfpWEQiAXZ3etnd5WX40NKE9xlSGvj62dmZQ42dLW/ChmfggHPBmfhnice/mlYzsqSWsaX1wXW7h09j3PvLcLd+gWfoyLSdC+CwUYfx9CdP89vVv+Xw0YfjSpNkId9xGqpfeXU5lK3EZFeXNziM7dazZ4bJCKCvh9gikWfwCYKQHYp+GNuODg+PvPk5fznvQJ674kjuPv8gvH6Ttm5/v+E2C//6Bl6fyUU2w2QuOGICV/Q2dKz1F9/7Jt1ekwWzx3PHCx9zw2kzwp7AfMNpM3AYirtWbeJnJ03ngQsP4WcnTeem/6zDZ2rbJzbLXSJhoJimzthTvyOHdZ02a1y/Z0pd+dC7LDp6EkC/Sb2nzhwbM/2YmjKuPHYKhoL6ISVMrK/kiq9ODg7XeeTNzfzx7Jlh5eaPZ8/k3pc3ccbtr/DLx9dw2TFTwp6ObtrcnbX2jzbp2BqmZvv09SIsp5t2dAAwIoXGTs4Y2bSGp34KpVUw5cS0HXZ7zy7e2L2BA6smhckbdg2fBkDVZ6+l7VwWTsPJGVPO4LO2z7hv7X1pP36+Ullq9KsfFMp2iLllFbS++//fidP71SuhUiHoewaf3RBZQRCyR9H37Cg0J+47OqwH54bTZuCPMhwm2jAZt9OwXb/nsHK2tXZz6swxmFrzv9/4EuNqy1FKsW13NyOqSmwnPmpTy10iIe1kekJ95LAup0PZlouJ9RU8f+UcVO+yRTTBx/i6Cp674khcDsWuLh8LloZoYs86gLrKQOPixmfWA4EhcZZtqb7CzYgj9uKcw8bjchq0d/s45ZaVtp8/8u5svEnHcjc3wIbGdgBGVpfFSdnHkNJAb8OuTg8dPT7KXI7s/t/euR8+exkO/T64Ex+OF48nGl9Dozm0ZlrY+p6Keror6qj+7DWa9pmXtvNZfKn+S8yom8Ef3/kjx48/nobyhrSfI99o7zZZ8eF27rvgELTWKKWifqdHLhsKli44CEOBqaHEqXA4iv5+sSDkBUVfUnt8Zr8eHOuujt1kQ0dvN3jkemeU9X4N8+94lTNuf4X5d7zKT/7xHk6Hwfw7XuGbt71Mt9e0n/io5S6RkH4yPaE+UkjgMIyo5WiPYRX9ylM0wce67W0cdePzdHvNfhOML44QHDzwxmbcTgd7DKtgdE05brczWI4UKuxp6XafP7TcJTLpWMopvLt5F2UuByOT6NkZ2tuzs2ZrK4f85lkue/DtTGUvPs0b4MkfwvC9Ya+vpO2wWmuWbX+FvcpHMbykOnyjUuxumMqQLW+hfD32Bxgg86fOx+v38rOXfoapzfg7FDhup4MH3tjMEdcv58gbVnDE9cujf3eb4c/iUkrxlcXPc9SNz/OVxc8z/45Xi1JEIgj5SNE3dqLd1en2+rnu1PBhZ3846wBWrW/iD2cd0G+9T+t+6a87dQYlThW27o5zZuFQfXeO2nt8tuePtOhkcuiRUDxkekJ95LCuLo/Ptlw4exsEShG2/eE3Pu9Xvv549kwefiMwjdth2PcU+U0zmD7WMLJon7/L609oWFuxDlOLxysf72BiQ0VSDb3aihIMBbe98DFt3T6Wvb2Vbm8WxA497fDQeaAc8OUrwUjffKs17Z/zcec2DquZart914jpOPwehm5+M23nDGV4xXDOmHIGq75YxZ/f+3NGzpFP2JVnl0PZ1jkPrf4suHzdqTMCwxxDKFYRiSDkI0U/jM3lMGwnGG/d3c2tKzbyy3n7MKG+Aq9fc/vzG3nwjc2cPnMMSxcchMuhcDkMyt0GnT3+4Nwby+hy16pNXHPKPvzj4sPw+s3gEBdLn7t5ZxeNbT1xJzjLszyEdJHpCfWRw7qUUtz49Lp+5cISBChUv3LzxDtbeODCQ4L5rS518j+n7MNVJ5rBnqB++XcYts/NSfTzb2xsZ8HS1+MOayvWYWqx2NDYxkfb2/n2IXsktZ+jt0dse2tfr8babW3sN7Y6xl5pxtcDD5wF29fAUT+Dirq0Hv6x7a/gUg4OrJpsu72tbi98rnKGbVjO7j0PTeu5LeaMncP6XetZ8tYSRlaO5KQJJ2XkPPmAXXn2+Pw88c6WsAcdr1zfyGmzxnHUtBHBOutHx4UPQyxGEYkg5CtF27Nj9ZS4HarfhMUbTpvBrSs20tTew4iqUkZXBaxpqz7eAcCqj3fg9ZuMrSlnVHUZ1eUllLodLJg9nl8+viY4EXrB7PGUuAwahpaGDXEJvbt064qN/cQFkXeO5VkeQroYjJ6K0GFdI4aWctkxU8LKxWXHTAmez+1S/crNnKnDKXUbwTLjcjkYVV3GuGEVDB9Syq0R5fXWs2cyfEhpQsPI7D7/DafNYMmzgbk+8Ya1FeswtVj86cVNuB0Gh++VfEOhvneC95d7912ztTWteYuJ3wcPfwc+XgGHLYIxs9J6+B7TyxONr7Pv0AlURDG7acNJy6gZVH+yCsPbndbzWyilWLD3AqbWTOWnL/6U+z68T56/E0K52+Ck/cawYOnrHHXj8yxY+jozx9dxw1Nrg3XSoqMn247SkB5eQcgPirJnx06P+7cLD8E0NS6HgduluOXM/cPu4sa7u9vl8XP9v8PvYF//73Xccub+UBF+/sjjlbkd/ON7h+H1mbbHlmd5COkiGz0VJU6DX87bh3K3g06PnxJn3z2Wbo+ZcLkBcDoNpg4fwoMXHYrPb+J0GDRUluB0JnbfJvLzA1xy31u89fmuYBopW4mzZVcXD72xmblTGxiaxANFLU7edxQev8k3Z43htU9a+LipPQO5tEFrePwH8OFjcOAFsNfRaT/FvxpXs8vXwZG1X4qZrmX0/jR8+grVn75My16ZeSaOy+HiBzN/wG3v3MZvXvsNq7ev5icH/aTopAV2oyTu/e7B/aQF677YzdUn781VJ04P1jGGoaSHVxDylJxo7Cil7gROAhq11vtk+nyRPSW3vfgJT7y/nX9+bzb1Q3pVkjYNlOA2G9xOB03tPVx0zxvBdbG6ueMdL/LY8iwPIV0kE3sDZUeHJ0wIAIHYtcpasuUGAg2eUUlYvyIJ/fxNbT00tYdPDpeylTj3vvIpGjhl31Ep7T9jTDUzxgSGrY2sKmXjYDR2tIanr4K3/gozvgXT029C01pzz5bnGF06jOmVY2OmbaubiKd0KLUblmessQNQ4ijhkv0v4d+b/s0/N/yTl7a8xHf2+Q7n7n0upWl8plAuYzdKwuvXPPDG5qDJEQJ1wIMXHcq4YeFWvsGqNwVBSC+5MoxtKXDcYJ0s0Z6SZKQAmRweJJOkhXwlXllLR2wPRN4hZWtgvLi+mUkNldRVDvxH4MiqMjZkurGjNTz7C3j5Fph6Eux3VkZO8/KutXzUsYWvDNsv7Nk6tiiDHWMOoOrTV3G1N2ckPxaGMjhhwgn86rImeeUAACAASURBVPBfMb12Ore8fQvzHpnHs58+WxRD2+zqo9uf32g7NDbyGTqCIOQvOdGzo7V+QSm152CdL5GekmSlAJkcHiSTpIV8JV5ZG2hsD1TeIWUrdXZ3eflg626+vv/otBxvZHUpr27aQbfXT6krQz1rz18HLy2GycfBQRcGdIBpRmvNkk2PMsw1hMMinq0TjaY9ZzNiw/PUf/g4Ww88L+15iqShvIH/2v+/WNuylvs+vI9LV1zKYaMO45rDrmFExYiMnz9b2NVHu7o8lLrCh9qWu7P8zCdBENJKrvTsDCqJ3M1NRQqQyYnMMklayEcSKWsDie10yDukbKXGa5taMDVMH1WVluONqirD1PDpjs60HK8fL/wWVvwGJn4FDvkeqMx8/T3T/DYftH/KKcMPwWUkdj+xp2IYu4dPo+GDxzC8XfF3SBNTa6dy9aFXM3/qfN7Y/ganPnoqz3767KCdf7Cxq4+uOnE65/3ldRYsfZ0zbn+FBUtf55w7XxMBkCAUEDnRs5MISqkLgQsBxo0bN6BjJXI3V6QAwkBJZ8zmK5nuOZFymj6SjddVG5txOwwmNVSm5fzWPKyNTe1MGTEkLccEwDThmZ/Dqpthwlw47PsZa+i0+jr5zcYHGVM6LOFeHYsvJh/NtBdvpuH9R9m2/xkZyZ8dDsPBMXscw4y6Gdz27m1cuuJSzpl+DpfOvBSXkbx0YjBJNmajqaelDhGEwiZvena01rdrrWdprWfV19cP+Hjx7uZGPgkeZOKykBzpjtl8JZM9J1JO00ey8bpqww4mjxiCy5Ger5GRVYFJ8hsb0zhvx9sNjywMNHSmnAizL03rQ0ND0Vpz3Ya/s8PTyoIxx+BIskHVXjueXQ1TGfn233B27Yq/Q5oZXjGcnx78U44edzR3r7mb7zz1HRo7Gwc9H8mQSh0bWR9JHSIIhU/eNHYGG5m4LAi5j5TT7LCjvYd129vYe+TQtB2z1OWgvrKEj5s70nPAlk3w52Pg3Qdg/2/DwQsz1tABuHvLszza+ConNRzE+PLU5r18vvcpGN5Oxq36Y5pzlxhOw8lZ087iwhkXsmbHGk5/7HRe3/Z6VvIyWEgdIgiFT04MY1NK3Q/MAeqUUpuBq7XWf85mnmTisiDkPlJOs8PKjYEHLO89Kn2NHQhICgbcs6N1QCv99P8D0w9H/QzGHpyeDEbhrs3P8NuP/8GsqkmcMvyQlI/TPXQEX0w6mtHrnqZ11L40TzshjblMnENGHsLYIWP5w9t/4IKnL+AHB/yA8/Y+L75ZLg+ROkQQCp+caOxorednOw92DObzSARBSA0pp4PPM2u2U1XmYmJ9eubrWIysKuOF9U14/WZqw+O2vgVP/T/4dCUM3xtmXwZDMmcXa/a08r8bH+SppjeZVTWJC8YeizHABsHWycdQufMz9njh/9AONzsmfyVNuU2O0ZWjueqQq/jL+39h8RuLWbllJVfMuoJpw5Kbi5QPSB0iCIVNTjR2BEEQhPxgd5eXZ9du58A9atN+93ufUUN56oNtrNq4gyMnJzjPracdNvwn0Juz4RkoGQKHfh8mHZMREYHH9PJ+26c82biaR7a/jF/7+caIwzihfhZGOs5nONgw6xwmvXYnE577X4ZuXs3WA86mp3rMwI+dJGXOMi7e92JWfL6Cf274J6c/fjpfHv1lTtnrFGaPms0QdxpFEoIgCBlCGjuCIAhCQjz9wTb+uGIjnT1+jt0n/T0mM8ZUU1Pu4upl73Pj6fsxc4+avo1v3QudzQHpgKcddm+Glo9h23ug/VBeF5ibM/VkcJennIfd3k6eaHwNj+nFo330mD46/d1s69nJF90trO/Yikf7cCknB1ZP4qSGgxhRUhP/wElgukr56JALGbXuaUasX07dR8/QVT2WzvrJeMqH4SutQjtcNE4/Ee3MbI+EUoq54+Zy8MiDeeqTp1i5dSUvbnkRhWJ81XjGDR3H8PLhVJdUU+Iowe1wU+4q55uTv5nRfAmCICSKysenJiulmoBPE0xeB2T2sdTZpxg+IwzO52zWWh+X7oMmGbOpkutxIPkbGHb5y+d4jUY+XodcIZfzBoH8rc1CzOb6/yUWkvfsYeU/I/WsMHjkZWMnGZRSq7XWs7Kdj0xSDJ8Riudzpkqu/38kfwMj1/OXLnL9c+Zy/nI5b5C9/OX6/yUWkvfske/5F/oQ9bQgCIIgCIIgCAWJNHYEQRAEQRAEQShIiqGxc3u2MzAIFMNnhOL5nKmS6/8fyd/AyPX8pYtc/5y5nL9czhtkL3+5/n+JheQ9e+R7/oVeCn7OjiAIgiAIgiAIxUkx9OwIgiAIgiAIglCESGNHEARBEARBEISCRBo7giAIgiAIgiAUJNLYEQRBEARBEAShIMnLxs5xxx2nAXnJKxOvjCAxK68MvTKCxKu8MvjKCBKz8srgS8hz8rKx09zcnO0sCEJSSMwK+YTEq5BvSMwKghCNvGzsCIIgCIIgCIIgxEMaO4IgCIIgCIIgFCQZbewopcYqpZYrpdYopT5QSv3AJs0cpdRupdTbva+fZzJPgiAIgiAIgiAUB84MH98HXKG1flMpNQR4Qyn1H631moh0L2qtT8pwXooSU5u0dLfg8XtwO9zUltZiKPs2bjJp07mvIITiM300dzXj9XtxOVzUldXhNKJXVRJ7QrEQLdat9aZpYmJialPKgiAIQi8Zbexorb8Avuh936aU+hAYDUQ2doQMYGqT9TvXs+i5RWzt2MqoilEsOWoJk2om9fsCTCbtQM4jCLHwmT4+2vkRly2/LBhLN829ick1k20bPBJ7QrEQLdYnVk9k466N/P6t33Pm9DO5euXVUhYEQRBCGLQaUCm1J7A/8KrN5kOVUu8opf6llNp7sPJU6LR0twS/GAG2dmxl0XOLaOluGVDadO4rCKE0dzUHGzoQiKXLll9Gc5e9aUliTygWosV6c1czi55bxLxJ84INndDtUhbio7Xmzpc28cXurmxnRRCEDDAojR2lVCXwMHCp1ro1YvObwB5a632Bm4FHohzjQqXUaqXU6qampsxmuEDw+D3BLz6LrR1b8fg9A0qbzn0LGYnZ5PH6vbax5DW9tukl9tKHxGtuEy3WvWagzFS5q4quLKQrZjc0tvOLx9fww7+/k8bcCYKQK2S8saOUchFo6Nyrtf5H5HatdavWur33/ZOASylVZ5Pudq31LK31rPr6+kxnuyBwO9yMqhgVtm5UxSjcDveA0qZz30JGYjZ5XA6XbSy5DJdteom99CHxmttEi3WXESgzuz27i64spCtmNza1A7Cr0/6miiAI+U2mbWwK+DPwodZ6cZQ0I3rToZQ6qDdPOzKZr2KhtrSWJUctCX4BWmO4a0trB5Q2nfsKQih1ZXXcNPemsFi6ae5N1JX1u/8BSOwJxUO0WK8rq2PJUUtYtn4Z18y+RspCCrR2+wDo8ZlZzokgCJlAaa0zd3ClDgdeBN4DrFrkp8A4AK31rUqpS4CLCZjbuoDLtdarYh131qxZevXq1RnLdy6SqnEqGbNVMK3pxWXEt2ClI385iMrEQQs5ZpO99vHSR8bhsNJh7Pbsjpq+gGIvFSRe85hEY9cqE9b3tUbns40t52L2rlWfcPWjHzCxvoJnr5iT3owJhUBGYlYYPDJtY3uJOEGitb4FuCWT+ch3UjVOmdpk466NCdvYEk1rh6GMqHffhcIl2dhMJL3TcDKiYkTC6SX2hHwk0bITy1AoZSA9dHr8AJiZu/crCEIWyblbPkJ/UjVODZaNTSheko2bTKcXhHwh0dhO1lAoJE+XJzCMzeeXYWyCUIhIYycPSNU4NVg2NqF4STZuMp1eEPKFRGM7WUOhkDxWz471VxCEwkIaO3lAqsapwbKxCcVLsnGT6fSCkC8kGtvJGgqF5On0SmNHEAoZaezkAakapwbLxiYUL8nGTabTC0K+kGhsJ2soFJKnq7eR0+OTxo4gFCIZtbFlimI0BSVjnApLa7gxDINuX3f4+1gWrF5zm9tw0+3vDt5B7PZ1xzW6JUsytrhBIudMQdkmXuwlbFfrvca1JbW09LQEl2tKamjpbsFn+nAaTurK6nA5XFH3z4EYySUkXvMYO8uaoQxcyoVXe4NWtSp3FTu6d+A1vTiVE7fhRitNdUk1u3p2Bcte5HKyNrZBMhvmXMxedM9qnvpgOwAbfnU8TofcBxbCEBtbniO/GPKERG070Qw/E6snxrSt2dnYrp19Lb9783c0dzWHvbdMQAP9wRnLMiQ/ZnODgdrQol3jW9++leWblzN3zFwW7rew3/ZJ1ZNwOVwDtgQKQq5ixfbv3/o9Z04/k6tXXs3Wjq3MHTOXi/a9iMtXXB6zDo9WdqyylWxZSdX6WQiEDl/r8ZnS2BGEAkNKdIERzfDT3NUc0/xjt99VK6/i/C+d3+99ukxAYhnKfQZqQ4t2jedNmgfAvEnzYsaA2NiEQsWK7XmT5gUbOhAoE1ZDB6LX4dHKjlW2ki0rxVzWQhs7HnmwqCAUHNLYKTCiGX68pr3RxzL/RNuvyl1l+z4dJiCxDOU+A7WhRbvGVixVuatst/tMX1rOLwi5ihXbkWUgWpmIrMOjpbPKlrWcaFkp5rLW2aueBvCIfloQCg5p7BQY0Qw/LsPe6GOZf6Ltt9uz2/Z9OkxAYhnKfQZqQ4t2ja1Y2u3ZbbvdGsYoNjahULFiO7IMRCsTkXV4tHRW2bKWEy0rxVzWukKHsXmlsSMIhYY0dgqMaIafurK6mOYfu/2unX0td753Z7/36TIBiWUo9xmoDS3aNV62fhkAy9YvixkDYmMTChUrtpetX8Y1s68Jxviy9ctYPGdx3Do8WtmxylayZaWYy5rHZ+JyBOage/xiZBOEQkNsbAVINKNOmNXKcFHqLKXT1xlMA9ha3II2tl4zWzptWF6/l+au5qgmriyQc6agbDNQG1vkNR5WOoxWb2sw/VDXUHZ074gaAwM1RA2SYSpbSLzmMVZsmqaJiYmpTVsbW6hlzVAGBgYmZsx0icR6ZNkYqM0tQXIuZmdd+wwen5/Wbh9PLDqcvUdVxd9JKCbExpbniPKqALGzY8WzrVnWncHsVTG1yce7Py5K+08+Ecu2Fs/glOg1Hlk5MqXzx6OYDVNC7pNIbNvFcCzrWqJlRcpGH16/SanLQWu3TwQFglCAFFeNVsTEs61lw7pTzPafQiHeNcz2Nc72+QVhoNjF8ECsa7GOW6xlw+s3KXM5gIB6WhCEwkIaO0VCIra1wbbuFLP9p1CIdw2zfY2zfX5BGCjx6m5rOdmYlrLRh9WzA6KeFoRCRBo7RUIitrXBtu4Us/2nUIh3DbN9jbN9fkEYKPHqbms52ZiWshFAa43XrylzS2NHEAoVaewUCfFsa9mw7hSz/adQiHcNs32Ns31+QRgodjE8EOtarOMWY9nwmwFJU6kr8HNIhrEJQuEhNrYiIsy8E2Jby6ahKgdNWTlnCsp1Bmpry3b+8hyJ1yIgU9a0LJWNnIrZLo+faT//N3OnNLB8XSM3nbEvX99/TAZyKOQxYmPLc8TGVqwoqC6pxiiN8qM0pDFU6izFNE08ZvgXYqpflAX+41OIwG/68fq9gb948fl9YerpTDeOBmJzE4RsEFoGLBW1T/sA8Jk+WrpaMAyDhvIGdvXsYlvHtgHVwSMqRhRtHezxB3pyynp7dmQYmyAUHtLYKRISUQRHbr929rU8tvExTp54MletvCpsv4nVE/uprBPRlorutLCIdz29fi/rd63nsuWXsbVjK3PHzGXhfguDy4nEocSHUEzYlYHFcxZz2zu3BVXT18y+hvvW3MfC/RZGVVAnc/xiLmPe3sZOqczZEYSCpfhqtiIlFUXwVSuv4tx9zg02dEL3a+5qTklbKrrTwiLe9Wzuag42bADmTZoXtpxrqmpByDZ2ZeDyFZeHqaavXnl1sCwlq6CWMhaON9izI+ppQShUpLFTJKSqCHYoh+16r+lNSVsqutPCIt719Jm+sO1V7qqcVlULQrZJVDVtlaVkFdRSxsLx+gLzlqWxIwiFizR2ioRUFcF+7bdd7zJcKWlLRXdaWMS7nk7DGbZ9t2d3TquqBSHbJKqatspSsgpqKWPhWHN25Dk7glC4ZLSxo5Qaq5RarpRao5T6QCn1A5s0Sim1RCm1QSn1rlLqgEzmqVhJRRF87exruev9u7h29rX99qsrq0tJWyq608Ii3vWsK6vjprk3BbcvW78sbDnXVNWCkG3sysDiOYvDVNPXzL4mWJaSVVBLGQvHGsbmchg4DRVcFgShcMioelopNRIYqbV+Uyk1BHgD+JrWek1ImhOA7wMnAAcD/6e1PjjWcQtZi5pJPXTosUsdpXhMD16/F5fDRV1ZHYYyBt3Glu5jp4Gc0qJmg2T/916/l+auZnymD6fhpK6sDpfDFdzu8XnY0b0juL22tJY2b1tUjW66tLqpfp48o+jjNZ+ItKwZGJiYuJQLr/ZiarNfGbBsbNZ2az/DMBIuK9HU1aZpYmIGz1uM6un3Nu/m5Fte4sqvTuHm5ev59iF78P9OnJ6BHAp5jKin85yM2ti01l8AX/S+b1NKfQiMBtaEJJsH3K0Dra5XlFLVSqmRvfsWFdGMaL9783c0dzUP2JhjKXh9po+Pdn4UZsS6ae5NTK6ZnJSiN1Wlr7VfNCtQqqY3YeAka2rymb4w21poLDkNJ6Y22dS6KSkLYDqvtZinhFzBLhavmX0NKz9fyXETjuPyFZenFKPx6mCpZ2NjDWNzGAqnYeD159+zBwVBiM2g1WhKqT2B/YFXIzaNBj4PWd7cu67oiGZEO/9L56fVmBNpyNrasZXLll9Gc1fzgI+dDNGsQKma3oSBk6ypKV4spWIBTOe1FvOUkCvYxeLVK6/ma5O/FmzoWOsHowxIPRvAGrbmdCichgo2fgRBKBwGZ1yQUpXAw8ClWuvWFI9xoVJqtVJqdVNTU3ozmCPEs/Cky5jj9dub1Lymd8DHToZonzda/vLNFpSPMZusqSleLKVqAUzXtRbzVOLkY7zmE8kaLzNdBlI1auYS6YjZYGPHMHA6lAgKBKEAyfzgXKVcBBo692qt/2GTZAswNmR5TO+6MLTWt2utZ2mtZ9XX12cms1kmnoUnXcYcl8PepOYyXFH2yAzRPm+0/OWbLSgfYzZZU1O8WErVApiuay3mqcTJx3jNJ5I1Xma6DKRq1Mwl0hGz4T07hggKBKEAybSNTQF/Bj7UWi+OkuxR4JxeK9shwO5inK8D0Y1od753Z1qNOZGGLGueRSrzbwZCNCtQqqY3YeAka2qKF0upWADTea3FPCXkCnaxeM3sa3jko0dYPGfxoJcBqWcDeHqfs+M0FE6H2NgEoRDJtI3tcOBF4D3AqkF+CowD0Frf2tsgugU4DugEFmitYypVCtkUFM3GZmdPcxrx/RJxj2d6cRmJH29An8fG9hNtu9jYsofP9NHc1Rw11iLtazWlNezs3hnVxpZqDKQLsbElTz7Faz5hxaJpmvi1H7/241AOHIYDv+lHo8Nsa8nGarL1aZbKRk7F7GPvbOX797/Fb0/bl1uWr2evhiH86dxZGcihkMeIjS3PybSN7SXiBEmvhe2/MpmPfMLOcBbLnhargZJpu1s8EjFhRTO6pWp6EwaGqc2Yhiav32trX7v17VtZvnl5UtfYItPXWmJJyBUMFWjA2FnZ7ltzHwv3WxizLMUiXn0r9aw9YcPYHDKMTRAKkYK5vVnIpGpPGyy7WzLnL0bbTz4R75pFi8V5k+bZphcEIZxoVrZ5k+YNqCxJfZsafYICJQ8VFYQCRRo7eUCq9rTBsrsle/58sv0UG/Gumc/0xYypyPSCIIQTq14eSFmS+jY1rOfqOAyFwxAbmyAUItLYyQNStacNlt0t2fPnk+2n2Ih3zZyGM2ZMRaYXBCGcWPXyQMqS1Lep0TeMzZDn7AhCgSKNnTwgVXvaYNndkjl/Mdp+8ol41yxaLC5bv8w2vSAI4USzsi1bv2xAZUnq29QIHcbmkjk7glCQZNTGlimK0RRkGbI0GjRo04/bcGI43HT7u8NMa5HWtR5/T9CU5TScdPu6cTlcuI3efXstPEDKZp6o1jdnKaZp4jHzxoSVU6agbBC0sUUx9Xl8HnZ07wizse3q2RVMX1tSS0tPS9DmVl1SHWZrqy2tpc3bFoyzalcVu7qb8Zg+3IaT2rJ6DEdG3SmFRNHHa77g83lp7m7Ga/oocZQETWx+7cenfTiVk2Glw2j1tgbLRpW7ih3dO/D6vTgNJ27DjVaa6pJqdvXs6itDvcumaWJiYmozbH2OmQhzKmZ/v3wDNzy1jnvOP4hblm+gub2HZ6+Yk/4MCvmM2NjyHPlFkSc4DScNpXWs3/kRi1b0mbBC7Wqx3j+28TFOnngyV628KqqZze1ws/A/C6Oa06KRbeubkD7i2dhMbbKpdVNw+9wxc1m438KodrYbDr+BcdXj+m3/18Z/sfTDpfY2tzk3MalmsjR4hILB5/OyfvcGLl1+abAcLD5yMYZhhK0LLWt2Fs7Fcxbz9va32X/E/sH1dmVwyVFLmFg9MWZZFgJYc3QchmVjy78bwIIgxEZqvDyipasp2NCB/na1WO/P3efcYEPHbt9Fzy1ic9vmlEw+2ba+CekjntEpcrtlkIpmZ5sxfIbt9q9N/ppt+q0dW1m04jJaupoG70MLQoZp7m4ONmogEOctPS391sUzH16+4nKOHHdk2Hq7MrjouUU0dzWLnS0BvH4Tp6FQKmBjE0GBIBQe0tjJIzxxTFix3juUI+6+Zc6yftsTMflk2/ompI94RqfI7ZZBKjK9de39pt92u0M5bNMHz2f60vOBBCEH8NrU3WXOsphlLZqF09RmQmXQa9rvL3VxOF6/idMRGKUk6mlBKEyksZNHuOOYsGK992t/3H27fF39tidi8sm29U1IH/GMTpHbLYNUZHrr2jsMh+12v/bbpg+eL8bDcgUh33DZ1N1dvq6YZS2ahdNQRkJl0GXY7y91cThev8ZpBH4KyUNFBaEwkcZOHlFbVs+SOeEmrFC7Wqz3d71/F9fOvjammW3MkDEpmXyybX0T0kc8o1PkdssgFc3O9u72d223P/LRI7bprTk7tWX1g/ehBSHD1JXW8bu5vwsrB7Ultf3WxTMfLp6zmOc/ez5svV0ZXHLUEurK6sTOlgDWMDYAl0PJnB1BKEDExjZAwixkg2C7Mf0+Wrqaes1Vrrg2NutZPN3+gJkNwGt6w9NbNjbTDDl2clasaDa2HDIAJUpOmYKyQTwbW2TMRxqfggap3v3729hqaPO2i40tPRR9vOYa0b4TgjY27cOhHLiUk2oc7NYePGjbujK0LDpVfBtb5DkH+/spQXIqZn/00Ds8t7aRm+cfwAOvf8bj737Bhl+fkIEcCnmM2NjyHPlFMQDsLGSZtt0YDid1lSPTf2DThKa11P1tPuz6DKrHwbfuh4bpYMT/LIYy4j73R8h94tnYwP5aRy6PqBjRe0ATGtcwMiKuSiLiKiMxLQiDTKzvBKfTxYjKkcEyQW+ZqAutayO+N5yGs68sRRCvDILUy4kQOozNYRj4TI1pagxDft8KQqGQ8C9ypdQ3lFLrlVK7lVKtSqk2pVRrJjOX68QzV+UVnU3BL18g8Pdv8wPrhaIh7TEtcSUUEQmVHykTOYXHb+LobdhYogKPzNsRhIIimZ6d64GTtdYfZioz+UY8c1Ve4fP0ffla7PossF4oGtIe0xJXQhGRUPmRMpFTeH3hNjYIzOMpdTli7SYIQh6RzFir7dLQCSeeuSqvcLoDQ4xCqR4XWC8UDWmPaYkroYhIqPxImcgpQgUF1nA2kRQIQmERt7HTO3ztG8BqpdQDSqn51rre9UVLPHNVXlFeHxg3bn0JW+PIy8WKVUykPaYlroQiIqHyI2Uip/CZOjiMzeXo69kRBKFwSGQY28kh7zuBr4Ysa+Afac1RHmEog0k1k7j3xHtzzXaTPIYRmCD73WcCwymc7sCXbwJyAqFwSHtMS1wJRURC5UfKRE7h8Zkhz9lRwXWCIBQOcRs7WusFAEqp2VrrlaHblFKzM5WxfCGrthu/D9q3gd8LDhdUjoBIZa9pBia+JvKlahhQOTylrOSo4lRIgbgxnUxMAaaCFocDDw7cDge1Kk6XcuTxy4ZB1w75YSjkBYaGOr8ffH7AH4jnLpvyUjm8L9ZbtyQV21Lfpg9vqKAgOIxNGjuCUEgkIyi4GTgggXXCYOD3wfb34cFv9yl9T78Hhu/T1+CJUJwmq5NOlGwouIUskWRMJR0bkcefciIc+aPwOM9ADAtCWrArH6ffA89fD+ueCI9fSKl+lvo2vXj8Ji6rZ8cQG5sgFCKJzNk5VCl1BVCvlLo85PU/gOhKskX7tr4fgBD4++C3A+stBklxWlAKbiE2ScZU0rERefz95vePc9H0CrmKXfl48NuBOLaWrfhNsX6W+ja9+Pw6OHzNYc3Z8YmgQBAKiUR6dtxAZW/aISHrW4HTMpEpIQH8Xnt9qd/btzxIitOCUnALsUkyppKOjcjjl9WIplfIH6KVj7Ka8GUrflOIbalv00voc3asHh7p2RGEwiKROTvPA88rpZZqrT8dhDwJieBwBYY9hH5ZVo8LrLewFKeRadKsOLV0q6FfwHmr4BZik2RMJR0bkcfv2jkoMSwIaSFa+ejaGb5sxW8KsS31bXrx2ggKZM6OIBQWiQxje0wp9Shws1Lq0chXnH3vVEo1KqXej7J9jlJqt1Lq7d7Xz1P8HMVH5YjAWPBQfenp9wTWWwyS4rSgFNxCbJKMqaRjI/L4b9/fP85F0yvkKnbl4/R7AnFsLVvxm2L9LPVtevH6tc1zdqSxIwiFRCLD2H7b+/cbwAjgr73L84HtcfZdCtwC3B0jzYta65MSyEdhEc1oFc90FbLdrB1Py3eewqN9uJWT2rI6DIczPM3QUeFpnOUYrVvAVQam8Co3ZwAAIABJREFUH/wh54GU8lRQCu4CxPT7aOlqwmP6cBtOasvqA3ESdYcY19tGm2uW1tLSub3v+KV1GN0t4PNgON1MGron9x77l5A4rcVo3dpnEawYDr3pcbqhfmq4lrdsmGh6hdwjWjmp2wvOexJMHxhOKK2G46+DY38FDjcYTsy2rbQYBuaQBszvPo0ZWjbjxLbUt+nFZ5rBHh3p2RGEwiTRYWwopW7UWs8K2fSYUmp1nH1fUErtOaAcFiLRjFb1U6FpbXQ7T8h+5vgjWD/7YhatuKLPyDPnJiZVT8JoXhcjzY1M+uBJjHGHwrLv9Z3n7H+Crzv5PPWSVQW3EBXT72P9zo9YtOKy8DipmWzf4EnEthaiKLc//mImPfNrjLWPw5QTMY78EXWWZMDOrnb6PfDew/DykuhGqhSV6IKQEaKVk7q9oHFtX3zbxLv5zbtYr3z8/sO/cub0M7l65dVJW9Wkvk0fXr/GEWljk+fsCEJBkcytoAql1ARrQSk1HqhIQx4OVUq9o5T6l1Jq7zQcL/eJZuFp3xbbzhOyX8uXLw02YqDXyLPiMlq64qW5gpb95/c1dKzz7Pw4tTwJOU1LV1OwIQIRcWJHsrY12+NfTssBZwUSRNrU7OxqD34b9j8rofMJQk4QtQ5vihvvLd07WbTqZ8ybNC/Y0AGxqmULj9/sN4zN4xcbmyAUEsk8Z+cyYIVS6mNAAXsAFw3w/G8Ce2it25VSJwCPAJPsEiqlLgQuBBg3btwAT5tlohl7ohnWLDtPyH4ew2lv5NG++GnQ/c/jKk8tT0JUciFmPabPPgZMn/0OydrWoh2/vHf+QKRNLZpdzXCEL0t8DTq5EK95Q7RyYvrixrunpIKtHVupcleJVW2ApCNmvX6bYWzSsyMIBUXCPTta638TaIj8AFgETNFaPzWQk2utW7XW7b3vnwRcSinbvnmt9e1a61la61n19Xk+Odky9oRimdTs1lt2npD93KYvOEHVYlTFKNzKGT8Nqv95vJ2p5UmISi7ErNtw2seAEeU+R7TYjGZbi3b8zt6705ZNzSJy2Tq+6U/ofELmyIV4zRuilRPDGTfe3T0djKoYxW7PbvuyI1a1hBlozGqtA8/ZCfbsyJwdQShEErGxHdX79xvAicDE3teJvetSRik1Qimlet8f1JufHQM5Zl4QzcJTOSK2nSdkv9oXf8eSOTeGG3nm3ERtWbw0N1L71v0w7w/h56mZkFqehJymtqyeJXNuso8TO5K1rdkefzG1b94bSBBpU7Ozq51+D7x1b0LnE4ScIGodXh833mtLa1hy2C9Ztn4Z18y+RqxqWcRnBoarBefsOMTGJgiFiNI69thUpdQ1WuurlVJ/sdmstdbnx9j3fmAOUEfA3HY14Ord8Val1CXAxYAP6AIu11qvipfpWbNm6dWrY7oRcodkrWvJ2NjcFbSYPXhMH6WOUkztw2N6A8YrRymGpyMsjdtwUmuUYHg6wm1s1nttBoZhaD8YLnCWgLer1yDk6P++MM1YKhMHzWbMDtTGZpYNo8Wzq8/85K7G6NoRYmOroaWrOcS2NgyjoynEtlYXmMtg2akq6yOWGyA0feUIiJU/IZSCi9ecI9G6urQWOrYH6k6/B5QCrQP1qjL6lpXCVA5aDDCVAxMTU5sxrWqmNmnpbikU+1rOxGynx8f0nz/FmQeN4+R9R9HR4+O7d6/mqhOn8d0vT4h/AKFYyEjMCoNHIja2q3v/Lkj24Frr+XG230JATV2YxDNb2Rmmoq232W4QaEWaPi/rd33EohWXhxuxqidjOF30GxdYHnLn0C6Ppy0NfFn/88KUzGxCbmE4nNRVjkxihxDbmjZZv3M9i55bFG5ze+ZX9rY1q6fm+eth3RNw6CL40qmx7Wuh6SWmhFwi0Trc74Pt78e0sPHNu+GFG2DdExjV46hLMM5ty2CC1jYhNl5f4GZvf/W0CAoEoZBIuKZUSm1USt2rlFpYNNa0gZKk2SpVAkasy/sbsaIZt+LlsWtHX0MnNN9iZis6Wrpbgj+yIMTmFs22ZtnV9uu9z7H/WfHta6HpJaaEXCLROrx9W3zr4N/PSSnObcugWNvSgtcMDFdzyENFBaGgSea20HTgNmAYcENv4+efmclWgZCk2SpVPDqKEUtHMW6FYpdHMbMJvXj8nuRsaxBYLqsJvDccidnXrPTWssSUkAskWodH1o3xykW049gQtQyKtW3AWI0aq5HjMBSGksaOIBQayTR2/IC3968JNPa+hGgkabZKFbeKYsRSCcx7sMujmNmEXtwOd3K2NQgsd+0MvDf9idnXrPTWssSUkAskWodH1o3xykW049gQtQyKtW3ABIexGX1TMpyGgUcaO4JQUCTT2GkFfgdsAs7VWh+qtR7oc3YKmyTNVqkSMGIt7m/EimbcipfHsmHw9dvFzCZQW1rLkqOW9Le5RbOtWXNw3r4/sPzWvfHta6HpJaaEXCLROrxyRHzr4DfvTinObcugWNvSgtWosebqWO+tRpAgCIVBXBtbMKFS84DDgYMAD7AKeEFr/WzmsmdPXpmC4tnVIDC5tX1bYCiEqxxMb+C9uwJ8PSHWqhHgdNmfxufpM2IZLmqVE8PbnZhJzTq/6QPlCJzL4eqztSVji8t/csYUlDFC483OfhbPxuaqwujYnoRtLXJ5OHTv7IuhsmGBeWKFG1OZpPDjdbCIZ11DBerjoGXNH6gvHa5AWYi0rpUMhZ7WkHTuQH2KDpguk4hzsbHFJ5WYXbutleN+9yKXfmUSB48fBsBF96zmlP1Gce3XvpSJbAr5idjY8pyE/a5a62XAMqXUVOB44FLgR0BZhvJWGMSzq4VafCob4Oj/gWXfg/FHwIHfhQfPCbdYNezdv8FjmhjNH1H3t/nhx7D2m/cHePZ/oL0x/H0qhrV4n0fIbSKtUVZcDd8n0OCxsU8Z37qfOiseIrfbWaci7WqR9jW7+JKYErJJPOtaaS00fhCI64MvgkcvsbWsUT0OTrkFXr0NjrgyfP0ALIOGMqgrs33etjAA+oax9V0Tp8OQnh1BKDCSsbE9rJTaAPwfUA6cA9TE3kuIS6jFZ/alfY2UQ7/f19CBPmtV+7b+xwg1BoUew9pv2fcC6yPfi2Gt+Ii0RkXGVTz7VOT2eDY2O/uaxJeQa8SLe6vc7De/r6FjpYu0rD16SWA5RfuaMHgEh7GFzdlRIigQhAIjmSf3/QZ4S2vtt9uolDpGa/2f9GSriAi1+IQafKJZrEwbw1qoMSieBSjyvRjWioto19vvDbyPZ5+K3J6odSrSvibxJeQS8eLe9PXFdSLxbqUTy2BO47WZs+NyiKBAEAqNhHt2tNarozV0erkuDfkpPkItPqEGn2gWK8OmfRpqDIpnAYp8L4a14iLa9Xb0Do2MZ5+K3J6odSrSvibxJeQS8eLecPbFdSLxbqUTy2BOYzV2HEaEoEAaO4JQUKRzhqNM4EqFUIvPyt8F5tRUj4OXb4bT7+5vsaoc0f8Yocag0GNY+837Q2B95HsxrBUfkdaoyLiKZ5+K3B7PxmZnX5P4EnKNeHFvlZu37w/MyYllWTvllsByivY1YfDw+W3m7BgKr1/m7AhCIZGwjS3ugZR6U2t9QFoOFoeCMwX5vH02NGdpwOrj9wRsPt7OPouVuwK6d9sbtMKMbr3OCL83MRtb4RvWkiFnTEEZI0kbG6W1EGZfa4COxuRsbB3N0c8nDITCj9fBImkbW699LZqNzVUOvq6AiU37A+ukfoUcitmnPtjGRfe8wW++8SX2HFYBwP88+gH1Q0r463cPzkQ2hfxEbubnOfKLI9uYJjSv628BsrOkhZrUIg1ayRjVIhHDWnHhcELVmOjbQ+Mhmr0t1K4WaV8Ljb14litByBXs6kErfpf/OtzCloiFMBXbpTCoBIexqb7fsg5DyZwdQSgw0lnbfpLGYxUP0SxAdpa0UJNaMgYtQUiVaPa2ULtapH0tlr1NYlPIJ6z4jbSwJWIhFNtlzmMnKJA5O4JQeMTt2VFKfSPWdq31P3r/xkwnRCGaBSiaNSvSpBbrGGL+EQZKtDiMtKtFs05JbAr5jBW/kRa2RK1sYrvMabxR5uxIz44gFBaJDGM7OcY2DfwjTXkpTiwLUOgXYqglLXJ9pEkt1jHE/CMMlGhxGGlXi2adktgU8hkrfi27mhXHkctgXw6ilR+J/5zAtmfHMOjoiSWeFQQh34g7jE1rvSDG6/zByGRBE80CZGdJCzWpJWPQEoRUiWZvC7WrRdrXYtnbJDaFfMKK30gLWyIWQrFd5jxeX3/1tMMhPTuCUGgkZWNTSp0I7A2UWuu01r/IQL5iUnCmoHgWIJ+nrxfH25WYQUuMP6mSM6agnCHS3lYxHLpb+mKtbBh07YgeexKbmUTiNdNY8Wua4Va1yLiPVg4k/iPJmZi944WP+dWTH/Lnc2dR7g58n/5hxQY+bupg5U+OykQ2hfxEbGx5TsI2NqXUrUA5MBf4E3Aa8FqG8pXfDOTLTQNdLf310PEQo5qQKvFU05Uj+tvbImMtVuxJbArZYKCNjND9lQoopB2u8OMkUg4k/nMWqwfH5Qids2OIoEAQCoxk1NOHaa1nKKXe1Vpfo5S6EfhXpjKWtySr2rVLH6qYFk2pkEki4y+aUtfSnAtCPjBQ5bnd/qfcAq/eBnN/KnVygdDj9aMISAksXDKMTRAKjmRq667ev51KqVGAFxiZ/izlOcmqdu3ShyqmRVMqZJLI+Ium1LU054KQDwxUeW63/6OXBMqH1MkFQ4/PxO00UCpUUCDqaUEoNJK5Vfu4UqoauAF4k8CAqz9lJFf5TLKq3WjpQxXToikVMkVk/EVT6lqac0HIBwaqPI9VL0udXDD0+MywIWwATocRVFILglAYJNOzc73WepfW+mFgD2AqcG1mspXHWKrSUGKpRqOlD1VMi6ZUyBSR8WcpdUMJ1ZwLQj6QbD2c6P5W+ZA6uSDo8flxOcLnnjsNhcdnkoy8SRCE3CaZxs7L1hutdY/WenfoOjuUUncqpRqVUu9H2a6UUkuUUhuUUu8qpQ5IIj+5SbKqXbv0oYpp0ZQKmSQy/qIpdS3NuSDkAwNVntvtf8otgfIhdXLB0O0NDGMLxdJQ+0xp7AhCoRB3GJtSagQwGihTSu1Pn4JvKAE7WyyWArcAd0fZfjwwqfd1MPDH3r/5Q6ixx10B3k4oq4bzngQ0OEtiW4AMA+qnwoJ/9dmvXOVw2lJwlQUe3ti6BRzuwFPrQy1tIEpTIb51KnJ7pCK3bnIgXk0fGE6obAiPx4rhiSl2BSFXMIyAROC7z/SP034q9QboaOyvVi+tCpQD5Qgop5UDTrg+UBe3be1TUEv85y2Bnp3+w9gg8MDRyG2CIOQniczZORY4DxgDLA5Z3wr8NNaOWusXlFJ7xkgyD7hbB/qLX1FKVSulRmqtv0ggX9kn1Ngz/gg48Lvw4Dn9LVaxvghNE5rW9rcG1U/tvz7S0uYshb9+PTXbkFAYxLNOJWpbe/56WPdEf/ua3fEj00vcCbmInfLZ74Pt70ePf7vyEVofL/81HHxRQFYg9W7e0+Pt36CxhrV5fRpktKIgFARxa2et9V1a67nAeVrruSGveVrrfwzw/KOBz0OWN/euyw9CjT2Hfr+voQOJW6yiWYPat8W3tO38OHXbkFAYxLNOJWpb229++LIVt3bHj0wvcSfkC+3bYse/XfkIrY/3m9/X0AndLvGfl/T4TNyRPTu9w9h6fP5sZEkQhAyQzK2olUqpPyul/gWglJqulPpOhvLVD6XUhUqp1Uqp1U1NOfLFEmrsMRypWayiWX/83viWNld5/+1iCcoZBiVm41mnErWtWXFlLVtxG88WGHk+IW/JyTo23cSrV2PZCENtbJHbJf6zwkBjtsfnxxkhKCh1OQDo9EhjRxAKhWQaO38BngJG9S5/BFw6wPNvAcaGLI/pXdcPrfXtWutZWutZ9fU5Mjk01Nhj+lOzWEWz/jhc8S1t3s7+28USlDMMSszGs04laluz4spatuI2ni0w8nxC3pKTdWy6iVevxrIRhtrYIrdL/GeFgcas3TC2UmegsdPh8aUlj4IgZJ9kGjt1WusHARNAa+0DBnrr41HgnF4r2yHA7ryZrwPhxp6Xb4bT707eYhXNGlQ5Ir6lrWZC6rYhoTCIZ51K1Lb29v3hy1bc2h0/Mr3EnZAvVI6IHf925SO0Pn77/oCVTerdgqDL6+83jK3EFViWnh1BKBxUoi55pdQK4FTgP1rrA3obJ9dprY+Msc/9wBygDtgOXA24ALTWt6rAY4tvAY4DOoEFWuvV8fIya9YsvXp13GSDg52NzbL6VI4ITPJO5hihdp/Q9WJjGyxU/CTJk9GYHaiNrbQWOrZHj9t4+0vcZZP8i9dsk6iNLVp9bJoBO5vY2FIlZ2L2iOuXM7amjEuOmhRc99H2Nq5+9AOWLjiQOVMa0p1NIT/JSMwKg0ciNjaLywn0xExQSq0E6oHTYu2gtZ4fZ7sG/iuJPOQe/Yw/tWk4Rpz1ocTbLhQ+8eLEbnvkctWYge0vCPmCw9k/3iOXU62PhbzCTj0tc3YEofBIprGzBvgngR6YNuARAvN2BEEQBEEQ8ooen4nLGTlnJ7Dc0SNzdgShUEim7/1uYCrwa+BmYDJwTyYyJQiCIAiCkEns1NPSsyMIhUcyPTv7aK2nhywvV0qtSXeGBEEQBEEQMonWmh5v9GFsYmMThMIhmZ6dN3ulBAAopQ4GCnQGqyAIgiAIhUq318TUUOYK/xnkcigMBZ090rMjCIVCMj07M4FVSinriWrjgHVKqfcIuAZmpD13giAIgiAIaaatJ/Dg5DK3I2y9UopSl0OGsQlCAZFMY+e4jOVCEARBEARhkOjo7bmxhq2FUupy0N7bGBIEIf9JuLGjtf40kxkRBEEQBEEYDNq7A3NyInt2AIaUOmnp8Ax2lgRByBDyJDRBEARBEIoKaxhbuU3PTnWZi8bWnsHOkiAIGUIaO4IgCIIgFBV9PTv9B7hUl7tpbJPGjiAUCtLYEQRBEAShqLDU0qWu/j+DqstdNLf3YJp6sLMlCEIGkMaOIAiCIAhFRbBnJ8owNp+p2dkp83YEoRBIxsYmCIIgCIKQ97T1BBo75TbD2OqGlADwyY4OhlWWDGq+0oa3G1b/Gb54B8YeBPudBa6ybOdKELKC9OwIgiAIglBUtHX7cBgKl0P127ZXfSUAb322a7CzlR683XDP1+Gpn8L6p+GJK+C2I2HnJ9nOmSBkBWnsCIIgCIJQVOxo76GqzIVS/Rs71eVuGoaUsPqTnVnIWRpY8Wv4bBV8+Qr45t1w9NXQugWWngRt27KdO0EYdKSxIwiCIAhCUdHc7qGqzBV1+7SRQ1n1cTP+fJMUNG+Al/8Ae30FJswFpWDMgfDVa6GjCf52JvjlgalCcSGNHUEQBEEQiortrd0MLY0+bflLo6to7fLx4Retg5irNPDSYjAccMC54euH7QWzL4Utb8CK/81O3gQhS0hjJ42YpqaprYctOztpahNtpSBkAiln+YlcNyFX0FqzqbmDEVXRJ+xPqK8AYE0+NXbam+C9v8PEo6Gspv/2PQ8PbFv5O2hcO/j5E4QsITa2NGGamnXb27jg7tVs3tnFmJoy7jhnFlOGD8Ew+o8JFgQheaSc5Sdy3YRcorGth06Pn1FVpVHTDB9SittpsPaLtuC6j5vacTkMxtaWD0Y2k+etu8HvgWknR08z63zY/Br8+8dwzrLBy5sgZBHp2UkTOzo8wS9ygM07u7jg7tXs6BBPvyCkCyln+YlcNyGX+LipA4CR1dF7dgxDMaamjHXbAz07nR4fR934PHN/uyI3eyW1hncegIbpUDUmerrSKpjxLfh4BWx6cdCyJwjZRBo7acLj8we/yC027+zC4/NnKUeCUHhIOctP5LoJucRH2wO9NaNjNHYARleVsbEx0DB649OAmc1najY0tWc2g6mw/QNoXgfjj4yfdvJxUD5M5u4IRYM0dtKE2+lgTE14xTmmpgy3s//TmQVBSA0pZ/mJXDchl/jwi1aGlDqpKY9uYwMYUVXKttZuOj0+1m3rG862sTEHGzvvPQjKEZiXEw9nCUybB5++BF+8m/m8CUKWkcZOmhhW4eaOc2YFv9CtMenDKtxZzpkgFA5SzvITuW5CLvHB1lb2qC23fcZOKKN6e342NXewsakda3rZll1dMfbKAqYJ7z0Eow8IDFNLhElfBWcpvHZbZvMmCDlAxgUFSqnjgP8DHMCftNb/G7H9POAGYEvvqlu01n/KdL7SjWEopgwfwj+/NxuPz4/b6WBYhVsm3wpCGpFylp/IdRNyBZ/f5KPtbRw9bXjctCN7BQabmjtYv72dScOH8GlzR+41dj57OfDQ0H3nJ75PSSWMPwLe/wccfz24KzKXP0HIMhnt2VFKOYDfA8cD04H5SqnpNkkf0Frv1/vKu4aOhWEo6oeUMLJXZ/nF7i5RrApCksRTFFvlbHRNOfVDSuQHc54g9aOQC2zd1U2Pz2RMnPk6EBjGBgGhwYamdkZXl1E3pIStudbYee/vgV6asYckt9+Eo8DbCWufzEy+BCFHyHTPzkHABq31xwBKqb8B84A1GT5v1hDFqiCkjpSfwkaur5Btmtq7AahJYAhlidNBXWUJqz/dya5OL6Ory2hu78mtnh1fD6x5BMYeDK7oKm1bhk+HigZ49wGY8c3M5E8QcoBMz9kZDXwesry5d10kpyql3lVKPaSUGpvhPGUUUawKQupI+Sls5PoK2aaprQeA6jhyAouRVaW88FETAONqy6kuc9HclkPxuu5f0LUTJh6V/L7KCAxl2/gctDemP2+CkCPkgqDgMWBPrfUM4D/AXXaJlFIXKqVWK6VWNzU1DWoGk0EUq4JFvsRsLiHlJ3sMRrzK9RXSSSoxG2zslCXW2BkR8uDRPYdVUFXmYkdHD1rnyPDLt++FinoYuV9q+0+YA9oPHz6a0u4+08erX7zK39b+jbs+uIsXN79Ip7cztbwIQobI9DC2LUBoT80Y+kQEAGitd4Qs/gm43u5AWuvbgdsBZs2alSO1TH8sxWroF7ooVouTfInZXELKT/YYjHiV6yukk1RitrGtB0PB0NLEGjtjew2CZS4HlaVOqsrceP2a1i4fVQn2DmWM1i9gwzOwz2lgpFiGqveAIaMC83YO/G5Su77f/D7//eJ/80nrJ2HrSxwlHLPHMZy393lMqZ2SWr4EIY1kumfndWCSUmq8UsoNfAsIu32glBoZsngK8GGG85RRRLEqCKkj5aewkesrZJumth6GlLoSniN28Phh7FVfwZkHjwMINnCa2nsylseEeec+0CZMPDr1YygVmO+z6QXobk14t6c/eZpz/3Uu7d52Fs5YyE1zbmLJ3CVcOetKDh11KM98+gynPXYaFz9zMa9vez13esKEoiSjPTtaa59S6hLgKQLq6Tu11h8opX4BrNZaPwosUkqdAviAFuC8TOYpk5imZkeHh5pyFw9ceAh+U+MwFA2V/Y1RXq+fxvYefKbG2ZvG5XLYHk9UrUKxYKcori51sq21G6/fxOUwaKgswemMfp8mstzUlLnY2eUNO15Thyfh48VDymk4kf+PqhIHTR2eYF1n1Y+GAo2ivsIt/z9h0Nje2p3wfB2AoWUufvm1LwWXq3qHvzW397BXQ2Xa85cw3m549dbA8LUqu6nQSTDuYFjzz95eom/ETf5249v85MWfsMfQPfj+/t9niHtIcNu0YdOYNmwap046lec+e45nP3uW8586n2m10zhz2pkcP/54ShwlA8uvICRJxp+zo7V+EngyYt3PQ97/N/Dfmc5HprEsQzf9Zx3nHjaeHz/8btA2dOvZM5k6fEjwB5XX62dtYzsX//WNYJo/nj2TqQ2VwQaPWIuEYsVSFAP4fCZrt7exMKSsRJanUCLLzVenN7Do6Mlh+//x7Jnc/OxHPL2mMe7x4iHlNJzI/8dFX96Tk/YbE1bXXXfqDO5atYlzDxvPC+u2c/J+Y8KuTzH//4TM09TWk/B8HTtCGztZ5e17A1KBwxYN/Fj10wIPI133ZNzGzs7unVy6/FJqSmtYtP8iKt32Db4KVwUnTzyZY/c8lpVbVvLc58/xs5U/47erf8sxexzDV8Z9hQNHHIjbIb26QubJBUFBQWBZhk6dOTbY0IHA5NuFf32DxpCKsbG9J/jlb6W5OCKNWIsEIVBWFkaUlcjyFEpkuTl15th++1/81zc4debYhI4XDymn4UT+P06bNa5fXffjh98N1pOnzRrX7/oU8/9PyDxN7T1Ul6f+AzvY2GnLYmOnswWeuxYa9oYR+w78eIYDRh8I658Gvzdm0uteu45dPbu4ZL9LojZ0QnE73MwdN5dfHPYLfjjrh0ypmcLjGx9n4TMLOfxvh/P9Z7/Pg+seZGv71oF/DkGIQsZ7dooFyzJUXeaytQ35/GZw2Wdq+zQhD9cTa5EggNdvxi1PoUSWm2jlMfTObqzjxeP/t3fncU7V9/7HX59MZh92BoRBZBHRUREZRFxqXVHqViu97tv1p79aq7fW5bb11qWtbS39+bu2KlTUq9atLWq1WsUVF1RkEVmGRTbZnRkEZGCYLZ/7R04gk0kymclysnyejwcPkpNzcj5z8j3f5Jtzzju2n7YVuj3yPBJx+2/Y1hDx8Vzdfia5fD5la33T3gFLV3Qr9OIRqKt3aUDu88ErP4Y9O+DUu/3X3CTC4KNh1Vvw5Sx/QlsYszbO4tU1r3LO8HMY1G1Qp55eRKjsU0lln0qaW5tZsnUJi+oWsahuETM3zARgeI/hXHjwhZw34jw71c0klB3ZSZBAytD2hua9F98GDOpVjDdv36b2eiT8PEGnbQSeL3QeSy0yuSQ/z9Ph/hQsdL+JtD9ub2hucz/S83XE9tO2QrdHq08jbv9BvYojPp6r288k146GZlp82qlrdkJ5PEL34nwwn6cdAAAfxklEQVR3TmNrbYHXboPql2DM5dB7aOKee+CRkFfoT2ULt2pfK5PnTKZ/SX/OHHZmXKvKz8tndL/RXFZ5Gfd+615+fdyvuWDkBSjKPbPv4bv/+C5zt8yNax3GBLPBToIEUoaen7eee88f1SZtaOqlVfQr2/ctRb+yQqZcWtVmnikh81hqkTH+fWVqyL4Suj8FC91vnp+3vt3yUy6t4vl562N6vo7YftpW6PaYPnddu77u3vNH7e0np89d1+71yeXtZ5IrkKAWzzU74D+VLeWDnd1fwzPfhznToPI8OLTjIIFO8Rb5BzzLXoEwyWkvr3qZVTtWcf6I88n3JC5yW0QYWDaQ04eczu1H387NVTfT2NrIv8/4d55e+nTC1mNym2RiHODYsWN17tzkjPqDk4TyvR68HqGhqW1KUKT0pZYWn3Puv6IKrarkiT9tqKCg7RmDsaSxBZ4vUalRJiZJuSo6mW22s1KdHhbv+kL3lb4lBWxtaI64X3SUBhZYvqXVhzfz09jSrr0GXi8R/2cmnyoeETwCPvWf2tbqU+fsG3//uH1Pi6Wx5Q7X2uyslXVc8shsfnFWJZUDund5Xb99bSkiwkvXH9fl5+iULYvg2Yth5yY4+jo46PTkrGflWzDrv+Ha92Dgvh8pbWhp4KwXz6Isv4zbj74dSdSpcxE0tjTy8KKH+azmM35wxA+4fvT1SV1fDKxDynB2zU6QcMlKkyeN4vevL6e2vpFpl49lRHkZX9TWt0tfCjc9kDp04ykHtUt7ys/Po6JXSdRawq3HUopMPFKdHhbv+nw+ZWXdrqjpXqFpavGkuXVF8PpyXUuLj+U19fzx7RXtUimD+8M/BqXhWb9mUqVm5x4AesV5ZKd7UT5r6nYloqSOfVUNT5wNkgdn3AvlSfyRzkFHgXhg2attBjvPLH2Gmt01XHXUVUkf6AAUegu5fvT1PL7kcaZ+PpWehT255JBLkr5ek73sMEGQcMlKt05fyA9OHL43JaimvjFs+lK46YHUoa6kPVnKk0mGVLereNcXS7pXtP2rs2luJj6B7R0ulTK4PwxOw7N+zaRKrZOg1iOOa3Zg32lsST8zprEenrvYH0Jwxu+SO9ABf/x0/0P9p7I5djTu4NHFjzKq7yhG9k7y+oN4xMMVlVdwZL8j+f2c3zN78+yUrdtkHxvsBImUrBQ4v3fDtoao6VDRUoc6m/ZkKU8mGVLdruJdX6zpXpH2r86muZn4BLZ3tBS8cGl41q+ZVKjd2UiB10NxfnwBGD2K82ls8VHf2JKgyiJ4+27Ytha+dSt02y+56wrYfzzUVMPXqwF4fMnj7GzayfdGJPgaoRjkefK45vBr2K9kP257/zZqdtekvAaTHWywEyRSslIguWlQr+Ko6VDRUoc6m/ZkKU8mGVLdruJdX6zpXpH2r86muZn4BLZ3tBS8cGl41q+ZVAj8oGi8p2Lt+2HRJB6RrF0Bcx6Fkd+B/Q5P3npC7X+0//9l/6KuoY6nqp9i3H7jGNx9cOpqCFLkLeK60dexq3kXt753K60++2LEdJ694wcJl6w0edIops5ctffc8n5lhWHTl8JND6QOdSXtyVKeTDKkul3Fu75Y0r2i7V+dTXMz8Qls73CplMH9YXAanvVrJlVqdzbGfQobBA92kng67Lv3gLcQjrgoeesIp9t+0HsYLP0nf/78zzT5mjjvwPNSW0OIirIKLqu8jPk183my+klXazGZydLYaJv2lJ/nodAr7Gn2UZTvobHZR7NPyfcI+V4Pu5taKSvKY0+Tb2+6U7fiPHY2tNKjOI8dDa20+pQ8z770oaJ8D/WNrXsPnQdSpAAamlspK8yjsUXbpUvFkgxnEi7t0q0SLdnpYaHP370gj7rd+9LQ+hQX8PWefWlqvYvy2drQ9vGdza17l++Wn9fu8dD7wc/XtySfut377vcpzm+z/tDkw462h8tpax1xrb2GbpeeRV5qdzVRWiDUN/r7zz3N+/pJj8f/e4iBlLaSAg8NzWppbLnHtTZ72n3v0bMkn5+cFt+1J2u37uJnLyxiyiVjmHj4gLieK6zt6+D+I/zx0lVXJv75O/L5s2xY/BxnDx7McRXHc8WhV6S+hhCqygMLHmBJ3RL+fvbfGdZzWCpXbx1Shsv5NLbm5laW1dS3SXeacmkVI/qU8sXWXW2mT540ihfnb+S8MRXcOn1hm/kL8+Dr3c1t5g+kD/3o5BG8t6yGsUN7t1lu8qRRzF3zNd8+uB8/fHp+2LSo8m6FKU/QMtktmelhoW01XHralEur+JOTxnXXWQdTNbTv3scnVPbjhlMOijh/pPuvLNjAnz9YG9PywftOR/uW7XvhhW6XwHaft6aOqqF9eWXBBs48oqJNvxboD684dihPfLSGG045iG31DXxdVtwmLc+2r0mW2vpGDuhTGvfzJP3IzqfTAPGfwuaG4acwedMMvKqcM/wcd2oIISJcXnk5v5j1C27/8Hb+8p2/4PXk/EdYE6OcP42tpr6xXbrTdU/NY2tDU7vpt05fyDUnDNs7YAmev7Qwv938gfShHz49n3PHDGq33K3TF3LumEF7PxAEpoemRVkym8kUsaSnXReUxnVy5YA2j59ftX/U+SPdnzR2cMzLB+87He1btu+FF7pdAts98HpOGju4Xb8W6A8D/1/31DyG9+veLi3Ptq9JhqYWH9t3N9MzAaexdS/KR4DaZFyz07QL5j0Bg8dDWb/EP38M3m+q4Z3SEq7e1USvwh6u1BBOj8IeXHLIJSzeupgnljzhdjkmg+T8YKfFp+HTmiJMj5gGFWH+QPqQT8M/rhGmB6dFWTKbyRSxpqcF0rhC94toKV7R7uc5RwFiXT6w73S0b9m+F17odgnt56K97m1SKiP0m7m+fU3ibd3l/wIxEYOdPI/QrcibnCM7C/8KjTvgEHeOqOxsaeCeL56jwlPM1TWb6LZpoSt1RDJuv3FU9aviwQUPsnLbSrfLMRki5wc7Xo+ET2uKMD1iGlSE+QPpQx4J/7hEmB6cFmXJbCZTxJqeFkjjCt0voqV4Rbvf6tNOLR/Ydzrat2zfCy90u4T2c9Fe9zYplRH6zVzfvibxar5xfmMnzh8UDehTVthuoB43VfhkKvQeDv0qE/vcMfrtyr+ypXEbVx4wESkopf+iF1ypIxIR4bLKyyjyFvHzD39Oc2tzxwuZnJfzg51+ZYXt0p2mXFpFn+KCdtMnTxrFtPdXM3nSqHbz72psbjd/IH3ooUvG8NL8De2WmzxpFC/N38BDl4yJmhZlyWwmU8SSnjYlKI3rnerNbR5/ft76qPNHuj997rqYlw/edzrat2zfCy90uwS2e+D1nD53Xbt+LdAfBv6fcmkVq2q+aZeWZ9vXJMPmHXsA6F2SmLZV0bOYFVt2JuS59lo9E+qW+4/qxBmP3RXPbXqPf9Z8yln9xjG822C+GnIsvdZ+RNG2L1NeSzTdC7tzeeXlLP16KVMXTnW7HJMBcjaNLThJqKTQw+5GX7u0ptCUNm+esKepleKCPJpb/CltpQV5NDm3i/PzaPEpLT4feSL+vkqhwOthV1MrRV4PItImjW1PcyulThpbS6sPb1AaW6R6LbEoqbI+jS3ZQttqLGlq8aSx9S0pYGtD8979J5DGFrjfUdpXaL29ivPZ1tAc8X6a7XtpkcYmIhTlCw1N+1LY8vOE5lbdl07ppLEF/i8t9NDcKum+fU3iudJm/2fWGu7+ZzVTL61KyNGdlxZs5Lk561l41wS6FyXmaBHP/Busmw2THoO81A74365bwE+qpzGq21B+NOQsPOLB21jPqDd/zY4DxrNqwh0prScWjy56lI83f8yTE5/kiPIjkrkq65AyXE5GWcSasJSfn0dFr5KIyx47rA+XHnNAm8ShKZdWcXC/MvLz8xKa5JTMBC1jEim4rba0+Fj21c69F6FHSmcL7DMBRUVtu6aKDu4PLAy5X9D2fnmUX0wPrtfS12Ln8Qh9SgvCbq8R5WV8UVvP7FW1bdL2Aq/3yPJSCoJeI+vbTLJt3rGH/Dyhe1FiPvYEUt0WrNvOCQeVx/+EtStgxQz/7+qkeKDz0pZPuHPFUwwp6c//PWAiHvF/2dpSWMaWA0+iYvkMvtq8iPoBKfxx0xhcfMjFLN+2nJ998DOmnz2dkvySjhcyOSknT2OLJ2EpeNlrThjWLnHouqAkNUtyMrmupr6xTdpWpHS2mmT+OF8n2D7bOZG2V019I9c8Obdd2l7g9a617WlSbNP2BvqUFiIJOj3skAHdKMjz8PbSrxLyfHz8gH+QM/LMxDxfDPa0NvHrL57jv1Y8yciyCm4e+j0KPW2PUm058EQai3sx9N0/4GlO8DVKcSr2FnP1YVezYecG7vroLjLxTCWTGjk52IknYSl42WjJbPGux5hs0NzqiymdLbDPuM322c6JtL1anNc9UgplurzeJnds3NZAn7LEHTEp9OZRNaQXf5u3gY3b4xwE1NfA58/B8JOhuGdiCoxCVXmr7jPOmftL/rr5fU7vO4YfD/kuxWGOKPm8hawZcxGF32xiyHv3gfrCPKN7RvYeyfkjzue1ta/x2OLH3C7HpKmcHOzEk7AUvGy0ZLZ412NMNsjP88SUzuZNk1PEbJ/tnEjby+u87pFSKNPl9Ta5QVVZVVvPgB7FHc/cCReM3R8B/s8Tc9jV2AJA9aZvePLjtWzrzNHLD+4DXzNUnpfQ+sKp3rmOaxb9kZuqp+EVD7cNO58LBp6A1xO5j9vZ90A2HjKRPivfZfCsB9NuwDNx6ETG7TeO++ffz7vr3nW7HJOGcnKwE0/CUvCy095f3S5xaEpQkpolOZlc16+ssE3aVqR0tuD0QTfZPts5kbZXv7JCpl0+tl3aXuD1LrftaVKodmcj3+xpoaJnYgc7/bsXcePJI1i+ZSc3PDOfv3zyJec++CF3vLSE86d+RENTDEeEv14Ncx6BA0+FHhUJrS/YF7s2cVP1w1zw2e9YsvNLLhl4EneMuJiDy/aPafnNI05hy/Bv03/xSwx76zdpdUqbiHDVYVcxpMcQbn7vZj7a+JHbJZk0k/Q0NhE5A7gfyAMeUdXfhTxeCDwJVAFbgQtUdW2050x0GltnE4BiSXJLxHqMKyyNLcFaWnz+VEMnHa1PcT51u5si7jNuy7B91vX2Gml7BabneZSGpn19ZHlpQZtwApNzUt5mZ62s45JHZvPz7xzC4RU9Er7uN6u/4rFZawD/tTwnjezHQzNX8aOTDuSW00dGXtDngyfOgk2fwXenQEmfhNf2ZUMND619lddq51LkKWBC+ZFM6HskxXld+IJJlf1Wvsug6ldp7D6A1af8lF393fk9oHDqm+r5w9w/sHnXZn513K84c1jCrn9K2zcAE5ukvuOISB7wIHAasAGYIyIvq2p10GxXA9tU9UARuRC4F7ggmXVBfOlmocv2ihIAYilqJtd5vR4GhnyjWlGYvh92bZ/tnEjbq8300hQXZUyQ2Wu+xiMwrG9yGuJplf0ZXl7Ktt3NjN6/J3keYcH67Uz7YDUXjtufQZE+JMz8DXw5C469MeEDnQXfrObJDW/zdt0C8j1eJpZXcUZ5FWXeOI5uibBlxMnU9zqAYfOf4ZB//Ji6kaezYdxVtJT0TlzxXVRWUMYtY2/hwQUP8tMPfsqCmgXcVHWTpbSZpEdPjwNWqupqABF5DjgXCB7snAvc5dyeDjwgIqIWq2GMMcaYOL23vIahfUspTeKXLMPKy9rcv2jcYOau3ca9ry/nTxcd2XZmXyu8+xv44A8wYgIceFpCaqhr2sGM2vm8UvMpi3d+SWleIaeXj2FC3zH0yE/cQK++73CWnHQLA5e/Qb/lb9B75bvUHXw6Xx3+PRqTeCpeLMoKyrh57M1MXzGdvy7/K++sf4drDr+Gc4afY4OeHJbswU4FsD7o/gbg6EjzqGqLiOwA+gB1Sa7NGGOMMVnsreqv+HzDDi4bf0BK19u3rJAzRw3gxc82Mm5IL8YfUMaA5vWUfTXXf41O7TL/QGf89dCJOGyf+tjZ0sDXzfXUNe1gze4trNi1iXk7vmDl7s0A7F/Ul4sHnsjxvSopStJv9rTmF7P+sHOpGXIsA1e8SXn1K/Rf/BK7+o5g+wHj2VV+EHt6Daa5uBe+/GL/3xgINpDkXi7u9Xi58OALqepfxd9X/J17Zt/DffPu4+gBRzO2/1gO6nUQg8oG0ae4D8Xe4oTFkZv0lb7nkoQQkWuBawEGDx7scjXGdMzarMkk1l5NpumozTY0tfKfzy9kaJ9SJh62HwXe1GYyfX/sIFbV1nPnS4tYXHg1JeL8nlivIXDyHTDk+KgDnRk1c/jVsr/Qqq20qI8WbaVF24ceFHkKGFFWwfcHfpuqngdRUdw3SX9Re75eg9lw9NVsadhB7y8/ocfmhQyc9xTCvpNzVDygiqCsOf2X7Bj+7ZTUVtmnkjvG38EX279g1sZZVG+tZub6mW3mEYRibzEXjLyAn4z9SUrqMqmX1IACETkGuEtVT3fu/wxAVX8bNM8MZ56PRcQLbAHKo53GJiK1wJcxltGX7D9KlAt/I6Tm76xT1TMS/aSdbLNdle7twOqLT7j6Mrm9RpKJr0O6SOfawF/fMhfabLpvl2isdvcE6k9KP2tSJ9lHduYAI0RkKLARuBC4OGSel4ErgI+BScA7HV2vo6rlsRYgInNVdWynqs4wufA3Qmb/nZ1ps12V7tvH6otPKutLRXuNxF6Hrkvn2mBvfUn50Bitzab7donGandPptdv9knqYMe5BudHwAz80dOPqeoSEfklMFdVXwYeBf4iIiuBr/EPiIwxxhhjjDEmLkm/ZkdV/wX8K2TaHUG39wDfT3YdxhhjjDHGmNyS2qv13PGw2wWkQC78jZA7f2dXpfv2sfrik+71JUq6/53pXF861wbu1Zfu2yUaq909mV6/cSQ1oMAYY4wxxhhj3JILR3aMMcYYY4wxOSirBzsikicin4nIK27Xkiwi0lNEpovIMhFZ6sR9ZxURuUlElojIYhF5VkSK3K4pXYjI/iLyrohUO9voP9yuKZSIFInIpyLyuVPj3W7XFE469xcislZEFonIAhGZ63Y9iZYJ7RjSvo2k9XuBW/24iJwhIstFZKWI/DQV60wEEXlMRGpEZLHbtXRWpuzP4WTK+5XpnKwe7AD/ASx1u4gkux94XVUPBo4gy/5eEakAbgTGquph+FP9LLFvnxbgZlWtBMYD14tIpcs1hWoETlbVI4DRwBkiMt7lmsJJ9/7iJFUdnaVRqJnQjiG920javhe41Y+LSB7wIDARqAQuStN2Fc7jQKb+tkum7M/hZMr7lemErB3siMgg4EzgEbdrSRYR6QGcgD++G1VtUtXt7laVFF6g2PnR2RJgk8v1pA1V3ayq853bO/F/wKlwt6q21K/euZvv/EuriwVzob9IZ5nQjtO5jWTIe4Eb/fg4YKWqrlbVJuA54NwUrDduqvo+/p/jyDiZsD9HkgnvV6bzsnawA/w3cBvgc7uQJBoK1AL/45xa8YiIlLpdVCKp6kbgD8A6YDOwQ1XfcLeq9CQiQ4AjgdnuVtKec/rPAqAGeFNV063GdO8vFHhDROaJyLVuF5NMadyO07mNpPV7gYv9eAWwPuj+BjLkQ3e2SOP9OaIMeL8ynZSVgx0ROQuoUdV5bteSZF5gDDBFVY8EdgEZc05yLESkF/5v4oYCA4FSEbnU3arSj4iUAc8DP1bVb9yuJ5SqtqrqaGAQME5EDnO7poAM6S+OV9Ux+E/HuV5ETnC7oGRI13acAW0krd8LrB/PTem6P3cknd+vTNdk5WAHOA44R0TW4j9sfbKIPOVuSUmxAdgQ9K3DdPxveNnkVGCNqtaqajPwAnCsyzWlFRHJx/+G8rSqvuB2PdE4p9a8S3qdi572/YXzzTiqWgO8iP/0nKyS5u043dtIur8XuNWPbwT2D7o/yJlmkizN9+eYpOn7lemCrBzsqOrPVHWQqg7BfxHkO6qadd8iqeoWYL2IjHQmnQJUu1hSMqwDxotIiYgI/r8xbS68dZuzTR4FlqrqfW7XE46IlItIT+d2MXAasMzdqvZJ9/5CREpFpFvgNjAByLiEpmjSvR2nexvJgPcCt/rxOcAIERkqIgX4X7uXU7DenJbu+3M06f5+ZbrG63YBJm43AE87Hflq4CqX60koVZ0tItOB+fgTXj7DftU42HHAZcAi5xxjgJ+r6r9crCnUAOAJJxnJA/xNVdMuujeN9Qde9H9+wAs8o6qvu1tSwmVCO053afte4FY/rqotIvIjYAb+BLjHVHVJstebCCLyLHAi0FdENgB3quqj7lYVs0zen+39KguJqoVMGGOMMcYYY7JPVp7GZowxxhhjjDE22DHGGGOMMcZkJRvsGGOMMcYYY7KSDXaMMcYYY4wxWckGO8YYY4wxxpisZIMdY4wxxhhjTFaywU6GE5ETRSRiBryIXCkiDyRhvVeKyMCg+2tFpG+i12OyV0dtN4blx4rIHyM8tlZE+opITxH5YaLWabJHaB8WZb7HRWRSlMdnisjYBNc2RETC/nBsrHWb7JWothvD8r8UkVPDTN/bjzq3j03UOo1JBhvsmK66ErA3XOMaVZ2rqjd2MFtP4IcdzGNy05VkZh92JZlZt0mcK0lBG1DVO1T1rQ5mOxE4toN5jHGVDXZSQERKReRVEflcRBaLyAUiUiUi74nIPBGZISIDnHlnisj9IrLAmXecM32ciHwsIp+JyEciMrILdZSLyPMiMsf5d5wz/S4RecxZ92oRuTFomV+IyHIR+VBEnhWRW5xvbcbi/7XuBSJS7Mx+g4jMF5FFInJw3BvOuM7Ntuu0o57it1VELnemPykip4V8u9hHRN4QkSUi8gggztP8Dhju1DTZmVYmItNFZJmIPC0i0n7tJtM4R0MCr+lS5zUuCddew/VhInKH0y8uFpGHu9IuRGSC09bni8jfRaTMmb5WRO4O7R+dPvnNQLsVkS9l3xHyPBGZ5jz2hlNjpL7XZDA32q6IHCUiLzi3zxWRBhEpEJEiEVntTN97lEZEznBqnA98L1A38APgJqeWbzlPf4LT168WO8pj0oANdlLjDGCTqh6hqocBrwN/AiapahXwGHBP0Pwlqjoa/zfSjznTlgHfUtUjgTuA33ShjvuB/6+qRwHnA48EPXYwcDowDrhTRPJFJDDfEcBE/B0sqjodmAtcoqqjVbXBeY46VR0DTAFu6UJ9Jv242XZnAccBhwKrgcAb6THARyHz3gl8qKqHAi8Cg53pPwVWOe30VmfakcCPgUpgmLMOkx1GAg+p6iHAN8D1hGmvEfqwB1T1KKedFwNndWbFziDlv4BTnX5wLvCToFnC9Y93Au847XY6+9otwAjgQeex7cD5Ufpek/lS3XY/A0Y7t78FLAaOAo4GZgfPKCJFwDTgbKAK2A9AVdcCU/F/rhitqh84iwwAjnfq+F1nN4QxieZ1u4AcsQj4fyJyL/AKsA04DHjT+QImD9gcNP+zAKr6voh0F5GeQDfgCREZASiQ34U6TgUqg7706R745hF4VVUbgUYRqQH64/8Q+JKq7gH2iMg/O3j+F5z/5+F882Mynptt9wPgBOBL/B8QrxWRCmCbqu4K+fLyBJw2p6qvisi2KM/7qapuABCRBcAQ4MMYazLpbb2qznJuPwX8nOjtNdhJInIbUAL0BpYAHfV5wcbjH0DPctZVAHwc9Hi4/vF44DwAVX09pN2uUdUFQcsM6UQtJvOktO2qaouIrBKRQ/B/yXkf/n40D3/fG+xg/O3xCwAReQq4NsrT/0NVfUC1iPSPVocxqWCDnRRQ1RUiMgb4DvBr4B1giaoeE2mRMPd/Bbyrquc5h45ndqEUDzDeGbzs5XSkjUGTWula2wg8R1eXN2nG5bb7Pv5vNwcDt+P/UDiJ9m/EnZWItm7SU2j720n09grs/eb6IWCsqq4XkbuAok6uW4A3VfWiCI93tn8Mbad2ylp2c6Ptvo//rI1m4C3gcfyDnVujLBOL4LZrpwkb19lpbCkg/tSU3ar6FDAZ/2HichE5xnk8X0QODVrkAmf68cAOVd0B9AA2Oo9f2cVS3gBuCKprdJR5wX8a0dnOObxltD00vhP/N/Ymi7nZdlV1PdAXGKGqq/EffbkF/xt0qPeBi511TwR6OdOtneaWwYG2ib89fELk9hrcNgIfDuucvq4r1xl8AhwnIgc66yoVkYM6WGYW8G/O/BPY126jsTadndxoux/gP6X3Y1WtBfrgP50uNAlwGTBERIY794MH9NYeTdqzwU5qHA586pwycyf+6xYmAfeKyOfAAtqmmewRkc/wnwt7tTPt98Bvneld/Sb6RmCsiCwUkWr8FxZGpKpzgJeBhcBr+E9p2uE8/Dgw1S6SzXput93ZwArn9gdABeFPObsb/0WxS/CfIrQOQFW34j+taLHsCygw2Ws5cL2ILMU/cPgTkdvr4zh9GP5voqfh/5A3A5jT2RU7HxavBJ4VkYX4T2HrKKjlbmCC+GOmvw9swf/hMZq9dVvfm1XcaLuz8Z+yHvgCaSGwSFXbHGVyzga5FnjVCSioCXr4n8B5IQEFxqQVCWnTxmUiMhO4RVXnul0LgIiUqWq9iJTg7xCvVdX5btdl0k+6tV2TW5xTJF9xLtLOCCJSCLQ6108cA0xxAj5MDsnEtmtMJrFz1U1HHhaRSvyHyp+wgY4xxiTMYOBvIuIBmoBrXK7HGGOyjh3ZyRIichXwHyGTZ6nq9W7UY0ysrO2aTCAiLwJDQyb/p6rOcKMeY2JlbdfkOhvsGGOMMcYYY7KSBRQYY4wxxhhjspINdowxxhhjjDFZyQY7xhhjjDHGmKxkgx1jjDHGGGNMVrLBjjHGGGOMMSYr/S/ekm2y7cyCMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 823.25x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is_R9oWGCn-O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebd66a69-2134-45d0-cfcb-5feab6235555"
      },
      "source": [
        "dataset = df.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y_obj = dataset[:,4]\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# 라벨인코더 + 원한인코딩\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "Y_encoded = tf.keras.utils.to_categorical(Y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=4, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss = \"categorical_crossentropy\", optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(X, Y_encoded, epochs = 50, batch_size = 1)\n",
        "\n",
        "print(\"|n Accuracy: %.4f\" % (model.evaluate(X,Y_encoded)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "150/150 [==============================] - 0s 936us/step - loss: 1.4297 - accuracy: 0.3600\n",
            "Epoch 2/50\n",
            "150/150 [==============================] - 0s 854us/step - loss: 0.9161 - accuracy: 0.4533\n",
            "Epoch 3/50\n",
            "150/150 [==============================] - 0s 781us/step - loss: 0.7148 - accuracy: 0.7400\n",
            "Epoch 4/50\n",
            "150/150 [==============================] - 0s 860us/step - loss: 0.6155 - accuracy: 0.7800\n",
            "Epoch 5/50\n",
            "150/150 [==============================] - 0s 813us/step - loss: 0.5447 - accuracy: 0.7667\n",
            "Epoch 6/50\n",
            "150/150 [==============================] - 0s 796us/step - loss: 0.4939 - accuracy: 0.7800\n",
            "Epoch 7/50\n",
            "150/150 [==============================] - 0s 798us/step - loss: 0.4823 - accuracy: 0.8333\n",
            "Epoch 8/50\n",
            "150/150 [==============================] - 0s 757us/step - loss: 0.4620 - accuracy: 0.8067\n",
            "Epoch 9/50\n",
            "150/150 [==============================] - 0s 783us/step - loss: 0.4314 - accuracy: 0.8467\n",
            "Epoch 10/50\n",
            "150/150 [==============================] - 0s 756us/step - loss: 0.4118 - accuracy: 0.9067\n",
            "Epoch 11/50\n",
            "150/150 [==============================] - 0s 818us/step - loss: 0.4046 - accuracy: 0.8733\n",
            "Epoch 12/50\n",
            "150/150 [==============================] - 0s 770us/step - loss: 0.3742 - accuracy: 0.9000\n",
            "Epoch 13/50\n",
            "150/150 [==============================] - 0s 814us/step - loss: 0.3489 - accuracy: 0.8867\n",
            "Epoch 14/50\n",
            "150/150 [==============================] - 0s 799us/step - loss: 0.3502 - accuracy: 0.8667\n",
            "Epoch 15/50\n",
            "150/150 [==============================] - 0s 882us/step - loss: 0.3443 - accuracy: 0.9000\n",
            "Epoch 16/50\n",
            "150/150 [==============================] - 0s 781us/step - loss: 0.3278 - accuracy: 0.9200\n",
            "Epoch 17/50\n",
            "150/150 [==============================] - 0s 786us/step - loss: 0.3118 - accuracy: 0.9400\n",
            "Epoch 18/50\n",
            "150/150 [==============================] - 0s 757us/step - loss: 0.3016 - accuracy: 0.9200\n",
            "Epoch 19/50\n",
            "150/150 [==============================] - 0s 793us/step - loss: 0.2896 - accuracy: 0.9133\n",
            "Epoch 20/50\n",
            "150/150 [==============================] - 0s 810us/step - loss: 0.2776 - accuracy: 0.9533\n",
            "Epoch 21/50\n",
            "150/150 [==============================] - 0s 772us/step - loss: 0.2695 - accuracy: 0.9400\n",
            "Epoch 22/50\n",
            "150/150 [==============================] - 0s 770us/step - loss: 0.2614 - accuracy: 0.9267\n",
            "Epoch 23/50\n",
            "150/150 [==============================] - 0s 809us/step - loss: 0.2414 - accuracy: 0.9667\n",
            "Epoch 24/50\n",
            "150/150 [==============================] - 0s 757us/step - loss: 0.2413 - accuracy: 0.9467\n",
            "Epoch 25/50\n",
            "150/150 [==============================] - 0s 836us/step - loss: 0.2334 - accuracy: 0.9533\n",
            "Epoch 26/50\n",
            "150/150 [==============================] - 0s 780us/step - loss: 0.2253 - accuracy: 0.9600\n",
            "Epoch 27/50\n",
            "150/150 [==============================] - 0s 804us/step - loss: 0.2191 - accuracy: 0.9533\n",
            "Epoch 28/50\n",
            "150/150 [==============================] - 0s 832us/step - loss: 0.2122 - accuracy: 0.9467\n",
            "Epoch 29/50\n",
            "150/150 [==============================] - 0s 810us/step - loss: 0.2118 - accuracy: 0.9667\n",
            "Epoch 30/50\n",
            "150/150 [==============================] - 0s 780us/step - loss: 0.1959 - accuracy: 0.9667\n",
            "Epoch 31/50\n",
            "150/150 [==============================] - 0s 848us/step - loss: 0.1930 - accuracy: 0.9600\n",
            "Epoch 32/50\n",
            "150/150 [==============================] - 0s 747us/step - loss: 0.1845 - accuracy: 0.9800\n",
            "Epoch 33/50\n",
            "150/150 [==============================] - 0s 827us/step - loss: 0.1754 - accuracy: 0.9733\n",
            "Epoch 34/50\n",
            "150/150 [==============================] - 0s 752us/step - loss: 0.1752 - accuracy: 0.9800\n",
            "Epoch 35/50\n",
            "150/150 [==============================] - 0s 783us/step - loss: 0.1717 - accuracy: 0.9600\n",
            "Epoch 36/50\n",
            "150/150 [==============================] - 0s 732us/step - loss: 0.1662 - accuracy: 0.9733\n",
            "Epoch 37/50\n",
            "150/150 [==============================] - 0s 754us/step - loss: 0.1612 - accuracy: 0.9733\n",
            "Epoch 38/50\n",
            "150/150 [==============================] - 0s 732us/step - loss: 0.1609 - accuracy: 0.9533\n",
            "Epoch 39/50\n",
            "150/150 [==============================] - 0s 837us/step - loss: 0.1540 - accuracy: 0.9533\n",
            "Epoch 40/50\n",
            "150/150 [==============================] - 0s 785us/step - loss: 0.1505 - accuracy: 0.9733\n",
            "Epoch 41/50\n",
            "150/150 [==============================] - 0s 782us/step - loss: 0.1508 - accuracy: 0.9533\n",
            "Epoch 42/50\n",
            "150/150 [==============================] - 0s 732us/step - loss: 0.1416 - accuracy: 0.9733\n",
            "Epoch 43/50\n",
            "150/150 [==============================] - 0s 763us/step - loss: 0.1357 - accuracy: 0.9733\n",
            "Epoch 44/50\n",
            "150/150 [==============================] - 0s 749us/step - loss: 0.1398 - accuracy: 0.9733\n",
            "Epoch 45/50\n",
            "150/150 [==============================] - 0s 854us/step - loss: 0.1355 - accuracy: 0.9667\n",
            "Epoch 46/50\n",
            "150/150 [==============================] - 0s 783us/step - loss: 0.1278 - accuracy: 0.9733\n",
            "Epoch 47/50\n",
            "150/150 [==============================] - 0s 828us/step - loss: 0.1217 - accuracy: 0.9733\n",
            "Epoch 48/50\n",
            "150/150 [==============================] - 0s 778us/step - loss: 0.1227 - accuracy: 0.9733\n",
            "Epoch 49/50\n",
            "150/150 [==============================] - 0s 780us/step - loss: 0.1280 - accuracy: 0.9667\n",
            "Epoch 50/50\n",
            "150/150 [==============================] - 0s 773us/step - loss: 0.1207 - accuracy: 0.9733\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9800\n",
            "|n Accuracy: 0.9800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wClO6bXZFXMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "11f52236-5d45-466e-d323-c462712f9a3b"
      },
      "source": [
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/sonar.csv', header=None)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0200</td>\n",
              "      <td>0.0371</td>\n",
              "      <td>0.0428</td>\n",
              "      <td>0.0207</td>\n",
              "      <td>0.0954</td>\n",
              "      <td>0.0986</td>\n",
              "      <td>0.1539</td>\n",
              "      <td>0.1601</td>\n",
              "      <td>0.3109</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.1609</td>\n",
              "      <td>0.1582</td>\n",
              "      <td>0.2238</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.0660</td>\n",
              "      <td>0.2273</td>\n",
              "      <td>0.3100</td>\n",
              "      <td>0.2999</td>\n",
              "      <td>0.5078</td>\n",
              "      <td>0.4797</td>\n",
              "      <td>0.5783</td>\n",
              "      <td>0.5071</td>\n",
              "      <td>0.4328</td>\n",
              "      <td>0.5550</td>\n",
              "      <td>0.6711</td>\n",
              "      <td>0.6415</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.8080</td>\n",
              "      <td>0.6791</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>0.1307</td>\n",
              "      <td>0.2604</td>\n",
              "      <td>0.5121</td>\n",
              "      <td>0.7547</td>\n",
              "      <td>0.8537</td>\n",
              "      <td>0.8507</td>\n",
              "      <td>0.6692</td>\n",
              "      <td>0.6097</td>\n",
              "      <td>0.4943</td>\n",
              "      <td>0.2744</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>0.2834</td>\n",
              "      <td>0.2825</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.2641</td>\n",
              "      <td>0.1386</td>\n",
              "      <td>0.1051</td>\n",
              "      <td>0.1343</td>\n",
              "      <td>0.0383</td>\n",
              "      <td>0.0324</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0159</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0167</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0453</td>\n",
              "      <td>0.0523</td>\n",
              "      <td>0.0843</td>\n",
              "      <td>0.0689</td>\n",
              "      <td>0.1183</td>\n",
              "      <td>0.2583</td>\n",
              "      <td>0.2156</td>\n",
              "      <td>0.3481</td>\n",
              "      <td>0.3337</td>\n",
              "      <td>0.2872</td>\n",
              "      <td>0.4918</td>\n",
              "      <td>0.6552</td>\n",
              "      <td>0.6919</td>\n",
              "      <td>0.7797</td>\n",
              "      <td>0.7464</td>\n",
              "      <td>0.9444</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.8874</td>\n",
              "      <td>0.8024</td>\n",
              "      <td>0.7818</td>\n",
              "      <td>0.5212</td>\n",
              "      <td>0.4052</td>\n",
              "      <td>0.3957</td>\n",
              "      <td>0.3914</td>\n",
              "      <td>0.3250</td>\n",
              "      <td>0.3200</td>\n",
              "      <td>0.3271</td>\n",
              "      <td>0.2767</td>\n",
              "      <td>0.4423</td>\n",
              "      <td>0.2028</td>\n",
              "      <td>0.3788</td>\n",
              "      <td>0.2947</td>\n",
              "      <td>0.1984</td>\n",
              "      <td>0.2341</td>\n",
              "      <td>0.1306</td>\n",
              "      <td>0.4182</td>\n",
              "      <td>0.3835</td>\n",
              "      <td>0.1057</td>\n",
              "      <td>0.1840</td>\n",
              "      <td>0.1970</td>\n",
              "      <td>0.1674</td>\n",
              "      <td>0.0583</td>\n",
              "      <td>0.1401</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.0621</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>0.0530</td>\n",
              "      <td>0.0742</td>\n",
              "      <td>0.0409</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0125</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0191</td>\n",
              "      <td>0.0140</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0582</td>\n",
              "      <td>0.1099</td>\n",
              "      <td>0.1083</td>\n",
              "      <td>0.0974</td>\n",
              "      <td>0.2280</td>\n",
              "      <td>0.2431</td>\n",
              "      <td>0.3771</td>\n",
              "      <td>0.5598</td>\n",
              "      <td>0.6194</td>\n",
              "      <td>0.6333</td>\n",
              "      <td>0.7060</td>\n",
              "      <td>0.5544</td>\n",
              "      <td>0.5320</td>\n",
              "      <td>0.6479</td>\n",
              "      <td>0.6931</td>\n",
              "      <td>0.6759</td>\n",
              "      <td>0.7551</td>\n",
              "      <td>0.8929</td>\n",
              "      <td>0.8619</td>\n",
              "      <td>0.7974</td>\n",
              "      <td>0.6737</td>\n",
              "      <td>0.4293</td>\n",
              "      <td>0.3648</td>\n",
              "      <td>0.5331</td>\n",
              "      <td>0.2413</td>\n",
              "      <td>0.5070</td>\n",
              "      <td>0.8533</td>\n",
              "      <td>0.6036</td>\n",
              "      <td>0.8514</td>\n",
              "      <td>0.8512</td>\n",
              "      <td>0.5045</td>\n",
              "      <td>0.1862</td>\n",
              "      <td>0.2709</td>\n",
              "      <td>0.4232</td>\n",
              "      <td>0.3043</td>\n",
              "      <td>0.6116</td>\n",
              "      <td>0.6756</td>\n",
              "      <td>0.5375</td>\n",
              "      <td>0.4719</td>\n",
              "      <td>0.4647</td>\n",
              "      <td>0.2587</td>\n",
              "      <td>0.2129</td>\n",
              "      <td>0.2222</td>\n",
              "      <td>0.2111</td>\n",
              "      <td>0.0176</td>\n",
              "      <td>0.1348</td>\n",
              "      <td>0.0744</td>\n",
              "      <td>0.0130</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0244</td>\n",
              "      <td>0.0316</td>\n",
              "      <td>0.0164</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0623</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0205</td>\n",
              "      <td>0.0368</td>\n",
              "      <td>0.1098</td>\n",
              "      <td>0.1276</td>\n",
              "      <td>0.0598</td>\n",
              "      <td>0.1264</td>\n",
              "      <td>0.0881</td>\n",
              "      <td>0.1992</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>0.2261</td>\n",
              "      <td>0.1729</td>\n",
              "      <td>0.2131</td>\n",
              "      <td>0.0693</td>\n",
              "      <td>0.2281</td>\n",
              "      <td>0.4060</td>\n",
              "      <td>0.3973</td>\n",
              "      <td>0.2741</td>\n",
              "      <td>0.3690</td>\n",
              "      <td>0.5556</td>\n",
              "      <td>0.4846</td>\n",
              "      <td>0.3140</td>\n",
              "      <td>0.5334</td>\n",
              "      <td>0.5256</td>\n",
              "      <td>0.2520</td>\n",
              "      <td>0.2090</td>\n",
              "      <td>0.3559</td>\n",
              "      <td>0.6260</td>\n",
              "      <td>0.7340</td>\n",
              "      <td>0.6120</td>\n",
              "      <td>0.3497</td>\n",
              "      <td>0.3953</td>\n",
              "      <td>0.3012</td>\n",
              "      <td>0.5408</td>\n",
              "      <td>0.8814</td>\n",
              "      <td>0.9857</td>\n",
              "      <td>0.9167</td>\n",
              "      <td>0.6121</td>\n",
              "      <td>0.5006</td>\n",
              "      <td>0.3210</td>\n",
              "      <td>0.3202</td>\n",
              "      <td>0.4295</td>\n",
              "      <td>0.3654</td>\n",
              "      <td>0.2655</td>\n",
              "      <td>0.1576</td>\n",
              "      <td>0.0681</td>\n",
              "      <td>0.0294</td>\n",
              "      <td>0.0241</td>\n",
              "      <td>0.0121</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0117</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0762</td>\n",
              "      <td>0.0666</td>\n",
              "      <td>0.0481</td>\n",
              "      <td>0.0394</td>\n",
              "      <td>0.0590</td>\n",
              "      <td>0.0649</td>\n",
              "      <td>0.1209</td>\n",
              "      <td>0.2467</td>\n",
              "      <td>0.3564</td>\n",
              "      <td>0.4459</td>\n",
              "      <td>0.4152</td>\n",
              "      <td>0.3952</td>\n",
              "      <td>0.4256</td>\n",
              "      <td>0.4135</td>\n",
              "      <td>0.4528</td>\n",
              "      <td>0.5326</td>\n",
              "      <td>0.7306</td>\n",
              "      <td>0.6193</td>\n",
              "      <td>0.2032</td>\n",
              "      <td>0.4636</td>\n",
              "      <td>0.4148</td>\n",
              "      <td>0.4292</td>\n",
              "      <td>0.5730</td>\n",
              "      <td>0.5399</td>\n",
              "      <td>0.3161</td>\n",
              "      <td>0.2285</td>\n",
              "      <td>0.6995</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>0.7262</td>\n",
              "      <td>0.4724</td>\n",
              "      <td>0.5103</td>\n",
              "      <td>0.5459</td>\n",
              "      <td>0.2881</td>\n",
              "      <td>0.0981</td>\n",
              "      <td>0.1951</td>\n",
              "      <td>0.4181</td>\n",
              "      <td>0.4604</td>\n",
              "      <td>0.3217</td>\n",
              "      <td>0.2828</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.1979</td>\n",
              "      <td>0.2444</td>\n",
              "      <td>0.1847</td>\n",
              "      <td>0.0841</td>\n",
              "      <td>0.0692</td>\n",
              "      <td>0.0528</td>\n",
              "      <td>0.0357</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0230</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0107</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>R</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       0       1       2       3       4   ...      56      57      58      59  60\n",
              "0  0.0200  0.0371  0.0428  0.0207  0.0954  ...  0.0180  0.0084  0.0090  0.0032   R\n",
              "1  0.0453  0.0523  0.0843  0.0689  0.1183  ...  0.0140  0.0049  0.0052  0.0044   R\n",
              "2  0.0262  0.0582  0.1099  0.1083  0.0974  ...  0.0316  0.0164  0.0095  0.0078   R\n",
              "3  0.0100  0.0171  0.0623  0.0205  0.0205  ...  0.0050  0.0044  0.0040  0.0117   R\n",
              "4  0.0762  0.0666  0.0481  0.0394  0.0590  ...  0.0072  0.0048  0.0107  0.0094   R\n",
              "\n",
              "[5 rows x 61 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blzPwC3jFtIe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2f56b4e6-3af7-4673-afe4-153a347494c7"
      },
      "source": [
        "dataset = df.values\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim = 60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.fit(X, Y, epochs = 200, batch_size=5)\n",
        "\n",
        "print(\"|n Accuracy: %.4f\" % (model.evaluate(X,Y)[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.6953 - accuracy: 0.5721\n",
            "Epoch 2/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.6346\n",
            "Epoch 3/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.6397 - accuracy: 0.6875\n",
            "Epoch 4/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.6297 - accuracy: 0.6538\n",
            "Epoch 5/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.6029 - accuracy: 0.7260\n",
            "Epoch 6/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.5775 - accuracy: 0.7644\n",
            "Epoch 7/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.5470 - accuracy: 0.7740\n",
            "Epoch 8/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7981\n",
            "Epoch 9/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7548\n",
            "Epoch 10/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.4815 - accuracy: 0.7788\n",
            "Epoch 11/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.4653 - accuracy: 0.7981\n",
            "Epoch 12/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8269\n",
            "Epoch 13/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.4209 - accuracy: 0.8269\n",
            "Epoch 14/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8317\n",
            "Epoch 15/200\n",
            "42/42 [==============================] - 0s 988us/step - loss: 0.3906 - accuracy: 0.8317\n",
            "Epoch 16/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8221\n",
            "Epoch 17/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8606\n",
            "Epoch 18/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3712 - accuracy: 0.8221\n",
            "Epoch 19/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3537 - accuracy: 0.8606\n",
            "Epoch 20/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8510\n",
            "Epoch 21/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3449 - accuracy: 0.8413\n",
            "Epoch 22/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.8702\n",
            "Epoch 23/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8750\n",
            "Epoch 24/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8942\n",
            "Epoch 25/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8462\n",
            "Epoch 26/200\n",
            "42/42 [==============================] - 0s 1000us/step - loss: 0.2908 - accuracy: 0.9087\n",
            "Epoch 27/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8894\n",
            "Epoch 28/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2855 - accuracy: 0.9087\n",
            "Epoch 29/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2819 - accuracy: 0.8942\n",
            "Epoch 30/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8846\n",
            "Epoch 31/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8990\n",
            "Epoch 32/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2548 - accuracy: 0.9135\n",
            "Epoch 33/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.8990\n",
            "Epoch 34/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.8942\n",
            "Epoch 35/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.8942\n",
            "Epoch 36/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9135\n",
            "Epoch 37/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9279\n",
            "Epoch 38/200\n",
            "42/42 [==============================] - 0s 977us/step - loss: 0.2373 - accuracy: 0.9135\n",
            "Epoch 39/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9087\n",
            "Epoch 40/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9327\n",
            "Epoch 41/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9327\n",
            "Epoch 42/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9087\n",
            "Epoch 43/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9279\n",
            "Epoch 44/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9471\n",
            "Epoch 45/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9327\n",
            "Epoch 46/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.8990\n",
            "Epoch 47/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9375\n",
            "Epoch 48/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9135\n",
            "Epoch 49/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9327\n",
            "Epoch 50/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9327\n",
            "Epoch 51/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9423\n",
            "Epoch 52/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9231\n",
            "Epoch 53/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9375\n",
            "Epoch 54/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9375\n",
            "Epoch 55/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9423\n",
            "Epoch 56/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9519\n",
            "Epoch 57/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9231\n",
            "Epoch 58/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9663\n",
            "Epoch 59/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9663\n",
            "Epoch 60/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9663\n",
            "Epoch 61/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9471\n",
            "Epoch 62/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9471\n",
            "Epoch 63/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9519\n",
            "Epoch 64/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9663\n",
            "Epoch 65/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.1216 - accuracy: 0.9615\n",
            "Epoch 66/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9615\n",
            "Epoch 67/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9615\n",
            "Epoch 68/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9615\n",
            "Epoch 69/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9808\n",
            "Epoch 70/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9760\n",
            "Epoch 71/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9760\n",
            "Epoch 72/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9808\n",
            "Epoch 73/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9760\n",
            "Epoch 74/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9808\n",
            "Epoch 75/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9760\n",
            "Epoch 76/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9856\n",
            "Epoch 77/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9760\n",
            "Epoch 78/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9904\n",
            "Epoch 79/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9808\n",
            "Epoch 80/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9760\n",
            "Epoch 81/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9904\n",
            "Epoch 82/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9952\n",
            "Epoch 83/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9904\n",
            "Epoch 84/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9904\n",
            "Epoch 85/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9904\n",
            "Epoch 86/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9856\n",
            "Epoch 87/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9952\n",
            "Epoch 88/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9952\n",
            "Epoch 89/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9952\n",
            "Epoch 90/200\n",
            "42/42 [==============================] - 0s 996us/step - loss: 0.0541 - accuracy: 0.9952\n",
            "Epoch 91/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0497 - accuracy: 0.9952\n",
            "Epoch 92/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 1.0000\n",
            "Epoch 93/200\n",
            "42/42 [==============================] - 0s 975us/step - loss: 0.0545 - accuracy: 0.9904\n",
            "Epoch 94/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 1.0000\n",
            "Epoch 95/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0493 - accuracy: 0.9952\n",
            "Epoch 96/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9904\n",
            "Epoch 97/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 1.0000\n",
            "Epoch 98/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 1.0000\n",
            "Epoch 99/200\n",
            "42/42 [==============================] - 0s 979us/step - loss: 0.0378 - accuracy: 1.0000\n",
            "Epoch 100/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 1.0000\n",
            "Epoch 101/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 1.0000\n",
            "Epoch 102/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 1.0000\n",
            "Epoch 103/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0344 - accuracy: 1.0000\n",
            "Epoch 104/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.9904\n",
            "Epoch 105/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 1.0000\n",
            "Epoch 106/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 1.0000\n",
            "Epoch 107/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0280 - accuracy: 1.0000\n",
            "Epoch 108/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0290 - accuracy: 1.0000\n",
            "Epoch 109/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0272 - accuracy: 1.0000\n",
            "Epoch 110/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 1.0000\n",
            "Epoch 111/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 1.0000\n",
            "Epoch 112/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 1.0000\n",
            "Epoch 113/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 114/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0242 - accuracy: 1.0000\n",
            "Epoch 115/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 1.0000\n",
            "Epoch 116/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 1.0000\n",
            "Epoch 117/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0281 - accuracy: 1.0000\n",
            "Epoch 118/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 1.0000\n",
            "Epoch 119/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0204 - accuracy: 1.0000\n",
            "Epoch 120/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 1.0000\n",
            "Epoch 121/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 122/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0191 - accuracy: 1.0000\n",
            "Epoch 123/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 1.0000\n",
            "Epoch 124/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0162 - accuracy: 1.0000\n",
            "Epoch 125/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0178 - accuracy: 1.0000\n",
            "Epoch 126/200\n",
            "42/42 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 1.0000\n",
            "Epoch 127/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 128/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0159 - accuracy: 1.0000\n",
            "Epoch 129/200\n",
            "42/42 [==============================] - 0s 995us/step - loss: 0.0146 - accuracy: 1.0000\n",
            "Epoch 130/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0141 - accuracy: 1.0000\n",
            "Epoch 131/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 132/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 1.0000\n",
            "Epoch 133/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0131 - accuracy: 1.0000\n",
            "Epoch 134/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0117 - accuracy: 1.0000\n",
            "Epoch 135/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0112 - accuracy: 1.0000\n",
            "Epoch 136/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0104 - accuracy: 1.0000\n",
            "Epoch 137/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0105 - accuracy: 1.0000\n",
            "Epoch 138/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 139/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0120 - accuracy: 1.0000\n",
            "Epoch 140/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 141/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 142/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0089 - accuracy: 1.0000\n",
            "Epoch 143/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0099 - accuracy: 1.0000\n",
            "Epoch 144/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 1.0000\n",
            "Epoch 145/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0085 - accuracy: 1.0000\n",
            "Epoch 146/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 147/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0088 - accuracy: 1.0000\n",
            "Epoch 148/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0083 - accuracy: 1.0000\n",
            "Epoch 149/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 150/200\n",
            "42/42 [==============================] - 0s 987us/step - loss: 0.0071 - accuracy: 1.0000\n",
            "Epoch 151/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 152/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0062 - accuracy: 1.0000\n",
            "Epoch 153/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 154/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0063 - accuracy: 1.0000\n",
            "Epoch 155/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 156/200\n",
            "42/42 [==============================] - 0s 996us/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 157/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 158/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 159/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 160/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 161/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 162/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0055 - accuracy: 1.0000\n",
            "Epoch 163/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 164/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 165/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0045 - accuracy: 1.0000\n",
            "Epoch 166/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 167/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 168/200\n",
            "42/42 [==============================] - 0s 992us/step - loss: 0.0041 - accuracy: 1.0000\n",
            "Epoch 169/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 170/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 171/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 172/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 173/200\n",
            "42/42 [==============================] - 0s 995us/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 174/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 175/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0033 - accuracy: 1.0000\n",
            "Epoch 176/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0048 - accuracy: 1.0000\n",
            "Epoch 177/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 178/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0035 - accuracy: 1.0000\n",
            "Epoch 179/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 180/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 181/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0039 - accuracy: 1.0000\n",
            "Epoch 182/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0027 - accuracy: 1.0000\n",
            "Epoch 183/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 184/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 185/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 186/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 187/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 188/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 189/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 190/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 191/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0022 - accuracy: 1.0000\n",
            "Epoch 192/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 194/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 195/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 196/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 197/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 198/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 199/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0016 - accuracy: 1.0000\n",
            "Epoch 200/200\n",
            "42/42 [==============================] - 0s 1ms/step - loss: 0.0015 - accuracy: 1.0000\n",
            "7/7 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000\n",
            "|n Accuracy: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdaUxfKQOdgR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6551333-514d-4824-edb7-5634d7294397"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(3)\n",
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/sonar.csv', header=None)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim = 60, activation='relu'))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=130, batch_size=5)\n",
        "\n",
        "print(\"|n Accuracy: %.4f\" % (model.evaluate(X_test,Y_test)[1]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.4897\n",
            "Epoch 2/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.6414\n",
            "Epoch 3/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.6207\n",
            "Epoch 4/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.6897\n",
            "Epoch 5/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.7379\n",
            "Epoch 6/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.7517\n",
            "Epoch 7/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.7517\n",
            "Epoch 8/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.8069\n",
            "Epoch 9/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.7862\n",
            "Epoch 10/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.8069\n",
            "Epoch 11/130\n",
            "29/29 [==============================] - 0s 981us/step - loss: 0.1544 - accuracy: 0.8138\n",
            "Epoch 12/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.8000\n",
            "Epoch 13/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.8000\n",
            "Epoch 14/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.8069\n",
            "Epoch 15/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.8207\n",
            "Epoch 16/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.8000\n",
            "Epoch 17/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.8345\n",
            "Epoch 18/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.8345\n",
            "Epoch 19/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.8276\n",
            "Epoch 20/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.8621\n",
            "Epoch 21/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.8690\n",
            "Epoch 22/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.8483\n",
            "Epoch 23/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.8621\n",
            "Epoch 24/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.8897\n",
            "Epoch 25/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.8414\n",
            "Epoch 26/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.8621\n",
            "Epoch 27/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.8690\n",
            "Epoch 28/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.8828\n",
            "Epoch 29/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.8759\n",
            "Epoch 30/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.8759\n",
            "Epoch 31/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.8690\n",
            "Epoch 32/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9034\n",
            "Epoch 33/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9034\n",
            "Epoch 34/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.8966\n",
            "Epoch 35/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9103\n",
            "Epoch 36/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9103\n",
            "Epoch 37/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9172\n",
            "Epoch 38/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9310\n",
            "Epoch 39/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9310\n",
            "Epoch 40/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9379\n",
            "Epoch 41/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9241\n",
            "Epoch 42/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9379\n",
            "Epoch 43/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9241\n",
            "Epoch 44/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9379\n",
            "Epoch 45/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0559 - accuracy: 0.9379\n",
            "Epoch 46/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9379\n",
            "Epoch 47/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9517\n",
            "Epoch 48/130\n",
            "29/29 [==============================] - 0s 980us/step - loss: 0.0493 - accuracy: 0.9448\n",
            "Epoch 49/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9517\n",
            "Epoch 50/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0462 - accuracy: 0.9655\n",
            "Epoch 51/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9448\n",
            "Epoch 52/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9586\n",
            "Epoch 53/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9517\n",
            "Epoch 54/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9793\n",
            "Epoch 55/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9724\n",
            "Epoch 56/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9931\n",
            "Epoch 57/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9862\n",
            "Epoch 58/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9862\n",
            "Epoch 59/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9931\n",
            "Epoch 60/130\n",
            "29/29 [==============================] - 0s 955us/step - loss: 0.0296 - accuracy: 0.9793\n",
            "Epoch 61/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0279 - accuracy: 0.9862\n",
            "Epoch 62/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9862\n",
            "Epoch 63/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9793\n",
            "Epoch 64/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 1.0000\n",
            "Epoch 65/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0215 - accuracy: 0.9931\n",
            "Epoch 66/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9862\n",
            "Epoch 67/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0177 - accuracy: 1.0000\n",
            "Epoch 68/130\n",
            "29/29 [==============================] - 0s 995us/step - loss: 0.0198 - accuracy: 1.0000\n",
            "Epoch 69/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0195 - accuracy: 1.0000\n",
            "Epoch 70/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 1.0000\n",
            "Epoch 71/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0175 - accuracy: 0.9931\n",
            "Epoch 72/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0150 - accuracy: 1.0000\n",
            "Epoch 73/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0211 - accuracy: 0.9862\n",
            "Epoch 74/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 75/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0133 - accuracy: 1.0000\n",
            "Epoch 76/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0128 - accuracy: 1.0000\n",
            "Epoch 77/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 78/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0147 - accuracy: 1.0000\n",
            "Epoch 79/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0129 - accuracy: 1.0000\n",
            "Epoch 80/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0119 - accuracy: 0.9931\n",
            "Epoch 81/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 82/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0115 - accuracy: 1.0000\n",
            "Epoch 83/130\n",
            "29/29 [==============================] - 0s 996us/step - loss: 0.0107 - accuracy: 1.0000\n",
            "Epoch 84/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0100 - accuracy: 1.0000\n",
            "Epoch 85/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 86/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 87/130\n",
            "29/29 [==============================] - 0s 1000us/step - loss: 0.0073 - accuracy: 1.0000\n",
            "Epoch 88/130\n",
            "29/29 [==============================] - 0s 975us/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 89/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 90/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0074 - accuracy: 1.0000\n",
            "Epoch 91/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 92/130\n",
            "29/29 [==============================] - 0s 991us/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 93/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 94/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0050 - accuracy: 1.0000\n",
            "Epoch 95/130\n",
            "29/29 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000\n",
            "Epoch 96/130\n",
            "29/29 [==============================] - 0s 985us/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 97/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0046 - accuracy: 1.0000\n",
            "Epoch 98/130\n",
            "29/29 [==============================] - 0s 993us/step - loss: 0.0051 - accuracy: 1.0000\n",
            "Epoch 99/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 100/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 101/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 102/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 103/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0043 - accuracy: 1.0000\n",
            "Epoch 104/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "Epoch 105/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 106/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0042 - accuracy: 1.0000\n",
            "Epoch 107/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0031 - accuracy: 1.0000\n",
            "Epoch 108/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 109/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0034 - accuracy: 1.0000\n",
            "Epoch 110/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0029 - accuracy: 1.0000\n",
            "Epoch 111/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0028 - accuracy: 1.0000\n",
            "Epoch 112/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 113/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 114/130\n",
            "29/29 [==============================] - 0s 971us/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Epoch 115/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0025 - accuracy: 1.0000\n",
            "Epoch 116/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 117/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0024 - accuracy: 1.0000\n",
            "Epoch 118/130\n",
            "29/29 [==============================] - 0s 988us/step - loss: 0.0020 - accuracy: 1.0000\n",
            "Epoch 119/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 120/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 121/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 122/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 123/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0019 - accuracy: 1.0000\n",
            "Epoch 124/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 125/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0021 - accuracy: 1.0000\n",
            "Epoch 126/130\n",
            "29/29 [==============================] - 0s 947us/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 127/130\n",
            "29/29 [==============================] - 0s 999us/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 128/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 129/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 130/130\n",
            "29/29 [==============================] - 0s 1ms/step - loss: 0.0014 - accuracy: 1.0000\n",
            "2/2 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.8571\n",
            "|n Accuracy: 0.8571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDuLB5iqTVio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex5jzrges0tz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8098db34-784d-4c6e-af95-836a8f70d113"
      },
      "source": [
        "model2 = load_model('my_model.h5')\n",
        "print(\"accuracy : %.4f\" % (model2.evaluate(X_test, Y_test)[1]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 2ms/step - loss: 0.1447 - accuracy: 0.8571\n",
            "accuracy : 0.8571\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_52aSrOtimA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e4d46a41-761b-4f13-bb1b-060c1ec85ccb"
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/sonar.csv', header=None)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:,0:60].astype(float)\n",
        "Y_obj = dataset[:,60]\n",
        "\n",
        "e = LabelEncoder()\n",
        "e.fit(Y_obj)\n",
        "Y = e.transform(Y_obj)\n",
        "\n",
        "n_fold = 10\n",
        "skf = StratifiedKFold(n_splits = n_fold, shuffle= True, random_state = seed)\n",
        "\n",
        "accuracy = []\n",
        "\n",
        "for train, test in skf.split(X,Y):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(24, input_dim = 60, activation='relu'))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(loss=\"mean_squared_error\", optimizer='adam', metrics=[\"accuracy\"])\n",
        "  model.fit(X[train], Y[train], epochs=100, batch_size=5)\n",
        "  k_accuracy = \"%.4f\" % (model.evaluate(X[test], Y[test])[1])\n",
        "  accuracy.append(k_accuracy)\n",
        "\n",
        "print(\"\\n %.f fold accuracy\" % n_fold, accuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.5455\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.6310\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.6578\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.7219\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.7754\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.7861\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.7861\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.8021\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.8021\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.8289\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.8503\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.8449\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.8449\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.8396\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.8503\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.8503\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.8503\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.8503\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.8396\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.8610\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.8717\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.7914\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.8556\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.8556\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.8717\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.8877\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.8877\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.8984\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9144\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9198\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.8984\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9037\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9037\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9198\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9305\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9091\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9358\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9198\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9412\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9465\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9358\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9251\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9572\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9412\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9465\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0524 - accuracy: 0.9626\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9465\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9412\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9465\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9733\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0424 - accuracy: 0.9679\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9626\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0405 - accuracy: 0.9679\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0386 - accuracy: 0.9733\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9786\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0357 - accuracy: 0.9733\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0373 - accuracy: 0.9786\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9786\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9679\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9733\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 987us/step - loss: 0.0335 - accuracy: 0.9679\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0297 - accuracy: 0.9840\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9733\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0300 - accuracy: 0.9840\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9733\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0269 - accuracy: 0.9840\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9840\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0229 - accuracy: 0.9840\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9893\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0216 - accuracy: 0.9840\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0270 - accuracy: 0.9786\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0205 - accuracy: 0.9893\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0185 - accuracy: 0.9893\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0189 - accuracy: 0.9840\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0168 - accuracy: 0.9893\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0217 - accuracy: 0.9893\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 0.9893\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0143 - accuracy: 0.9947\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0126 - accuracy: 0.9947\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0167 - accuracy: 0.9893\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0170 - accuracy: 0.9947\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0111 - accuracy: 0.9947\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9840\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9786\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0161 - accuracy: 0.9947\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0111 - accuracy: 0.9947\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0098 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0096 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0086 - accuracy: 0.9947\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0068 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0065 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0060 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.2494 - accuracy: 0.7143\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.5187\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.5882\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.5989\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.7647\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.7219\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.7754\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.7968\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.7914\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.8182\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.8182\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.8075\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.8289\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1000us/step - loss: 0.1382 - accuracy: 0.8128\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.8289\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 985us/step - loss: 0.1269 - accuracy: 0.8235\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.8396\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.8396\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.8342\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.8449\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.8663\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.8503\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.7968\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.8717\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.8503\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.8610\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.8824\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.8770\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.8610\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.8770\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.8717\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.8877\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.8930\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.8663\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.8984\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.8556\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9091\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9037\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.8770\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.8984\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9037\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9091\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.8984\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9144\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9091\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9198\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9251\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9037\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9412\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9251\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.9358\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9412\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9091\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9358\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9358\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9251\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9465\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9251\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.9465\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9465\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0541 - accuracy: 0.9519\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9251\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0516 - accuracy: 0.9626\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9572\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0588 - accuracy: 0.9412\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9465\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0458 - accuracy: 0.9626\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0523 - accuracy: 0.9572\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9465\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9679\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0428 - accuracy: 0.9733\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9358\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 993us/step - loss: 0.0416 - accuracy: 0.9626\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0399 - accuracy: 0.9679\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0400 - accuracy: 0.9626\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 998us/step - loss: 0.0400 - accuracy: 0.9679\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9679\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 994us/step - loss: 0.0360 - accuracy: 0.9679\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0359 - accuracy: 0.9733\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0318 - accuracy: 0.9840\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0315 - accuracy: 0.9786\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0302 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9840\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9840\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0268 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9786\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0254 - accuracy: 0.9840\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0246 - accuracy: 0.9733\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9840\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0206 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0192 - accuracy: 0.9893\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0226 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0165 - accuracy: 0.9947\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0180 - accuracy: 0.9947\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0157 - accuracy: 0.9893\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0153 - accuracy: 0.9947\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0140 - accuracy: 0.9947\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.6667\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.4813\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.6203\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.6096\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.6738\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.6898\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.7112\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.7112\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.7701\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.7433\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.7754\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.7326\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.7647\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.7594\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.7914\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.7968\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.7754\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.8182\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.8075\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.8396\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.8182\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1354 - accuracy: 0.8449\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.7968\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.8396\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.8182\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.8342\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.8396\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.8770\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.8717\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.8663\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.8503\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.8824\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.8770\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.8610\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.8824\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.8717\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.8610\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.8930\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.8877\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.8877\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.8930\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.8824\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.8984\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.8877\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9037\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9091\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.8930\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9037\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9037\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9144\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9251\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9198\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9251\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9251\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.9198\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9305\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9465\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9305\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9465\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9305\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9465\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9465\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9358\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9358\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9412\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9519\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9519\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9519\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0437 - accuracy: 0.9679\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9572\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0436 - accuracy: 0.9679\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9626\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0409 - accuracy: 0.9679\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0416 - accuracy: 0.9679\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9626\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9519\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0389 - accuracy: 0.9679\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9679\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0360 - accuracy: 0.9679\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9679\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9679\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9679\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.9733\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0308 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0292 - accuracy: 0.9786\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0337 - accuracy: 0.9626\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0282 - accuracy: 0.9786\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0289 - accuracy: 0.9733\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0274 - accuracy: 0.9786\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0263 - accuracy: 0.9786\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0269 - accuracy: 0.9733\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9786\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0259 - accuracy: 0.9786\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9733\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9893\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0251 - accuracy: 0.9733\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0249 - accuracy: 0.9786\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0220 - accuracy: 0.9786\n",
            "WARNING:tensorflow:5 out of the last 7 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05d5a06bf8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1825 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.5348\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.5348\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.5294\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.6150\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.6738\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2189 - accuracy: 0.6845\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.6898\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.7219\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.7005\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.8021\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.7433\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.7219\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.7807\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.8075\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.8235\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.7701\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.8128\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.8021\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.8342\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.8396\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.8075\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.7861\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.8396\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.8449\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.8556\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.8289\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.8556\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.8503\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.8610\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.8663\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.8610\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.8556\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.8717\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.8824\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.8717\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.8877\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.8770\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.8770\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.8663\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.8930\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.8877\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9091\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.8984\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9251\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9144\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9037\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.8930\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9305\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9198\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.8930\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9198\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9251\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9305\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9519\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9305\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9465\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9412\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9412\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9412\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9412\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9412\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9679\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9251\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9465\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0571 - accuracy: 0.9358\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9519\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9412\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0468 - accuracy: 0.9626\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0455 - accuracy: 0.9626\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0470 - accuracy: 0.9412\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9465\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0458 - accuracy: 0.9733\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9626\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9679\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9786\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0414 - accuracy: 0.9519\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9679\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0372 - accuracy: 0.9733\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0349 - accuracy: 0.9786\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9733\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9733\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0328 - accuracy: 0.9786\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9786\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9733\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9786\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0306 - accuracy: 0.9786\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0302 - accuracy: 0.9786\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0312 - accuracy: 0.9679\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9733\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0266 - accuracy: 0.9840\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0273 - accuracy: 0.9840\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0264 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0260 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0250 - accuracy: 0.9840\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0238 - accuracy: 0.9840\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9840\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0243 - accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0239 - accuracy: 0.9840\n",
            "WARNING:tensorflow:6 out of the last 8 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05cc61fc80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.8571\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.5882\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.6257\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.6310\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.6471\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.7112\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.7647\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.7647\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.8235\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.8075\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.7914\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.7754\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.8021\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.8128\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.8396\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.8396\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.8182\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1370 - accuracy: 0.8342\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.8128\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.8289\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.8235\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.8396\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.7754\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.8610\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.8342\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.8503\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.8503\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.8717\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.8770\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.8984\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.8717\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.8877\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.8556\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.8824\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.8930\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.8770\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9091\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.8930\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.8770\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.8770\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.8930\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.8556\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.8930\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9305\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9198\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9305\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9144\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9144\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9251\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9144\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9305\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9198\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9198\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9305\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9251\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0684 - accuracy: 0.9144\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.8984\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9305\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9305\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9198\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9305\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 996us/step - loss: 0.0621 - accuracy: 0.9305\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9412\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9251\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9412\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9305\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9358\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9412\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9465\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9465\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9358\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9412\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9626\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9519\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9412\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9519\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9305\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9412\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0479 - accuracy: 0.9519\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9465\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9572\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9572\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9679\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9626\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0381 - accuracy: 0.9679\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0413 - accuracy: 0.9572\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0386 - accuracy: 0.9733\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9733\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0362 - accuracy: 0.9679\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0340 - accuracy: 0.9679\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0325 - accuracy: 0.9679\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9572\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0309 - accuracy: 0.9786\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9679\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0296 - accuracy: 0.9786\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0278 - accuracy: 0.9786\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0277 - accuracy: 0.9786\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0262 - accuracy: 0.9786\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9947\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9893\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9840\n",
            "WARNING:tensorflow:7 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05cc6132f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.5668\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.5668\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.5668\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.6524\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.6952\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.7273\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.7433\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.7861\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.7701\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1711 - accuracy: 0.8021\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.7487\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.7754\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.7968\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.8182\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.8128\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.8128\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.8235\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.8021\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.8235\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.7968\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.7968\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.7968\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.8289\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.8289\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.8342\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.8503\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.8556\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.8556\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.8556\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.8663\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.8717\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.8556\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.8663\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.8717\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.8610\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.8610\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.8770\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.8877\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.8770\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.8770\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.8877\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.8824\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9037\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.8824\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.8824\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.8877\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.8663\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.8877\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.8877\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.8877\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.8930\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.8877\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9144\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9251\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9144\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.8824\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9144\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9305\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9144\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9251\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9198\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9198\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9037\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9198\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9358\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9198\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9144\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9144\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 996us/step - loss: 0.0696 - accuracy: 0.9305\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9091\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9465\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9465\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9144\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9519\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9465\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9358\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9251\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9519\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9572\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9519\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9412\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9358\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9358\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9572\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9519\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9519\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9572\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0559 - accuracy: 0.9465\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9572\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9572\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9572\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9519\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9519\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9465\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0573 - accuracy: 0.9412\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9358\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9572\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0495 - accuracy: 0.9679\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9519\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9519\n",
            "WARNING:tensorflow:8 out of the last 10 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05d5980730> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1539 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.5722\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.6738\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.6524\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.7166\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.7487\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.7380\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.7487\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.8075\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.8075\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.8075\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.8021\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.8075\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.7968\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.8289\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.8396\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.8449\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.8556\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.8556\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.8610\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.8717\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.8717\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.8235\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.8824\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.8717\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.8824\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.8930\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.8663\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.8824\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9251\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9251\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9251\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9144\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9144\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9198\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9144\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9198\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9305\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9465\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9465\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9305\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.9465\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9305\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9358\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9412\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0530 - accuracy: 0.9626\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0529 - accuracy: 0.9465\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9358\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9679\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.9626\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.9626\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9733\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9412\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0378 - accuracy: 0.9679\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9679\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0353 - accuracy: 0.9572\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0375 - accuracy: 0.9786\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 0.9786\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0329 - accuracy: 0.9679\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0322 - accuracy: 0.9733\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0324 - accuracy: 0.9786\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9626\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9465\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9840\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0276 - accuracy: 0.9840\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9840\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0228 - accuracy: 0.9893\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0212 - accuracy: 0.9947\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9893\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9947\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 1.0000\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 1.0000\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 1.0000\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 1.0000\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 1.0000\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0139 - accuracy: 0.9947\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 1.0000\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9947\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9840\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 1.0000\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 1.0000\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 1.0000\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 1.0000\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 1.0000\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 1.0000\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 1.0000\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000\n",
            "WARNING:tensorflow:9 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05d58987b8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0888 - accuracy: 0.8571\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2409 - accuracy: 0.5455\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2277 - accuracy: 0.6150\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2176 - accuracy: 0.6578\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2103 - accuracy: 0.7112\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.7433\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1906 - accuracy: 0.7326\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1830 - accuracy: 0.7487\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1735 - accuracy: 0.7701\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1657 - accuracy: 0.7807\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - accuracy: 0.8128\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1624 - accuracy: 0.7647\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1486 - accuracy: 0.7754\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1515 - accuracy: 0.7968\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.7968\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1389 - accuracy: 0.8128\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1336 - accuracy: 0.8289\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.8182\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.8342\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.8128\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.8235\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1192 - accuracy: 0.8824\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.7914\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1202 - accuracy: 0.8503\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1217 - accuracy: 0.8182\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1047 - accuracy: 0.8824\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.8610\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1026 - accuracy: 0.8770\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1005 - accuracy: 0.8877\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.8824\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.8877\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9091\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0944 - accuracy: 0.9037\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.8930\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0922 - accuracy: 0.9037\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0886 - accuracy: 0.9091\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.8984\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0855 - accuracy: 0.8984\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0823 - accuracy: 0.9198\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9091\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0832 - accuracy: 0.9037\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9144\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9251\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9251\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9305\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0729 - accuracy: 0.9358\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9305\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0749 - accuracy: 0.9251\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0680 - accuracy: 0.9251\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9251\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0701 - accuracy: 0.9305\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9198\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9358\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0641 - accuracy: 0.9251\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9251\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0629 - accuracy: 0.9465\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9412\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9412\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9412\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9572\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9519\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0569 - accuracy: 0.9412\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9519\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9465\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9412\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9412\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0534 - accuracy: 0.9519\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9519\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0502 - accuracy: 0.9519\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0493 - accuracy: 0.9572\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9412\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9572\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9572\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9519\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0466 - accuracy: 0.9572\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9626\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9626\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 0.9572\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0439 - accuracy: 0.9572\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9626\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9626\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9679\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0402 - accuracy: 0.9626\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0395 - accuracy: 0.9572\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0398 - accuracy: 0.9626\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.9519\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0412 - accuracy: 0.9626\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0376 - accuracy: 0.9626\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9519\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9679\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0361 - accuracy: 0.9679\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0364 - accuracy: 0.9626\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0338 - accuracy: 0.9733\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0342 - accuracy: 0.9626\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0316 - accuracy: 0.9733\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9679\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0313 - accuracy: 0.9679\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9572\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9572\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0298 - accuracy: 0.9733\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0287 - accuracy: 0.9733\n",
            "WARNING:tensorflow:10 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05d5756378> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.1379 - accuracy: 0.7619\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.5266\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.6436\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.6862\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.7128\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.7074\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.7128\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.7447\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.7819\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.7979\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.7979\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.8085\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.8298\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.7872\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.8085\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.8191\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.8138\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.7979\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 993us/step - loss: 0.1373 - accuracy: 0.8138\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.8085\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.8351\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.8191\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.8085\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.8298\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.8085\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.8404\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.8511\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.8617\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.8511\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.8404\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.8511\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.8617\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.8511\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.8457\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.8830\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.8777\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.8670\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.8989\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.8670\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 976us/step - loss: 0.0985 - accuracy: 0.8670\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.8883\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9096\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.8723\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.8936\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.8883\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.8936\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.8723\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 961us/step - loss: 0.0853 - accuracy: 0.9043\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9043\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9043\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.8989\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.8936\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9096\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9309\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9096\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9255\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9309\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9255\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9255\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9202\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9202\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9309\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9202\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9362\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9362\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9468\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9309\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9415\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9362\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9362\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9415\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9574\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9521\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0530 - accuracy: 0.9574\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9734\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0539 - accuracy: 0.9468\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9521\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9628\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0448 - accuracy: 0.9628\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0475 - accuracy: 0.9628\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0472 - accuracy: 0.9681\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0423 - accuracy: 0.9734\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0400 - accuracy: 0.9734\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9734\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0404 - accuracy: 0.9787\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9734\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0379 - accuracy: 0.9734\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9681\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9734\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9574\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0352 - accuracy: 0.9734\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0343 - accuracy: 0.9787\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0331 - accuracy: 0.9787\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0326 - accuracy: 0.9734\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0351 - accuracy: 0.9787\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0350 - accuracy: 0.9681\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0314 - accuracy: 0.9734\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9787\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0332 - accuracy: 0.9734\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0291 - accuracy: 0.9734\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0277 - accuracy: 0.9787\n",
            "WARNING:tensorflow:10 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05d560e1e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9000\n",
            "Epoch 1/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.5372\n",
            "Epoch 2/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.5904\n",
            "Epoch 3/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.6649\n",
            "Epoch 4/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.6755\n",
            "Epoch 5/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.7181\n",
            "Epoch 6/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.7766\n",
            "Epoch 7/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.7447\n",
            "Epoch 8/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.7660\n",
            "Epoch 9/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.7819\n",
            "Epoch 10/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.7926\n",
            "Epoch 11/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.7926\n",
            "Epoch 12/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.8298\n",
            "Epoch 13/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.8085\n",
            "Epoch 14/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.8138\n",
            "Epoch 15/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.8032\n",
            "Epoch 16/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.8191\n",
            "Epoch 17/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.7979\n",
            "Epoch 18/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.8404\n",
            "Epoch 19/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.8245\n",
            "Epoch 20/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.8245\n",
            "Epoch 21/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.8298\n",
            "Epoch 22/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.8032\n",
            "Epoch 23/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.8404\n",
            "Epoch 24/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.8511\n",
            "Epoch 25/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.8404\n",
            "Epoch 26/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.8298\n",
            "Epoch 27/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.8617\n",
            "Epoch 28/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.8670\n",
            "Epoch 29/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.8670\n",
            "Epoch 30/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.8617\n",
            "Epoch 31/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.8830\n",
            "Epoch 32/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.8564\n",
            "Epoch 33/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9043\n",
            "Epoch 34/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.8989\n",
            "Epoch 35/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.8936\n",
            "Epoch 36/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.8883\n",
            "Epoch 37/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.8989\n",
            "Epoch 38/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9043\n",
            "Epoch 39/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.8989\n",
            "Epoch 40/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9149\n",
            "Epoch 41/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.8936\n",
            "Epoch 42/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9149\n",
            "Epoch 43/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9149\n",
            "Epoch 44/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9202\n",
            "Epoch 45/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9309\n",
            "Epoch 46/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9255\n",
            "Epoch 47/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9202\n",
            "Epoch 48/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9043\n",
            "Epoch 49/100\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9309\n",
            "Epoch 50/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9362\n",
            "Epoch 51/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9202\n",
            "Epoch 52/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9521\n",
            "Epoch 53/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9255\n",
            "Epoch 54/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9468\n",
            "Epoch 55/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9415\n",
            "Epoch 56/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9415\n",
            "Epoch 57/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9309\n",
            "Epoch 58/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9628\n",
            "Epoch 59/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9415\n",
            "Epoch 60/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9468\n",
            "Epoch 61/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0557 - accuracy: 0.9521\n",
            "Epoch 62/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0513 - accuracy: 0.9574\n",
            "Epoch 63/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9521\n",
            "Epoch 64/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9574\n",
            "Epoch 65/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9574\n",
            "Epoch 66/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9468\n",
            "Epoch 67/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0478 - accuracy: 0.9574\n",
            "Epoch 68/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9628\n",
            "Epoch 69/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.9521\n",
            "Epoch 70/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0467 - accuracy: 0.9681\n",
            "Epoch 71/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9574\n",
            "Epoch 72/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0427 - accuracy: 0.9787\n",
            "Epoch 73/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0463 - accuracy: 0.9574\n",
            "Epoch 74/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0421 - accuracy: 0.9628\n",
            "Epoch 75/100\n",
            "38/38 [==============================] - 0s 975us/step - loss: 0.0389 - accuracy: 0.9681\n",
            "Epoch 76/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9734\n",
            "Epoch 77/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0380 - accuracy: 0.9787\n",
            "Epoch 78/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9787\n",
            "Epoch 79/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0358 - accuracy: 0.9787\n",
            "Epoch 80/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0367 - accuracy: 0.9787\n",
            "Epoch 81/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0327 - accuracy: 0.9787\n",
            "Epoch 82/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0356 - accuracy: 0.9787\n",
            "Epoch 83/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0330 - accuracy: 0.9734\n",
            "Epoch 84/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0366 - accuracy: 0.9681\n",
            "Epoch 85/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9840\n",
            "Epoch 86/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0294 - accuracy: 0.9840\n",
            "Epoch 87/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0285 - accuracy: 0.9840\n",
            "Epoch 88/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0305 - accuracy: 0.9734\n",
            "Epoch 89/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0323 - accuracy: 0.9734\n",
            "Epoch 90/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0261 - accuracy: 0.9840\n",
            "Epoch 91/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0255 - accuracy: 0.9894\n",
            "Epoch 92/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0248 - accuracy: 0.9840\n",
            "Epoch 93/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0247 - accuracy: 0.9840\n",
            "Epoch 94/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0271 - accuracy: 0.9840\n",
            "Epoch 95/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0236 - accuracy: 0.9894\n",
            "Epoch 96/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0230 - accuracy: 0.9894\n",
            "Epoch 97/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0233 - accuracy: 0.9894\n",
            "Epoch 98/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0224 - accuracy: 0.9840\n",
            "Epoch 99/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0222 - accuracy: 0.9840\n",
            "Epoch 100/100\n",
            "38/38 [==============================] - 0s 1ms/step - loss: 0.0196 - accuracy: 0.9894\n",
            "WARNING:tensorflow:11 out of the last 12 calls to <function Model.make_test_function.<locals>.test_function at 0x7f05c6553f28> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 0.0904 - accuracy: 0.9000\n",
            "|n 10 fold accuracy ['0.7143', '0.6667', '0.7619', '0.8571', '0.7619', '0.7619', '0.8571', '0.7619', '0.9000', '0.9000']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfPZqAOSxxiP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82d94cca-879c-4040-9b4d-2b27e53d6e95"
      },
      "source": [
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac = 1)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "model.fit(X,Y, epochs = 200, batch_size = 200)\n",
        "\n",
        "print(\"\\n Accuracy : %.4f\" % (model.evaluate(X, Y)[1]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.7522\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 989us/step - loss: 0.3072 - accuracy: 0.8952\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9235\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9272\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9280\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9312\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 996us/step - loss: 0.2019 - accuracy: 0.9338\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9341\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9364\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 981us/step - loss: 0.1879 - accuracy: 0.9372\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9364\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 969us/step - loss: 0.1795 - accuracy: 0.9383\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9404\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9407\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9400\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9432\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 978us/step - loss: 0.1631 - accuracy: 0.9447\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9438\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 955us/step - loss: 0.1531 - accuracy: 0.9454\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 974us/step - loss: 0.1499 - accuracy: 0.9458\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9464\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9472\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 985us/step - loss: 0.1406 - accuracy: 0.9478\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9500\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9521\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9531\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9557\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9554\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9584\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9604\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 977us/step - loss: 0.1078 - accuracy: 0.9632\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 972us/step - loss: 0.1092 - accuracy: 0.9628\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 993us/step - loss: 0.1035 - accuracy: 0.9651\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9671\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9671\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9680\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9697\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 997us/step - loss: 0.0978 - accuracy: 0.9688\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9701\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9734\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9724\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 995us/step - loss: 0.0907 - accuracy: 0.9718\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 995us/step - loss: 0.0879 - accuracy: 0.9720\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 969us/step - loss: 0.0856 - accuracy: 0.9746\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9726\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9740\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9734\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9746\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9741\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9749\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9741\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 978us/step - loss: 0.0876 - accuracy: 0.9729\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9748\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9769\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9761\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9758\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 994us/step - loss: 0.0767 - accuracy: 0.9752\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9751\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 977us/step - loss: 0.0784 - accuracy: 0.9744\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 978us/step - loss: 0.0751 - accuracy: 0.9751\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9774\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9757\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9777\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9785\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9760\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9783\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9763\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9768\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9783\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 999us/step - loss: 0.0674 - accuracy: 0.9791\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 995us/step - loss: 0.0658 - accuracy: 0.9795\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 997us/step - loss: 0.0748 - accuracy: 0.9766\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 994us/step - loss: 0.0646 - accuracy: 0.9798\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 984us/step - loss: 0.0657 - accuracy: 0.9817\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 976us/step - loss: 0.0630 - accuracy: 0.9803\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 974us/step - loss: 0.0660 - accuracy: 0.9786\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9797\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9809\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9780\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9769\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 997us/step - loss: 0.0634 - accuracy: 0.9798\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9805\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9809\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 970us/step - loss: 0.0638 - accuracy: 0.9803\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 997us/step - loss: 0.0629 - accuracy: 0.9801\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9797\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 956us/step - loss: 0.0602 - accuracy: 0.9818\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9801\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9811\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.9791\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9809\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9812\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 972us/step - loss: 0.0588 - accuracy: 0.9809\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 982us/step - loss: 0.0608 - accuracy: 0.9809\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9809\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 971us/step - loss: 0.0637 - accuracy: 0.9814\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9811\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9818\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9812\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9808\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9806\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9791\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9823\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9803\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9821\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9811\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9818\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9837\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9823\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9821\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9812\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9809\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9834\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9817\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9806\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0566 - accuracy: 0.9840\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9835\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0548 - accuracy: 0.9826\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9841\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9805\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9791\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9838\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9815\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9834\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9835\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9834\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 980us/step - loss: 0.0519 - accuracy: 0.9855\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9843\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9834\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9812\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9814\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9812\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9829\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0564 - accuracy: 0.9843\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0579 - accuracy: 0.9801\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9849\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9849\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0538 - accuracy: 0.9834\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0536 - accuracy: 0.9832\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9834\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9848\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9848\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9840\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9838\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9849\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0567 - accuracy: 0.9825\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9811\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9825\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9840\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9848\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 986us/step - loss: 0.0588 - accuracy: 0.9812\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9840\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9841\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9821\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9825\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9845\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9837\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0534 - accuracy: 0.9834\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9818\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9818\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9854\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9860\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0503 - accuracy: 0.9858\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 990us/step - loss: 0.0542 - accuracy: 0.9823\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0553 - accuracy: 0.9821\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9854\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9840\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0518 - accuracy: 0.9825\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9823\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9834\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9838\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9841\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9854\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9820\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9852\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 0.9851\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9843\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0496 - accuracy: 0.9851\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9835\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0483 - accuracy: 0.9860\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9838\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0505 - accuracy: 0.9843\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0530 - accuracy: 0.9843\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0521 - accuracy: 0.9837\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0514 - accuracy: 0.9835\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0533 - accuracy: 0.9831\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0491 - accuracy: 0.9854\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0560 - accuracy: 0.9832\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0510 - accuracy: 0.9843\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0469 - accuracy: 0.9857\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9848\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0465 - accuracy: 0.9863\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0481 - accuracy: 0.9841\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9848\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0515 - accuracy: 0.9823\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 979us/step - loss: 0.0502 - accuracy: 0.9845\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9851\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0492 - accuracy: 0.9845\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0459 - accuracy: 0.9875\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9848\n",
            "204/204 [==============================] - 0s 863us/step - loss: 0.0470 - accuracy: 0.9861\n",
            "\n",
            " Accuracy : 0.9861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1frS6_o-5Rke",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6160794b-5c7d-45dd-9e38-aedfad59eaba"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "#seed값 설정\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "#데이터 불러오기 및 피처(속성)/타겟(클래스) 분리\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac = 1)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "# 모델설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델컴파일\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "# 모델 저장 폴더 설정\n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "# 모델 저장 조건 설정\n",
        "modelpath = './model/{epoch:02d} - {val_loss:.4f}.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "\n",
        "# 모델 실행 및 저장\n",
        "model.fit(X, Y, validation_split=0.2, epochs = 200, batch_size = 200, verbose = 0, callbacks=[checkpointer])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.38706, saving model to ./model/01 - 0.3871.hdf5\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.38706 to 0.28561, saving model to ./model/02 - 0.2856.hdf5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.28561 to 0.24630, saving model to ./model/03 - 0.2463.hdf5\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.24630 to 0.23385, saving model to ./model/04 - 0.2338.hdf5\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.23385 to 0.22901, saving model to ./model/05 - 0.2290.hdf5\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.22901 to 0.22135, saving model to ./model/06 - 0.2214.hdf5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.22135 to 0.21528, saving model to ./model/07 - 0.2153.hdf5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.21528 to 0.20855, saving model to ./model/08 - 0.2086.hdf5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.20855 to 0.19963, saving model to ./model/09 - 0.1996.hdf5\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.19963 to 0.19792, saving model to ./model/10 - 0.1979.hdf5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.19792 to 0.18896, saving model to ./model/11 - 0.1890.hdf5\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.18896 to 0.18506, saving model to ./model/12 - 0.1851.hdf5\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.18506 to 0.18127, saving model to ./model/13 - 0.1813.hdf5\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.18127 to 0.17801, saving model to ./model/14 - 0.1780.hdf5\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.17801 to 0.17631, saving model to ./model/15 - 0.1763.hdf5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.17631 to 0.17160, saving model to ./model/16 - 0.1716.hdf5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.17160 to 0.16956, saving model to ./model/17 - 0.1696.hdf5\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.16956 to 0.16680, saving model to ./model/18 - 0.1668.hdf5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.16680 to 0.16383, saving model to ./model/19 - 0.1638.hdf5\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.16383 to 0.16313, saving model to ./model/20 - 0.1631.hdf5\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.16313 to 0.15975, saving model to ./model/21 - 0.1597.hdf5\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.15975 to 0.15546, saving model to ./model/22 - 0.1555.hdf5\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.15546 to 0.15327, saving model to ./model/23 - 0.1533.hdf5\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.15327 to 0.15144, saving model to ./model/24 - 0.1514.hdf5\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.15144 to 0.14768, saving model to ./model/25 - 0.1477.hdf5\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.14768 to 0.14690, saving model to ./model/26 - 0.1469.hdf5\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.14690 to 0.14301, saving model to ./model/27 - 0.1430.hdf5\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.14301\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.14301\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.14301 to 0.13609, saving model to ./model/30 - 0.1361.hdf5\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.13609 to 0.13560, saving model to ./model/31 - 0.1356.hdf5\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.13560\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.13560 to 0.13057, saving model to ./model/33 - 0.1306.hdf5\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.13057\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.13057\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.13057 to 0.12732, saving model to ./model/36 - 0.1273.hdf5\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.12732 to 0.12037, saving model to ./model/37 - 0.1204.hdf5\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.12037 to 0.11840, saving model to ./model/38 - 0.1184.hdf5\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.11840 to 0.11774, saving model to ./model/39 - 0.1177.hdf5\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.11774 to 0.10965, saving model to ./model/40 - 0.1096.hdf5\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.10965 to 0.10570, saving model to ./model/41 - 0.1057.hdf5\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.10570 to 0.10178, saving model to ./model/42 - 0.1018.hdf5\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.10178 to 0.10058, saving model to ./model/43 - 0.1006.hdf5\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.10058 to 0.09882, saving model to ./model/44 - 0.0988.hdf5\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.09882\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.09882\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.09882 to 0.09374, saving model to ./model/47 - 0.0937.hdf5\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.09374 to 0.09174, saving model to ./model/48 - 0.0917.hdf5\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.09174\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.09174 to 0.08911, saving model to ./model/50 - 0.0891.hdf5\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.08911\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.08911 to 0.08583, saving model to ./model/52 - 0.0858.hdf5\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.08583\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.08583\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.08583 to 0.08370, saving model to ./model/55 - 0.0837.hdf5\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.08370\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.08370\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.08370\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.08370\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.08370 to 0.08106, saving model to ./model/60 - 0.0811.hdf5\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.08106 to 0.07695, saving model to ./model/61 - 0.0769.hdf5\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.07695 to 0.07622, saving model to ./model/62 - 0.0762.hdf5\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.07622\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.07622\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.07622 to 0.07263, saving model to ./model/65 - 0.0726.hdf5\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.07263\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.07263 to 0.06797, saving model to ./model/72 - 0.0680.hdf5\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.06797\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.06797 to 0.06785, saving model to ./model/74 - 0.0679.hdf5\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.06785 to 0.06610, saving model to ./model/75 - 0.0661.hdf5\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.06610 to 0.06600, saving model to ./model/76 - 0.0660.hdf5\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.06600 to 0.06486, saving model to ./model/77 - 0.0649.hdf5\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.06486 to 0.06457, saving model to ./model/78 - 0.0646.hdf5\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.06457\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.06457\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.06457\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.06457 to 0.06328, saving model to ./model/82 - 0.0633.hdf5\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.06328 to 0.06278, saving model to ./model/83 - 0.0628.hdf5\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.06278\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.06278\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.06278\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.06278 to 0.06095, saving model to ./model/87 - 0.0610.hdf5\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.06095 to 0.05991, saving model to ./model/88 - 0.0599.hdf5\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.05991\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.05991\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.05991 to 0.05898, saving model to ./model/91 - 0.0590.hdf5\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.05898 to 0.05862, saving model to ./model/92 - 0.0586.hdf5\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.05862\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.05862\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.05862\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.05862\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.05862\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.05862 to 0.05744, saving model to ./model/98 - 0.0574.hdf5\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.05744 to 0.05690, saving model to ./model/99 - 0.0569.hdf5\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.05690\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.05690\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.05690\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.05690 to 0.05679, saving model to ./model/103 - 0.0568.hdf5\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.05679\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.05679\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.05679\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.05679 to 0.05649, saving model to ./model/107 - 0.0565.hdf5\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.05649\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.05649 to 0.05556, saving model to ./model/109 - 0.0556.hdf5\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.05556\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.05556\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.05556\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.05556\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.05556 to 0.05504, saving model to ./model/114 - 0.0550.hdf5\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.05504\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.05504\n",
            "\n",
            "Epoch 00117: val_loss improved from 0.05504 to 0.05482, saving model to ./model/117 - 0.0548.hdf5\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.05482\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.05482 to 0.05378, saving model to ./model/119 - 0.0538.hdf5\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.05378\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.05378\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.05378 to 0.05376, saving model to ./model/122 - 0.0538.hdf5\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.05376\n",
            "\n",
            "Epoch 00124: val_loss improved from 0.05376 to 0.05323, saving model to ./model/124 - 0.0532.hdf5\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.05323 to 0.05323, saving model to ./model/139 - 0.0532.hdf5\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.05323\n",
            "\n",
            "Epoch 00146: val_loss improved from 0.05323 to 0.05302, saving model to ./model/146 - 0.0530.hdf5\n",
            "\n",
            "Epoch 00147: val_loss improved from 0.05302 to 0.05150, saving model to ./model/147 - 0.0515.hdf5\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.05150\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.05150 to 0.04976, saving model to ./model/173 - 0.0498.hdf5\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.04976\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.04976\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.04976\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.04976\n",
            "\n",
            "Epoch 00178: val_loss improved from 0.04976 to 0.04928, saving model to ./model/178 - 0.0493.hdf5\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.04928\n",
            "\n",
            "Epoch 00190: val_loss improved from 0.04928 to 0.04904, saving model to ./model/190 - 0.0490.hdf5\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.04904\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.04904 to 0.04893, saving model to ./model/192 - 0.0489.hdf5\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.04893\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.04893\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f976ac9ec88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQRUhTt5cRlM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "#seed값 설정\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "#데이터 불러오기 및 피처(속성)/타겟(클래스) 분리\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac = 0.15)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "# 모델설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델컴파일\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "# 모델 저장 폴더 설정\n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "# 모델 저장 조건 설정\n",
        "modelpath = './model/{epoch:02d} - {val_loss:.4f}.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "\n",
        "# 모델 실행 및 저장\n",
        "history = model.fit(X, Y, validation_split=0.33, epochs = 3500, batch_size = 500)\n",
        "\n",
        "# y_vloss에 테스트셋으로 실험 결과의 오차값을 저장\n",
        "y_vloss = history.history['val_loss']\n",
        "\n",
        "# y_acc에 학습셋으로 측정한 정학도의 값을 저장\n",
        "y_acc = history.history['acc']\n",
        "\n",
        "# x값을 지정하고 정확도를 파란색으로 오차를 빨간색으로 표시\n",
        "x_len = np.arange(len(y_acc))\n",
        "plt.plot(x_len, y_vloss, \"o\", c = \"red\", markersize = 3)\n",
        "plt.plot(x_len, y_acc, \"o\", c = \"blue\", markersize = 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bcu8TXVaj0X1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "outputId": "c8008b22-e5ad-4b16-cec9-fbccf9d65d92"
      },
      "source": [
        "# y_vloss에 테스트셋으로 실험 결과의 오차값을 저장\n",
        "y_vloss = history.history['val_loss']\n",
        "\n",
        "# y_acc에 학습셋으로 측정한 정학도의 값을 저장\n",
        "y_acc = history.history['accuracy']\n",
        "\n",
        "# x값을 지정하고 정확도를 파란색으로 오차를 빨간색으로 표시\n",
        "x_len = np.arange(len(y_acc))\n",
        "plt.figure(figsize=(18,10))\n",
        "plt.plot(x_len, y_vloss, \"o\", c = \"red\", markersize = 3)\n",
        "plt.plot(x_len, y_acc, \"o\", c = \"blue\", markersize = 3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f976ef98c50>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeV4ovZPkEI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7edc5ca4-7e21-4ea0-9a96-33222a874fea"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "#seed값 설정\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "#데이터 불러오기 및 피처(속성)/타겟(클래스) 분리\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac = 0.15)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "# 모델설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델컴파일\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "# 학습자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.2, epochs=2000, batch_size=500, callbacks=[early_stopping_callback])\n",
        "\n",
        "print('\\n Accuracy: %.4f' % (model.evaluate(X, Y)[1]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2000\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 2.0342 - accuracy: 0.7256 - val_loss: 1.2069 - val_accuracy: 0.7744\n",
            "Epoch 2/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5270 - accuracy: 0.7256 - val_loss: 0.8604 - val_accuracy: 0.7744\n",
            "Epoch 3/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0603 - accuracy: 0.7256 - val_loss: 0.5777 - val_accuracy: 0.7744\n",
            "Epoch 4/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7117 - accuracy: 0.7231 - val_loss: 0.7209 - val_accuracy: 0.6256\n",
            "Epoch 5/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7558 - accuracy: 0.5756 - val_loss: 0.8500 - val_accuracy: 0.4615\n",
            "Epoch 6/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7474 - accuracy: 0.5603 - val_loss: 0.5361 - val_accuracy: 0.7333\n",
            "Epoch 7/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5617 - accuracy: 0.7128 - val_loss: 0.4439 - val_accuracy: 0.7692\n",
            "Epoch 8/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5535 - accuracy: 0.7231 - val_loss: 0.4426 - val_accuracy: 0.7795\n",
            "Epoch 9/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5795 - accuracy: 0.7244 - val_loss: 0.4358 - val_accuracy: 0.7795\n",
            "Epoch 10/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5744 - accuracy: 0.7244 - val_loss: 0.4106 - val_accuracy: 0.7795\n",
            "Epoch 11/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5373 - accuracy: 0.7295 - val_loss: 0.3788 - val_accuracy: 0.7744\n",
            "Epoch 12/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4828 - accuracy: 0.7385 - val_loss: 0.3649 - val_accuracy: 0.7846\n",
            "Epoch 13/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4433 - accuracy: 0.7615 - val_loss: 0.3944 - val_accuracy: 0.7795\n",
            "Epoch 14/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4402 - accuracy: 0.7833 - val_loss: 0.4339 - val_accuracy: 0.7744\n",
            "Epoch 15/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4451 - accuracy: 0.8141 - val_loss: 0.4091 - val_accuracy: 0.7949\n",
            "Epoch 16/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4216 - accuracy: 0.8256 - val_loss: 0.3522 - val_accuracy: 0.8308\n",
            "Epoch 17/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3971 - accuracy: 0.8359 - val_loss: 0.3153 - val_accuracy: 0.8410\n",
            "Epoch 18/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3859 - accuracy: 0.8423 - val_loss: 0.2991 - val_accuracy: 0.8718\n",
            "Epoch 19/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3836 - accuracy: 0.8487 - val_loss: 0.2899 - val_accuracy: 0.8769\n",
            "Epoch 20/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3776 - accuracy: 0.8526 - val_loss: 0.2832 - val_accuracy: 0.8821\n",
            "Epoch 21/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3660 - accuracy: 0.8641 - val_loss: 0.2817 - val_accuracy: 0.8974\n",
            "Epoch 22/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3534 - accuracy: 0.8744 - val_loss: 0.2866 - val_accuracy: 0.9026\n",
            "Epoch 23/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3459 - accuracy: 0.8833 - val_loss: 0.2946 - val_accuracy: 0.9179\n",
            "Epoch 24/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3429 - accuracy: 0.8962 - val_loss: 0.2955 - val_accuracy: 0.9179\n",
            "Epoch 25/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3393 - accuracy: 0.8962 - val_loss: 0.2852 - val_accuracy: 0.9179\n",
            "Epoch 26/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3332 - accuracy: 0.8974 - val_loss: 0.2693 - val_accuracy: 0.9179\n",
            "Epoch 27/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3261 - accuracy: 0.9000 - val_loss: 0.2567 - val_accuracy: 0.9179\n",
            "Epoch 28/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3209 - accuracy: 0.9026 - val_loss: 0.2482 - val_accuracy: 0.9179\n",
            "Epoch 29/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3172 - accuracy: 0.9038 - val_loss: 0.2430 - val_accuracy: 0.9231\n",
            "Epoch 30/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3138 - accuracy: 0.9051 - val_loss: 0.2398 - val_accuracy: 0.9231\n",
            "Epoch 31/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3101 - accuracy: 0.9038 - val_loss: 0.2385 - val_accuracy: 0.9231\n",
            "Epoch 32/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3062 - accuracy: 0.9051 - val_loss: 0.2387 - val_accuracy: 0.9231\n",
            "Epoch 33/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3034 - accuracy: 0.9051 - val_loss: 0.2393 - val_accuracy: 0.9282\n",
            "Epoch 34/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3004 - accuracy: 0.9090 - val_loss: 0.2371 - val_accuracy: 0.9333\n",
            "Epoch 35/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2978 - accuracy: 0.9077 - val_loss: 0.2329 - val_accuracy: 0.9385\n",
            "Epoch 36/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2951 - accuracy: 0.9077 - val_loss: 0.2279 - val_accuracy: 0.9333\n",
            "Epoch 37/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2926 - accuracy: 0.9064 - val_loss: 0.2227 - val_accuracy: 0.9333\n",
            "Epoch 38/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2904 - accuracy: 0.9064 - val_loss: 0.2185 - val_accuracy: 0.9282\n",
            "Epoch 39/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2888 - accuracy: 0.9064 - val_loss: 0.2162 - val_accuracy: 0.9333\n",
            "Epoch 40/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2866 - accuracy: 0.9064 - val_loss: 0.2140 - val_accuracy: 0.9333\n",
            "Epoch 41/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2849 - accuracy: 0.9064 - val_loss: 0.2133 - val_accuracy: 0.9385\n",
            "Epoch 42/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2825 - accuracy: 0.9103 - val_loss: 0.2126 - val_accuracy: 0.9385\n",
            "Epoch 43/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2806 - accuracy: 0.9115 - val_loss: 0.2117 - val_accuracy: 0.9385\n",
            "Epoch 44/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2788 - accuracy: 0.9115 - val_loss: 0.2100 - val_accuracy: 0.9385\n",
            "Epoch 45/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2772 - accuracy: 0.9115 - val_loss: 0.2081 - val_accuracy: 0.9385\n",
            "Epoch 46/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2760 - accuracy: 0.9115 - val_loss: 0.2066 - val_accuracy: 0.9385\n",
            "Epoch 47/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2739 - accuracy: 0.9115 - val_loss: 0.2026 - val_accuracy: 0.9385\n",
            "Epoch 48/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2723 - accuracy: 0.9115 - val_loss: 0.1981 - val_accuracy: 0.9333\n",
            "Epoch 49/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2709 - accuracy: 0.9103 - val_loss: 0.1945 - val_accuracy: 0.9333\n",
            "Epoch 50/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2694 - accuracy: 0.9103 - val_loss: 0.1923 - val_accuracy: 0.9282\n",
            "Epoch 51/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2679 - accuracy: 0.9128 - val_loss: 0.1916 - val_accuracy: 0.9231\n",
            "Epoch 52/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2654 - accuracy: 0.9167 - val_loss: 0.1921 - val_accuracy: 0.9231\n",
            "Epoch 53/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2637 - accuracy: 0.9154 - val_loss: 0.1942 - val_accuracy: 0.9179\n",
            "Epoch 54/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2618 - accuracy: 0.9154 - val_loss: 0.1956 - val_accuracy: 0.9179\n",
            "Epoch 55/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.9167 - val_loss: 0.1970 - val_accuracy: 0.9179\n",
            "Epoch 56/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2599 - accuracy: 0.9154 - val_loss: 0.1968 - val_accuracy: 0.9179\n",
            "Epoch 57/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2593 - accuracy: 0.9154 - val_loss: 0.1954 - val_accuracy: 0.9179\n",
            "Epoch 58/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2582 - accuracy: 0.9154 - val_loss: 0.1922 - val_accuracy: 0.9179\n",
            "Epoch 59/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2577 - accuracy: 0.9141 - val_loss: 0.1901 - val_accuracy: 0.9179\n",
            "Epoch 60/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2568 - accuracy: 0.9141 - val_loss: 0.1905 - val_accuracy: 0.9179\n",
            "Epoch 61/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2557 - accuracy: 0.9141 - val_loss: 0.1906 - val_accuracy: 0.9179\n",
            "Epoch 62/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2551 - accuracy: 0.9141 - val_loss: 0.1917 - val_accuracy: 0.9179\n",
            "Epoch 63/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2548 - accuracy: 0.9141 - val_loss: 0.1931 - val_accuracy: 0.9179\n",
            "Epoch 64/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2538 - accuracy: 0.9141 - val_loss: 0.1920 - val_accuracy: 0.9179\n",
            "Epoch 65/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2530 - accuracy: 0.9141 - val_loss: 0.1918 - val_accuracy: 0.9179\n",
            "Epoch 66/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2523 - accuracy: 0.9154 - val_loss: 0.1905 - val_accuracy: 0.9179\n",
            "Epoch 67/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2517 - accuracy: 0.9154 - val_loss: 0.1904 - val_accuracy: 0.9179\n",
            "Epoch 68/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2508 - accuracy: 0.9167 - val_loss: 0.1892 - val_accuracy: 0.9179\n",
            "Epoch 69/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2503 - accuracy: 0.9154 - val_loss: 0.1881 - val_accuracy: 0.9179\n",
            "Epoch 70/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2496 - accuracy: 0.9154 - val_loss: 0.1884 - val_accuracy: 0.9179\n",
            "Epoch 71/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2491 - accuracy: 0.9154 - val_loss: 0.1885 - val_accuracy: 0.9179\n",
            "Epoch 72/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2484 - accuracy: 0.9167 - val_loss: 0.1885 - val_accuracy: 0.9179\n",
            "Epoch 73/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2478 - accuracy: 0.9167 - val_loss: 0.1880 - val_accuracy: 0.9179\n",
            "Epoch 74/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2472 - accuracy: 0.9167 - val_loss: 0.1874 - val_accuracy: 0.9179\n",
            "Epoch 75/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2465 - accuracy: 0.9167 - val_loss: 0.1865 - val_accuracy: 0.9179\n",
            "Epoch 76/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2460 - accuracy: 0.9167 - val_loss: 0.1860 - val_accuracy: 0.9179\n",
            "Epoch 77/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2454 - accuracy: 0.9179 - val_loss: 0.1851 - val_accuracy: 0.9179\n",
            "Epoch 78/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2448 - accuracy: 0.9192 - val_loss: 0.1851 - val_accuracy: 0.9179\n",
            "Epoch 79/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2442 - accuracy: 0.9179 - val_loss: 0.1850 - val_accuracy: 0.9179\n",
            "Epoch 80/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2437 - accuracy: 0.9179 - val_loss: 0.1850 - val_accuracy: 0.9179\n",
            "Epoch 81/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2430 - accuracy: 0.9167 - val_loss: 0.1862 - val_accuracy: 0.9179\n",
            "Epoch 82/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2422 - accuracy: 0.9179 - val_loss: 0.1869 - val_accuracy: 0.9179\n",
            "Epoch 83/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2418 - accuracy: 0.9192 - val_loss: 0.1873 - val_accuracy: 0.9179\n",
            "Epoch 84/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2414 - accuracy: 0.9192 - val_loss: 0.1864 - val_accuracy: 0.9179\n",
            "Epoch 85/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2408 - accuracy: 0.9205 - val_loss: 0.1852 - val_accuracy: 0.9179\n",
            "Epoch 86/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2399 - accuracy: 0.9192 - val_loss: 0.1848 - val_accuracy: 0.9179\n",
            "Epoch 87/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2393 - accuracy: 0.9192 - val_loss: 0.1837 - val_accuracy: 0.9179\n",
            "Epoch 88/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2390 - accuracy: 0.9218 - val_loss: 0.1823 - val_accuracy: 0.9179\n",
            "Epoch 89/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2383 - accuracy: 0.9218 - val_loss: 0.1828 - val_accuracy: 0.9179\n",
            "Epoch 90/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2376 - accuracy: 0.9231 - val_loss: 0.1827 - val_accuracy: 0.9179\n",
            "Epoch 91/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.9231 - val_loss: 0.1827 - val_accuracy: 0.9179\n",
            "Epoch 92/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2366 - accuracy: 0.9231 - val_loss: 0.1837 - val_accuracy: 0.9179\n",
            "Epoch 93/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2361 - accuracy: 0.9218 - val_loss: 0.1833 - val_accuracy: 0.9179\n",
            "Epoch 94/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2356 - accuracy: 0.9218 - val_loss: 0.1830 - val_accuracy: 0.9179\n",
            "Epoch 95/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2351 - accuracy: 0.9218 - val_loss: 0.1830 - val_accuracy: 0.9179\n",
            "Epoch 96/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2345 - accuracy: 0.9231 - val_loss: 0.1819 - val_accuracy: 0.9179\n",
            "Epoch 97/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2340 - accuracy: 0.9231 - val_loss: 0.1807 - val_accuracy: 0.9179\n",
            "Epoch 98/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2335 - accuracy: 0.9231 - val_loss: 0.1803 - val_accuracy: 0.9179\n",
            "Epoch 99/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2331 - accuracy: 0.9231 - val_loss: 0.1799 - val_accuracy: 0.9179\n",
            "Epoch 100/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2326 - accuracy: 0.9231 - val_loss: 0.1794 - val_accuracy: 0.9179\n",
            "Epoch 101/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2323 - accuracy: 0.9231 - val_loss: 0.1786 - val_accuracy: 0.9179\n",
            "Epoch 102/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2317 - accuracy: 0.9218 - val_loss: 0.1768 - val_accuracy: 0.9179\n",
            "Epoch 103/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2313 - accuracy: 0.9218 - val_loss: 0.1768 - val_accuracy: 0.9179\n",
            "Epoch 104/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2309 - accuracy: 0.9231 - val_loss: 0.1774 - val_accuracy: 0.9179\n",
            "Epoch 105/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2302 - accuracy: 0.9231 - val_loss: 0.1775 - val_accuracy: 0.9179\n",
            "Epoch 106/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2299 - accuracy: 0.9231 - val_loss: 0.1778 - val_accuracy: 0.9179\n",
            "Epoch 107/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2295 - accuracy: 0.9231 - val_loss: 0.1772 - val_accuracy: 0.9179\n",
            "Epoch 108/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2291 - accuracy: 0.9231 - val_loss: 0.1785 - val_accuracy: 0.9179\n",
            "Epoch 109/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2284 - accuracy: 0.9231 - val_loss: 0.1793 - val_accuracy: 0.9179\n",
            "Epoch 110/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2279 - accuracy: 0.9231 - val_loss: 0.1795 - val_accuracy: 0.9179\n",
            "Epoch 111/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2276 - accuracy: 0.9231 - val_loss: 0.1787 - val_accuracy: 0.9179\n",
            "Epoch 112/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2271 - accuracy: 0.9231 - val_loss: 0.1785 - val_accuracy: 0.9179\n",
            "Epoch 113/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2265 - accuracy: 0.9231 - val_loss: 0.1767 - val_accuracy: 0.9179\n",
            "Epoch 114/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2258 - accuracy: 0.9231 - val_loss: 0.1746 - val_accuracy: 0.9179\n",
            "Epoch 115/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2254 - accuracy: 0.9218 - val_loss: 0.1729 - val_accuracy: 0.9179\n",
            "Epoch 116/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2255 - accuracy: 0.9218 - val_loss: 0.1726 - val_accuracy: 0.9179\n",
            "Epoch 117/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2252 - accuracy: 0.9218 - val_loss: 0.1743 - val_accuracy: 0.9179\n",
            "Epoch 118/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2239 - accuracy: 0.9231 - val_loss: 0.1758 - val_accuracy: 0.9179\n",
            "Epoch 119/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2234 - accuracy: 0.9231 - val_loss: 0.1779 - val_accuracy: 0.9179\n",
            "Epoch 120/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2232 - accuracy: 0.9231 - val_loss: 0.1784 - val_accuracy: 0.9179\n",
            "Epoch 121/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2228 - accuracy: 0.9231 - val_loss: 0.1764 - val_accuracy: 0.9179\n",
            "Epoch 122/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2223 - accuracy: 0.9231 - val_loss: 0.1742 - val_accuracy: 0.9179\n",
            "Epoch 123/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2216 - accuracy: 0.9231 - val_loss: 0.1733 - val_accuracy: 0.9231\n",
            "Epoch 124/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2214 - accuracy: 0.9244 - val_loss: 0.1726 - val_accuracy: 0.9231\n",
            "Epoch 125/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2209 - accuracy: 0.9256 - val_loss: 0.1743 - val_accuracy: 0.9179\n",
            "Epoch 126/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2201 - accuracy: 0.9231 - val_loss: 0.1763 - val_accuracy: 0.9179\n",
            "Epoch 127/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2198 - accuracy: 0.9231 - val_loss: 0.1777 - val_accuracy: 0.9179\n",
            "Epoch 128/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2195 - accuracy: 0.9231 - val_loss: 0.1762 - val_accuracy: 0.9179\n",
            "Epoch 129/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2190 - accuracy: 0.9231 - val_loss: 0.1735 - val_accuracy: 0.9179\n",
            "Epoch 130/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2187 - accuracy: 0.9244 - val_loss: 0.1700 - val_accuracy: 0.9231\n",
            "Epoch 131/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2181 - accuracy: 0.9256 - val_loss: 0.1696 - val_accuracy: 0.9231\n",
            "Epoch 132/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2176 - accuracy: 0.9256 - val_loss: 0.1695 - val_accuracy: 0.9231\n",
            "Epoch 133/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2171 - accuracy: 0.9256 - val_loss: 0.1691 - val_accuracy: 0.9282\n",
            "Epoch 134/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2168 - accuracy: 0.9256 - val_loss: 0.1709 - val_accuracy: 0.9231\n",
            "Epoch 135/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2159 - accuracy: 0.9256 - val_loss: 0.1715 - val_accuracy: 0.9282\n",
            "Epoch 136/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2156 - accuracy: 0.9256 - val_loss: 0.1719 - val_accuracy: 0.9231\n",
            "Epoch 137/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2151 - accuracy: 0.9244 - val_loss: 0.1713 - val_accuracy: 0.9231\n",
            "Epoch 138/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2145 - accuracy: 0.9244 - val_loss: 0.1706 - val_accuracy: 0.9282\n",
            "Epoch 139/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2140 - accuracy: 0.9256 - val_loss: 0.1696 - val_accuracy: 0.9282\n",
            "Epoch 140/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2137 - accuracy: 0.9244 - val_loss: 0.1684 - val_accuracy: 0.9282\n",
            "Epoch 141/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2131 - accuracy: 0.9256 - val_loss: 0.1662 - val_accuracy: 0.9282\n",
            "Epoch 142/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2129 - accuracy: 0.9256 - val_loss: 0.1657 - val_accuracy: 0.9282\n",
            "Epoch 143/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2123 - accuracy: 0.9269 - val_loss: 0.1651 - val_accuracy: 0.9282\n",
            "Epoch 144/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2120 - accuracy: 0.9282 - val_loss: 0.1658 - val_accuracy: 0.9282\n",
            "Epoch 145/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2114 - accuracy: 0.9256 - val_loss: 0.1688 - val_accuracy: 0.9282\n",
            "Epoch 146/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2106 - accuracy: 0.9244 - val_loss: 0.1715 - val_accuracy: 0.9333\n",
            "Epoch 147/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2107 - accuracy: 0.9244 - val_loss: 0.1725 - val_accuracy: 0.9333\n",
            "Epoch 148/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2109 - accuracy: 0.9244 - val_loss: 0.1716 - val_accuracy: 0.9333\n",
            "Epoch 149/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2102 - accuracy: 0.9269 - val_loss: 0.1669 - val_accuracy: 0.9282\n",
            "Epoch 150/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2097 - accuracy: 0.9282 - val_loss: 0.1646 - val_accuracy: 0.9282\n",
            "Epoch 151/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2088 - accuracy: 0.9269 - val_loss: 0.1662 - val_accuracy: 0.9333\n",
            "Epoch 152/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2083 - accuracy: 0.9269 - val_loss: 0.1654 - val_accuracy: 0.9333\n",
            "Epoch 153/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2078 - accuracy: 0.9256 - val_loss: 0.1641 - val_accuracy: 0.9282\n",
            "Epoch 154/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2072 - accuracy: 0.9269 - val_loss: 0.1646 - val_accuracy: 0.9333\n",
            "Epoch 155/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2065 - accuracy: 0.9282 - val_loss: 0.1649 - val_accuracy: 0.9333\n",
            "Epoch 156/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2061 - accuracy: 0.9282 - val_loss: 0.1655 - val_accuracy: 0.9333\n",
            "Epoch 157/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2057 - accuracy: 0.9282 - val_loss: 0.1642 - val_accuracy: 0.9333\n",
            "Epoch 158/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2062 - accuracy: 0.9295 - val_loss: 0.1624 - val_accuracy: 0.9333\n",
            "Epoch 159/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2051 - accuracy: 0.9269 - val_loss: 0.1643 - val_accuracy: 0.9333\n",
            "Epoch 160/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2043 - accuracy: 0.9295 - val_loss: 0.1639 - val_accuracy: 0.9385\n",
            "Epoch 161/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2037 - accuracy: 0.9295 - val_loss: 0.1635 - val_accuracy: 0.9385\n",
            "Epoch 162/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2034 - accuracy: 0.9308 - val_loss: 0.1618 - val_accuracy: 0.9385\n",
            "Epoch 163/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2040 - accuracy: 0.9282 - val_loss: 0.1590 - val_accuracy: 0.9385\n",
            "Epoch 164/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2025 - accuracy: 0.9295 - val_loss: 0.1609 - val_accuracy: 0.9385\n",
            "Epoch 165/2000\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2024 - accuracy: 0.9308 - val_loss: 0.1635 - val_accuracy: 0.9385\n",
            "Epoch 166/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2017 - accuracy: 0.9308 - val_loss: 0.1618 - val_accuracy: 0.9385\n",
            "Epoch 167/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2006 - accuracy: 0.9308 - val_loss: 0.1574 - val_accuracy: 0.9385\n",
            "Epoch 168/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2004 - accuracy: 0.9282 - val_loss: 0.1545 - val_accuracy: 0.9333\n",
            "Epoch 169/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2010 - accuracy: 0.9269 - val_loss: 0.1539 - val_accuracy: 0.9333\n",
            "Epoch 170/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2002 - accuracy: 0.9269 - val_loss: 0.1568 - val_accuracy: 0.9385\n",
            "Epoch 171/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1990 - accuracy: 0.9321 - val_loss: 0.1622 - val_accuracy: 0.9436\n",
            "Epoch 172/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1998 - accuracy: 0.9321 - val_loss: 0.1637 - val_accuracy: 0.9436\n",
            "Epoch 173/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1992 - accuracy: 0.9333 - val_loss: 0.1574 - val_accuracy: 0.9436\n",
            "Epoch 174/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1975 - accuracy: 0.9308 - val_loss: 0.1544 - val_accuracy: 0.9385\n",
            "Epoch 175/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1973 - accuracy: 0.9333 - val_loss: 0.1536 - val_accuracy: 0.9385\n",
            "Epoch 176/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1968 - accuracy: 0.9333 - val_loss: 0.1549 - val_accuracy: 0.9385\n",
            "Epoch 177/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1960 - accuracy: 0.9333 - val_loss: 0.1577 - val_accuracy: 0.9436\n",
            "Epoch 178/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1961 - accuracy: 0.9346 - val_loss: 0.1596 - val_accuracy: 0.9436\n",
            "Epoch 179/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1956 - accuracy: 0.9346 - val_loss: 0.1559 - val_accuracy: 0.9436\n",
            "Epoch 180/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1945 - accuracy: 0.9346 - val_loss: 0.1508 - val_accuracy: 0.9385\n",
            "Epoch 181/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1946 - accuracy: 0.9333 - val_loss: 0.1486 - val_accuracy: 0.9436\n",
            "Epoch 182/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1944 - accuracy: 0.9333 - val_loss: 0.1503 - val_accuracy: 0.9436\n",
            "Epoch 183/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1939 - accuracy: 0.9346 - val_loss: 0.1534 - val_accuracy: 0.9436\n",
            "Epoch 184/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1926 - accuracy: 0.9359 - val_loss: 0.1533 - val_accuracy: 0.9436\n",
            "Epoch 185/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1924 - accuracy: 0.9359 - val_loss: 0.1529 - val_accuracy: 0.9436\n",
            "Epoch 186/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1917 - accuracy: 0.9359 - val_loss: 0.1528 - val_accuracy: 0.9436\n",
            "Epoch 187/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1912 - accuracy: 0.9359 - val_loss: 0.1506 - val_accuracy: 0.9436\n",
            "Epoch 188/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1906 - accuracy: 0.9372 - val_loss: 0.1493 - val_accuracy: 0.9436\n",
            "Epoch 189/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1901 - accuracy: 0.9372 - val_loss: 0.1495 - val_accuracy: 0.9436\n",
            "Epoch 190/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1895 - accuracy: 0.9372 - val_loss: 0.1507 - val_accuracy: 0.9487\n",
            "Epoch 191/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1891 - accuracy: 0.9385 - val_loss: 0.1506 - val_accuracy: 0.9487\n",
            "Epoch 192/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1889 - accuracy: 0.9385 - val_loss: 0.1485 - val_accuracy: 0.9487\n",
            "Epoch 193/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1881 - accuracy: 0.9410 - val_loss: 0.1452 - val_accuracy: 0.9538\n",
            "Epoch 194/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1880 - accuracy: 0.9410 - val_loss: 0.1447 - val_accuracy: 0.9538\n",
            "Epoch 195/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1872 - accuracy: 0.9410 - val_loss: 0.1475 - val_accuracy: 0.9487\n",
            "Epoch 196/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1868 - accuracy: 0.9397 - val_loss: 0.1508 - val_accuracy: 0.9487\n",
            "Epoch 197/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1866 - accuracy: 0.9397 - val_loss: 0.1487 - val_accuracy: 0.9538\n",
            "Epoch 198/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1858 - accuracy: 0.9397 - val_loss: 0.1437 - val_accuracy: 0.9538\n",
            "Epoch 199/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1864 - accuracy: 0.9372 - val_loss: 0.1420 - val_accuracy: 0.9538\n",
            "Epoch 200/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1846 - accuracy: 0.9385 - val_loss: 0.1466 - val_accuracy: 0.9538\n",
            "Epoch 201/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1843 - accuracy: 0.9410 - val_loss: 0.1513 - val_accuracy: 0.9487\n",
            "Epoch 202/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1848 - accuracy: 0.9423 - val_loss: 0.1492 - val_accuracy: 0.9487\n",
            "Epoch 203/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1839 - accuracy: 0.9423 - val_loss: 0.1431 - val_accuracy: 0.9538\n",
            "Epoch 204/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1827 - accuracy: 0.9423 - val_loss: 0.1403 - val_accuracy: 0.9538\n",
            "Epoch 205/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1826 - accuracy: 0.9423 - val_loss: 0.1400 - val_accuracy: 0.9538\n",
            "Epoch 206/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1818 - accuracy: 0.9436 - val_loss: 0.1424 - val_accuracy: 0.9538\n",
            "Epoch 207/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1810 - accuracy: 0.9423 - val_loss: 0.1467 - val_accuracy: 0.9538\n",
            "Epoch 208/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1818 - accuracy: 0.9436 - val_loss: 0.1467 - val_accuracy: 0.9538\n",
            "Epoch 209/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1810 - accuracy: 0.9449 - val_loss: 0.1398 - val_accuracy: 0.9538\n",
            "Epoch 210/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1798 - accuracy: 0.9436 - val_loss: 0.1369 - val_accuracy: 0.9538\n",
            "Epoch 211/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1797 - accuracy: 0.9436 - val_loss: 0.1373 - val_accuracy: 0.9538\n",
            "Epoch 212/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1793 - accuracy: 0.9462 - val_loss: 0.1406 - val_accuracy: 0.9538\n",
            "Epoch 213/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1787 - accuracy: 0.9449 - val_loss: 0.1399 - val_accuracy: 0.9538\n",
            "Epoch 214/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1777 - accuracy: 0.9487 - val_loss: 0.1358 - val_accuracy: 0.9538\n",
            "Epoch 215/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1775 - accuracy: 0.9462 - val_loss: 0.1356 - val_accuracy: 0.9538\n",
            "Epoch 216/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1767 - accuracy: 0.9487 - val_loss: 0.1384 - val_accuracy: 0.9538\n",
            "Epoch 217/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1760 - accuracy: 0.9487 - val_loss: 0.1406 - val_accuracy: 0.9538\n",
            "Epoch 218/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1755 - accuracy: 0.9474 - val_loss: 0.1379 - val_accuracy: 0.9538\n",
            "Epoch 219/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1748 - accuracy: 0.9487 - val_loss: 0.1346 - val_accuracy: 0.9538\n",
            "Epoch 220/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1741 - accuracy: 0.9500 - val_loss: 0.1348 - val_accuracy: 0.9538\n",
            "Epoch 221/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1736 - accuracy: 0.9487 - val_loss: 0.1358 - val_accuracy: 0.9538\n",
            "Epoch 222/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1730 - accuracy: 0.9487 - val_loss: 0.1355 - val_accuracy: 0.9538\n",
            "Epoch 223/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1725 - accuracy: 0.9487 - val_loss: 0.1357 - val_accuracy: 0.9538\n",
            "Epoch 224/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1718 - accuracy: 0.9487 - val_loss: 0.1361 - val_accuracy: 0.9538\n",
            "Epoch 225/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1715 - accuracy: 0.9487 - val_loss: 0.1360 - val_accuracy: 0.9538\n",
            "Epoch 226/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1712 - accuracy: 0.9500 - val_loss: 0.1347 - val_accuracy: 0.9538\n",
            "Epoch 227/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1707 - accuracy: 0.9500 - val_loss: 0.1324 - val_accuracy: 0.9538\n",
            "Epoch 228/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1703 - accuracy: 0.9500 - val_loss: 0.1336 - val_accuracy: 0.9590\n",
            "Epoch 229/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1695 - accuracy: 0.9500 - val_loss: 0.1354 - val_accuracy: 0.9590\n",
            "Epoch 230/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1690 - accuracy: 0.9500 - val_loss: 0.1381 - val_accuracy: 0.9590\n",
            "Epoch 231/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1691 - accuracy: 0.9513 - val_loss: 0.1371 - val_accuracy: 0.9590\n",
            "Epoch 232/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1682 - accuracy: 0.9526 - val_loss: 0.1327 - val_accuracy: 0.9590\n",
            "Epoch 233/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1673 - accuracy: 0.9513 - val_loss: 0.1295 - val_accuracy: 0.9590\n",
            "Epoch 234/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1667 - accuracy: 0.9513 - val_loss: 0.1303 - val_accuracy: 0.9590\n",
            "Epoch 235/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1660 - accuracy: 0.9500 - val_loss: 0.1325 - val_accuracy: 0.9590\n",
            "Epoch 236/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1660 - accuracy: 0.9526 - val_loss: 0.1318 - val_accuracy: 0.9590\n",
            "Epoch 237/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1648 - accuracy: 0.9513 - val_loss: 0.1270 - val_accuracy: 0.9590\n",
            "Epoch 238/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1647 - accuracy: 0.9513 - val_loss: 0.1263 - val_accuracy: 0.9590\n",
            "Epoch 239/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1657 - accuracy: 0.9474 - val_loss: 0.1285 - val_accuracy: 0.9590\n",
            "Epoch 240/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1640 - accuracy: 0.9513 - val_loss: 0.1390 - val_accuracy: 0.9692\n",
            "Epoch 241/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1650 - accuracy: 0.9513 - val_loss: 0.1348 - val_accuracy: 0.9692\n",
            "Epoch 242/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1628 - accuracy: 0.9513 - val_loss: 0.1275 - val_accuracy: 0.9590\n",
            "Epoch 243/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1615 - accuracy: 0.9513 - val_loss: 0.1221 - val_accuracy: 0.9590\n",
            "Epoch 244/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1629 - accuracy: 0.9474 - val_loss: 0.1217 - val_accuracy: 0.9590\n",
            "Epoch 245/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1622 - accuracy: 0.9474 - val_loss: 0.1264 - val_accuracy: 0.9590\n",
            "Epoch 246/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1609 - accuracy: 0.9538 - val_loss: 0.1367 - val_accuracy: 0.9692\n",
            "Epoch 247/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1616 - accuracy: 0.9513 - val_loss: 0.1302 - val_accuracy: 0.9692\n",
            "Epoch 248/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1592 - accuracy: 0.9513 - val_loss: 0.1225 - val_accuracy: 0.9590\n",
            "Epoch 249/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1601 - accuracy: 0.9487 - val_loss: 0.1202 - val_accuracy: 0.9641\n",
            "Epoch 250/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1599 - accuracy: 0.9474 - val_loss: 0.1247 - val_accuracy: 0.9641\n",
            "Epoch 251/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1578 - accuracy: 0.9500 - val_loss: 0.1311 - val_accuracy: 0.9692\n",
            "Epoch 252/2000\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1583 - accuracy: 0.9513 - val_loss: 0.1290 - val_accuracy: 0.9692\n",
            "Epoch 253/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1571 - accuracy: 0.9526 - val_loss: 0.1218 - val_accuracy: 0.9590\n",
            "Epoch 254/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1568 - accuracy: 0.9500 - val_loss: 0.1182 - val_accuracy: 0.9641\n",
            "Epoch 255/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1582 - accuracy: 0.9474 - val_loss: 0.1204 - val_accuracy: 0.9590\n",
            "Epoch 256/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1565 - accuracy: 0.9538 - val_loss: 0.1304 - val_accuracy: 0.9692\n",
            "Epoch 257/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1571 - accuracy: 0.9513 - val_loss: 0.1278 - val_accuracy: 0.9692\n",
            "Epoch 258/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1558 - accuracy: 0.9487 - val_loss: 0.1187 - val_accuracy: 0.9590\n",
            "Epoch 259/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1555 - accuracy: 0.9487 - val_loss: 0.1193 - val_accuracy: 0.9641\n",
            "Epoch 260/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1551 - accuracy: 0.9487 - val_loss: 0.1258 - val_accuracy: 0.9692\n",
            "Epoch 261/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1542 - accuracy: 0.9526 - val_loss: 0.1245 - val_accuracy: 0.9692\n",
            "Epoch 262/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1532 - accuracy: 0.9538 - val_loss: 0.1195 - val_accuracy: 0.9641\n",
            "Epoch 263/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1530 - accuracy: 0.9500 - val_loss: 0.1194 - val_accuracy: 0.9641\n",
            "Epoch 264/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1526 - accuracy: 0.9500 - val_loss: 0.1208 - val_accuracy: 0.9641\n",
            "Epoch 265/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9513 - val_loss: 0.1258 - val_accuracy: 0.9692\n",
            "Epoch 266/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1519 - accuracy: 0.9526 - val_loss: 0.1250 - val_accuracy: 0.9692\n",
            "Epoch 267/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1516 - accuracy: 0.9526 - val_loss: 0.1196 - val_accuracy: 0.9692\n",
            "Epoch 268/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1506 - accuracy: 0.9513 - val_loss: 0.1185 - val_accuracy: 0.9692\n",
            "Epoch 269/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1504 - accuracy: 0.9513 - val_loss: 0.1197 - val_accuracy: 0.9692\n",
            "Epoch 270/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1499 - accuracy: 0.9526 - val_loss: 0.1214 - val_accuracy: 0.9692\n",
            "Epoch 271/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1494 - accuracy: 0.9538 - val_loss: 0.1196 - val_accuracy: 0.9692\n",
            "Epoch 272/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.9526 - val_loss: 0.1180 - val_accuracy: 0.9692\n",
            "Epoch 273/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1490 - accuracy: 0.9513 - val_loss: 0.1188 - val_accuracy: 0.9692\n",
            "Epoch 274/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1480 - accuracy: 0.9538 - val_loss: 0.1221 - val_accuracy: 0.9692\n",
            "Epoch 275/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1479 - accuracy: 0.9538 - val_loss: 0.1214 - val_accuracy: 0.9692\n",
            "Epoch 276/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1472 - accuracy: 0.9551 - val_loss: 0.1193 - val_accuracy: 0.9692\n",
            "Epoch 277/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1465 - accuracy: 0.9538 - val_loss: 0.1178 - val_accuracy: 0.9692\n",
            "Epoch 278/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1458 - accuracy: 0.9551 - val_loss: 0.1176 - val_accuracy: 0.9692\n",
            "Epoch 279/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1454 - accuracy: 0.9551 - val_loss: 0.1181 - val_accuracy: 0.9692\n",
            "Epoch 280/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1448 - accuracy: 0.9551 - val_loss: 0.1168 - val_accuracy: 0.9692\n",
            "Epoch 281/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1443 - accuracy: 0.9551 - val_loss: 0.1171 - val_accuracy: 0.9692\n",
            "Epoch 282/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1439 - accuracy: 0.9564 - val_loss: 0.1179 - val_accuracy: 0.9744\n",
            "Epoch 283/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1435 - accuracy: 0.9564 - val_loss: 0.1180 - val_accuracy: 0.9744\n",
            "Epoch 284/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9564 - val_loss: 0.1162 - val_accuracy: 0.9744\n",
            "Epoch 285/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1428 - accuracy: 0.9551 - val_loss: 0.1161 - val_accuracy: 0.9744\n",
            "Epoch 286/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1421 - accuracy: 0.9551 - val_loss: 0.1162 - val_accuracy: 0.9744\n",
            "Epoch 287/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1420 - accuracy: 0.9590 - val_loss: 0.1145 - val_accuracy: 0.9744\n",
            "Epoch 288/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1417 - accuracy: 0.9551 - val_loss: 0.1117 - val_accuracy: 0.9692\n",
            "Epoch 289/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1410 - accuracy: 0.9526 - val_loss: 0.1156 - val_accuracy: 0.9744\n",
            "Epoch 290/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1407 - accuracy: 0.9590 - val_loss: 0.1162 - val_accuracy: 0.9744\n",
            "Epoch 291/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1400 - accuracy: 0.9577 - val_loss: 0.1108 - val_accuracy: 0.9692\n",
            "Epoch 292/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1401 - accuracy: 0.9526 - val_loss: 0.1096 - val_accuracy: 0.9692\n",
            "Epoch 293/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1401 - accuracy: 0.9551 - val_loss: 0.1145 - val_accuracy: 0.9744\n",
            "Epoch 294/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9603 - val_loss: 0.1147 - val_accuracy: 0.9744\n",
            "Epoch 295/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1393 - accuracy: 0.9603 - val_loss: 0.1113 - val_accuracy: 0.9744\n",
            "Epoch 296/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1382 - accuracy: 0.9551 - val_loss: 0.1076 - val_accuracy: 0.9692\n",
            "Epoch 297/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1389 - accuracy: 0.9526 - val_loss: 0.1108 - val_accuracy: 0.9744\n",
            "Epoch 298/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1370 - accuracy: 0.9603 - val_loss: 0.1182 - val_accuracy: 0.9692\n",
            "Epoch 299/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1389 - accuracy: 0.9654 - val_loss: 0.1138 - val_accuracy: 0.9744\n",
            "Epoch 300/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1370 - accuracy: 0.9603 - val_loss: 0.1060 - val_accuracy: 0.9692\n",
            "Epoch 301/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1380 - accuracy: 0.9526 - val_loss: 0.1073 - val_accuracy: 0.9744\n",
            "Epoch 302/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1360 - accuracy: 0.9564 - val_loss: 0.1151 - val_accuracy: 0.9744\n",
            "Epoch 303/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1362 - accuracy: 0.9654 - val_loss: 0.1158 - val_accuracy: 0.9744\n",
            "Epoch 304/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1364 - accuracy: 0.9641 - val_loss: 0.1107 - val_accuracy: 0.9744\n",
            "Epoch 305/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1352 - accuracy: 0.9577 - val_loss: 0.1091 - val_accuracy: 0.9744\n",
            "Epoch 306/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1349 - accuracy: 0.9577 - val_loss: 0.1082 - val_accuracy: 0.9744\n",
            "Epoch 307/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1342 - accuracy: 0.9577 - val_loss: 0.1114 - val_accuracy: 0.9744\n",
            "Epoch 308/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1340 - accuracy: 0.9628 - val_loss: 0.1099 - val_accuracy: 0.9744\n",
            "Epoch 309/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1337 - accuracy: 0.9615 - val_loss: 0.1076 - val_accuracy: 0.9744\n",
            "Epoch 310/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1333 - accuracy: 0.9590 - val_loss: 0.1072 - val_accuracy: 0.9744\n",
            "Epoch 311/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1329 - accuracy: 0.9590 - val_loss: 0.1085 - val_accuracy: 0.9744\n",
            "Epoch 312/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1327 - accuracy: 0.9615 - val_loss: 0.1088 - val_accuracy: 0.9744\n",
            "Epoch 313/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1323 - accuracy: 0.9603 - val_loss: 0.1074 - val_accuracy: 0.9744\n",
            "Epoch 314/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1333 - accuracy: 0.9590 - val_loss: 0.1080 - val_accuracy: 0.9744\n",
            "Epoch 315/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1311 - accuracy: 0.9641 - val_loss: 0.1150 - val_accuracy: 0.9692\n",
            "Epoch 316/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1330 - accuracy: 0.9679 - val_loss: 0.1091 - val_accuracy: 0.9744\n",
            "Epoch 317/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1317 - accuracy: 0.9615 - val_loss: 0.1028 - val_accuracy: 0.9744\n",
            "Epoch 318/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1322 - accuracy: 0.9564 - val_loss: 0.1060 - val_accuracy: 0.9744\n",
            "Epoch 319/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1312 - accuracy: 0.9615 - val_loss: 0.1129 - val_accuracy: 0.9692\n",
            "Epoch 320/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1311 - accuracy: 0.9679 - val_loss: 0.1057 - val_accuracy: 0.9744\n",
            "Epoch 321/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1300 - accuracy: 0.9603 - val_loss: 0.1020 - val_accuracy: 0.9744\n",
            "Epoch 322/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1306 - accuracy: 0.9577 - val_loss: 0.1064 - val_accuracy: 0.9692\n",
            "Epoch 323/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1289 - accuracy: 0.9654 - val_loss: 0.1131 - val_accuracy: 0.9692\n",
            "Epoch 324/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1303 - accuracy: 0.9654 - val_loss: 0.1082 - val_accuracy: 0.9692\n",
            "Epoch 325/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1290 - accuracy: 0.9628 - val_loss: 0.1046 - val_accuracy: 0.9744\n",
            "Epoch 326/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1289 - accuracy: 0.9615 - val_loss: 0.1043 - val_accuracy: 0.9744\n",
            "Epoch 327/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1285 - accuracy: 0.9603 - val_loss: 0.1032 - val_accuracy: 0.9744\n",
            "Epoch 328/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1282 - accuracy: 0.9615 - val_loss: 0.1071 - val_accuracy: 0.9692\n",
            "Epoch 329/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1278 - accuracy: 0.9667 - val_loss: 0.1046 - val_accuracy: 0.9692\n",
            "Epoch 330/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1269 - accuracy: 0.9615 - val_loss: 0.1050 - val_accuracy: 0.9692\n",
            "Epoch 331/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1267 - accuracy: 0.9641 - val_loss: 0.1051 - val_accuracy: 0.9692\n",
            "Epoch 332/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1268 - accuracy: 0.9615 - val_loss: 0.1032 - val_accuracy: 0.9744\n",
            "Epoch 333/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1261 - accuracy: 0.9603 - val_loss: 0.1049 - val_accuracy: 0.9692\n",
            "Epoch 334/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1257 - accuracy: 0.9641 - val_loss: 0.1040 - val_accuracy: 0.9692\n",
            "Epoch 335/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1254 - accuracy: 0.9628 - val_loss: 0.1036 - val_accuracy: 0.9692\n",
            "Epoch 336/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1254 - accuracy: 0.9628 - val_loss: 0.1048 - val_accuracy: 0.9692\n",
            "Epoch 337/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1253 - accuracy: 0.9679 - val_loss: 0.1055 - val_accuracy: 0.9692\n",
            "Epoch 338/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1249 - accuracy: 0.9679 - val_loss: 0.1013 - val_accuracy: 0.9692\n",
            "Epoch 339/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1250 - accuracy: 0.9603 - val_loss: 0.1009 - val_accuracy: 0.9744\n",
            "Epoch 340/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9615 - val_loss: 0.1057 - val_accuracy: 0.9692\n",
            "Epoch 341/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1248 - accuracy: 0.9692 - val_loss: 0.1036 - val_accuracy: 0.9692\n",
            "Epoch 342/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1238 - accuracy: 0.9654 - val_loss: 0.0985 - val_accuracy: 0.9744\n",
            "Epoch 343/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1244 - accuracy: 0.9603 - val_loss: 0.1004 - val_accuracy: 0.9744\n",
            "Epoch 344/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9615 - val_loss: 0.1051 - val_accuracy: 0.9692\n",
            "Epoch 345/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1233 - accuracy: 0.9692 - val_loss: 0.1086 - val_accuracy: 0.9692\n",
            "Epoch 346/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1241 - accuracy: 0.9705 - val_loss: 0.1001 - val_accuracy: 0.9744\n",
            "Epoch 347/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1226 - accuracy: 0.9628 - val_loss: 0.0970 - val_accuracy: 0.9744\n",
            "Epoch 348/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1229 - accuracy: 0.9603 - val_loss: 0.1007 - val_accuracy: 0.9744\n",
            "Epoch 349/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1222 - accuracy: 0.9654 - val_loss: 0.1068 - val_accuracy: 0.9692\n",
            "Epoch 350/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1232 - accuracy: 0.9705 - val_loss: 0.0989 - val_accuracy: 0.9744\n",
            "Epoch 351/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1224 - accuracy: 0.9628 - val_loss: 0.0949 - val_accuracy: 0.9795\n",
            "Epoch 352/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1240 - accuracy: 0.9603 - val_loss: 0.0996 - val_accuracy: 0.9744\n",
            "Epoch 353/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1213 - accuracy: 0.9679 - val_loss: 0.1094 - val_accuracy: 0.9692\n",
            "Epoch 354/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1231 - accuracy: 0.9718 - val_loss: 0.0991 - val_accuracy: 0.9744\n",
            "Epoch 355/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1212 - accuracy: 0.9628 - val_loss: 0.0944 - val_accuracy: 0.9744\n",
            "Epoch 356/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1234 - accuracy: 0.9615 - val_loss: 0.0983 - val_accuracy: 0.9744\n",
            "Epoch 357/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1198 - accuracy: 0.9667 - val_loss: 0.1047 - val_accuracy: 0.9692\n",
            "Epoch 358/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1217 - accuracy: 0.9705 - val_loss: 0.1052 - val_accuracy: 0.9692\n",
            "Epoch 359/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1197 - accuracy: 0.9692 - val_loss: 0.0949 - val_accuracy: 0.9744\n",
            "Epoch 360/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1233 - accuracy: 0.9603 - val_loss: 0.0945 - val_accuracy: 0.9744\n",
            "Epoch 361/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1196 - accuracy: 0.9654 - val_loss: 0.1073 - val_accuracy: 0.9692\n",
            "Epoch 362/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1218 - accuracy: 0.9718 - val_loss: 0.1035 - val_accuracy: 0.9692\n",
            "Epoch 363/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1223 - accuracy: 0.9692 - val_loss: 0.0948 - val_accuracy: 0.9744\n",
            "Epoch 364/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1198 - accuracy: 0.9654 - val_loss: 0.0978 - val_accuracy: 0.9744\n",
            "Epoch 365/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1180 - accuracy: 0.9679 - val_loss: 0.0996 - val_accuracy: 0.9744\n",
            "Epoch 366/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1182 - accuracy: 0.9692 - val_loss: 0.0987 - val_accuracy: 0.9744\n",
            "Epoch 367/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1183 - accuracy: 0.9667 - val_loss: 0.0960 - val_accuracy: 0.9744\n",
            "Epoch 368/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1174 - accuracy: 0.9667 - val_loss: 0.0990 - val_accuracy: 0.9692\n",
            "Epoch 369/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1174 - accuracy: 0.9705 - val_loss: 0.0995 - val_accuracy: 0.9692\n",
            "Epoch 370/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1175 - accuracy: 0.9679 - val_loss: 0.0958 - val_accuracy: 0.9744\n",
            "Epoch 371/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1167 - accuracy: 0.9667 - val_loss: 0.0974 - val_accuracy: 0.9744\n",
            "Epoch 372/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1170 - accuracy: 0.9692 - val_loss: 0.0998 - val_accuracy: 0.9692\n",
            "Epoch 373/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1176 - accuracy: 0.9718 - val_loss: 0.0963 - val_accuracy: 0.9744\n",
            "Epoch 374/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1159 - accuracy: 0.9692 - val_loss: 0.0982 - val_accuracy: 0.9692\n",
            "Epoch 375/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1158 - accuracy: 0.9705 - val_loss: 0.0972 - val_accuracy: 0.9692\n",
            "Epoch 376/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9705 - val_loss: 0.0955 - val_accuracy: 0.9744\n",
            "Epoch 377/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1155 - accuracy: 0.9692 - val_loss: 0.0945 - val_accuracy: 0.9744\n",
            "Epoch 378/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1156 - accuracy: 0.9679 - val_loss: 0.0960 - val_accuracy: 0.9744\n",
            "Epoch 379/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1148 - accuracy: 0.9692 - val_loss: 0.0954 - val_accuracy: 0.9744\n",
            "Epoch 380/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9692 - val_loss: 0.0962 - val_accuracy: 0.9744\n",
            "Epoch 381/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1144 - accuracy: 0.9705 - val_loss: 0.0985 - val_accuracy: 0.9692\n",
            "Epoch 382/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1148 - accuracy: 0.9705 - val_loss: 0.0967 - val_accuracy: 0.9692\n",
            "Epoch 383/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1141 - accuracy: 0.9718 - val_loss: 0.0951 - val_accuracy: 0.9744\n",
            "Epoch 384/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1139 - accuracy: 0.9705 - val_loss: 0.0938 - val_accuracy: 0.9744\n",
            "Epoch 385/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1137 - accuracy: 0.9692 - val_loss: 0.0927 - val_accuracy: 0.9744\n",
            "Epoch 386/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1147 - accuracy: 0.9679 - val_loss: 0.0947 - val_accuracy: 0.9744\n",
            "Epoch 387/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1142 - accuracy: 0.9718 - val_loss: 0.1006 - val_accuracy: 0.9692\n",
            "Epoch 388/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1140 - accuracy: 0.9718 - val_loss: 0.0933 - val_accuracy: 0.9744\n",
            "Epoch 389/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1130 - accuracy: 0.9692 - val_loss: 0.0905 - val_accuracy: 0.9795\n",
            "Epoch 390/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1147 - accuracy: 0.9641 - val_loss: 0.0912 - val_accuracy: 0.9744\n",
            "Epoch 391/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1140 - accuracy: 0.9679 - val_loss: 0.0980 - val_accuracy: 0.9692\n",
            "Epoch 392/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1135 - accuracy: 0.9705 - val_loss: 0.0956 - val_accuracy: 0.9692\n",
            "Epoch 393/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1125 - accuracy: 0.9705 - val_loss: 0.0901 - val_accuracy: 0.9744\n",
            "Epoch 394/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1134 - accuracy: 0.9679 - val_loss: 0.0916 - val_accuracy: 0.9744\n",
            "Epoch 395/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1125 - accuracy: 0.9679 - val_loss: 0.0951 - val_accuracy: 0.9692\n",
            "Epoch 396/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1120 - accuracy: 0.9705 - val_loss: 0.0998 - val_accuracy: 0.9692\n",
            "Epoch 397/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1126 - accuracy: 0.9718 - val_loss: 0.0915 - val_accuracy: 0.9744\n",
            "Epoch 398/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1152 - accuracy: 0.9641 - val_loss: 0.0887 - val_accuracy: 0.9795\n",
            "Epoch 399/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1121 - accuracy: 0.9667 - val_loss: 0.0983 - val_accuracy: 0.9692\n",
            "Epoch 400/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1129 - accuracy: 0.9731 - val_loss: 0.0985 - val_accuracy: 0.9692\n",
            "Epoch 401/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1113 - accuracy: 0.9705 - val_loss: 0.0889 - val_accuracy: 0.9795\n",
            "Epoch 402/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1125 - accuracy: 0.9654 - val_loss: 0.0886 - val_accuracy: 0.9795\n",
            "Epoch 403/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1107 - accuracy: 0.9692 - val_loss: 0.0973 - val_accuracy: 0.9692\n",
            "Epoch 404/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1116 - accuracy: 0.9718 - val_loss: 0.0979 - val_accuracy: 0.9692\n",
            "Epoch 405/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1106 - accuracy: 0.9718 - val_loss: 0.0887 - val_accuracy: 0.9795\n",
            "Epoch 406/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1106 - accuracy: 0.9679 - val_loss: 0.0872 - val_accuracy: 0.9795\n",
            "Epoch 407/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1116 - accuracy: 0.9667 - val_loss: 0.0912 - val_accuracy: 0.9744\n",
            "Epoch 408/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1095 - accuracy: 0.9705 - val_loss: 0.0951 - val_accuracy: 0.9692\n",
            "Epoch 409/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1097 - accuracy: 0.9718 - val_loss: 0.0904 - val_accuracy: 0.9744\n",
            "Epoch 410/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1090 - accuracy: 0.9718 - val_loss: 0.0871 - val_accuracy: 0.9795\n",
            "Epoch 411/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1100 - accuracy: 0.9679 - val_loss: 0.0896 - val_accuracy: 0.9744\n",
            "Epoch 412/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1089 - accuracy: 0.9718 - val_loss: 0.0958 - val_accuracy: 0.9692\n",
            "Epoch 413/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1094 - accuracy: 0.9718 - val_loss: 0.0921 - val_accuracy: 0.9692\n",
            "Epoch 414/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1088 - accuracy: 0.9705 - val_loss: 0.0878 - val_accuracy: 0.9795\n",
            "Epoch 415/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1086 - accuracy: 0.9692 - val_loss: 0.0898 - val_accuracy: 0.9795\n",
            "Epoch 416/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9718 - val_loss: 0.0946 - val_accuracy: 0.9692\n",
            "Epoch 417/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1092 - accuracy: 0.9718 - val_loss: 0.0897 - val_accuracy: 0.9795\n",
            "Epoch 418/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1080 - accuracy: 0.9705 - val_loss: 0.0856 - val_accuracy: 0.9795\n",
            "Epoch 419/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1104 - accuracy: 0.9641 - val_loss: 0.0878 - val_accuracy: 0.9795\n",
            "Epoch 420/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1071 - accuracy: 0.9718 - val_loss: 0.0938 - val_accuracy: 0.9692\n",
            "Epoch 421/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1079 - accuracy: 0.9718 - val_loss: 0.0927 - val_accuracy: 0.9692\n",
            "Epoch 422/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1072 - accuracy: 0.9718 - val_loss: 0.0872 - val_accuracy: 0.9795\n",
            "Epoch 423/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1073 - accuracy: 0.9692 - val_loss: 0.0859 - val_accuracy: 0.9795\n",
            "Epoch 424/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1082 - accuracy: 0.9679 - val_loss: 0.0895 - val_accuracy: 0.9692\n",
            "Epoch 425/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1065 - accuracy: 0.9718 - val_loss: 0.0908 - val_accuracy: 0.9692\n",
            "Epoch 426/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1063 - accuracy: 0.9718 - val_loss: 0.0889 - val_accuracy: 0.9744\n",
            "Epoch 427/2000\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1075 - accuracy: 0.9705 - val_loss: 0.0891 - val_accuracy: 0.9795\n",
            "Epoch 428/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9718 - val_loss: 0.0938 - val_accuracy: 0.9692\n",
            "Epoch 429/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1068 - accuracy: 0.9718 - val_loss: 0.0890 - val_accuracy: 0.9795\n",
            "Epoch 430/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1059 - accuracy: 0.9705 - val_loss: 0.0852 - val_accuracy: 0.9795\n",
            "Epoch 431/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1062 - accuracy: 0.9692 - val_loss: 0.0874 - val_accuracy: 0.9795\n",
            "Epoch 432/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1051 - accuracy: 0.9718 - val_loss: 0.0934 - val_accuracy: 0.9692\n",
            "Epoch 433/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1067 - accuracy: 0.9744 - val_loss: 0.0916 - val_accuracy: 0.9692\n",
            "Epoch 434/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1049 - accuracy: 0.9718 - val_loss: 0.0846 - val_accuracy: 0.9795\n",
            "Epoch 435/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1071 - accuracy: 0.9654 - val_loss: 0.0838 - val_accuracy: 0.9795\n",
            "Epoch 436/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1059 - accuracy: 0.9679 - val_loss: 0.0908 - val_accuracy: 0.9692\n",
            "Epoch 437/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1056 - accuracy: 0.9731 - val_loss: 0.0921 - val_accuracy: 0.9692\n",
            "Epoch 438/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1050 - accuracy: 0.9718 - val_loss: 0.0850 - val_accuracy: 0.9795\n",
            "Epoch 439/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1053 - accuracy: 0.9705 - val_loss: 0.0844 - val_accuracy: 0.9795\n",
            "Epoch 440/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1042 - accuracy: 0.9731 - val_loss: 0.0900 - val_accuracy: 0.9744\n",
            "Epoch 441/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1043 - accuracy: 0.9718 - val_loss: 0.0898 - val_accuracy: 0.9795\n",
            "Epoch 442/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1044 - accuracy: 0.9718 - val_loss: 0.0854 - val_accuracy: 0.9795\n",
            "Epoch 443/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1058 - accuracy: 0.9679 - val_loss: 0.0825 - val_accuracy: 0.9795\n",
            "Epoch 444/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1046 - accuracy: 0.9692 - val_loss: 0.0882 - val_accuracy: 0.9795\n",
            "Epoch 445/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1039 - accuracy: 0.9731 - val_loss: 0.0916 - val_accuracy: 0.9692\n",
            "Epoch 446/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1040 - accuracy: 0.9731 - val_loss: 0.0843 - val_accuracy: 0.9795\n",
            "Epoch 447/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1031 - accuracy: 0.9692 - val_loss: 0.0825 - val_accuracy: 0.9795\n",
            "Epoch 448/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1039 - accuracy: 0.9692 - val_loss: 0.0853 - val_accuracy: 0.9795\n",
            "Epoch 449/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1026 - accuracy: 0.9705 - val_loss: 0.0882 - val_accuracy: 0.9795\n",
            "Epoch 450/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1026 - accuracy: 0.9718 - val_loss: 0.0851 - val_accuracy: 0.9795\n",
            "Epoch 451/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1023 - accuracy: 0.9718 - val_loss: 0.0825 - val_accuracy: 0.9795\n",
            "Epoch 452/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1040 - accuracy: 0.9705 - val_loss: 0.0840 - val_accuracy: 0.9795\n",
            "Epoch 453/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1020 - accuracy: 0.9705 - val_loss: 0.0839 - val_accuracy: 0.9795\n",
            "Epoch 454/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1018 - accuracy: 0.9705 - val_loss: 0.0851 - val_accuracy: 0.9795\n",
            "Epoch 455/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1015 - accuracy: 0.9705 - val_loss: 0.0865 - val_accuracy: 0.9795\n",
            "Epoch 456/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1019 - accuracy: 0.9705 - val_loss: 0.0862 - val_accuracy: 0.9795\n",
            "Epoch 457/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1013 - accuracy: 0.9705 - val_loss: 0.0871 - val_accuracy: 0.9795\n",
            "Epoch 458/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1014 - accuracy: 0.9705 - val_loss: 0.0847 - val_accuracy: 0.9795\n",
            "Epoch 459/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1015 - accuracy: 0.9718 - val_loss: 0.0837 - val_accuracy: 0.9795\n",
            "Epoch 460/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1010 - accuracy: 0.9731 - val_loss: 0.0860 - val_accuracy: 0.9795\n",
            "Epoch 461/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1005 - accuracy: 0.9718 - val_loss: 0.0835 - val_accuracy: 0.9795\n",
            "Epoch 462/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1004 - accuracy: 0.9718 - val_loss: 0.0824 - val_accuracy: 0.9795\n",
            "Epoch 463/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1004 - accuracy: 0.9705 - val_loss: 0.0841 - val_accuracy: 0.9795\n",
            "Epoch 464/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1001 - accuracy: 0.9718 - val_loss: 0.0846 - val_accuracy: 0.9795\n",
            "Epoch 465/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1004 - accuracy: 0.9705 - val_loss: 0.0823 - val_accuracy: 0.9795\n",
            "Epoch 466/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1005 - accuracy: 0.9718 - val_loss: 0.0806 - val_accuracy: 0.9795\n",
            "Epoch 467/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1000 - accuracy: 0.9705 - val_loss: 0.0854 - val_accuracy: 0.9795\n",
            "Epoch 468/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1003 - accuracy: 0.9731 - val_loss: 0.0873 - val_accuracy: 0.9795\n",
            "Epoch 469/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1001 - accuracy: 0.9718 - val_loss: 0.0814 - val_accuracy: 0.9795\n",
            "Epoch 470/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0995 - accuracy: 0.9705 - val_loss: 0.0814 - val_accuracy: 0.9795\n",
            "Epoch 471/2000\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.0996 - accuracy: 0.9705 - val_loss: 0.0834 - val_accuracy: 0.9795\n",
            "Epoch 472/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0985 - accuracy: 0.9705 - val_loss: 0.0826 - val_accuracy: 0.9795\n",
            "Epoch 473/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0987 - accuracy: 0.9718 - val_loss: 0.0819 - val_accuracy: 0.9795\n",
            "Epoch 474/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0995 - accuracy: 0.9718 - val_loss: 0.0815 - val_accuracy: 0.9795\n",
            "Epoch 475/2000\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0989 - accuracy: 0.9731 - val_loss: 0.0849 - val_accuracy: 0.9795\n",
            "Epoch 476/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0987 - accuracy: 0.9731 - val_loss: 0.0835 - val_accuracy: 0.9795\n",
            "Epoch 477/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0994 - accuracy: 0.9718 - val_loss: 0.0817 - val_accuracy: 0.9795\n",
            "Epoch 478/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0978 - accuracy: 0.9718 - val_loss: 0.0846 - val_accuracy: 0.9795\n",
            "Epoch 479/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9731 - val_loss: 0.0828 - val_accuracy: 0.9795\n",
            "Epoch 480/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0978 - accuracy: 0.9718 - val_loss: 0.0810 - val_accuracy: 0.9795\n",
            "Epoch 481/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0975 - accuracy: 0.9718 - val_loss: 0.0822 - val_accuracy: 0.9795\n",
            "Epoch 482/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0975 - accuracy: 0.9731 - val_loss: 0.0835 - val_accuracy: 0.9795\n",
            "Epoch 483/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0971 - accuracy: 0.9731 - val_loss: 0.0853 - val_accuracy: 0.9744\n",
            "Epoch 484/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0977 - accuracy: 0.9731 - val_loss: 0.0817 - val_accuracy: 0.9795\n",
            "Epoch 485/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0967 - accuracy: 0.9731 - val_loss: 0.0786 - val_accuracy: 0.9795\n",
            "Epoch 486/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0980 - accuracy: 0.9705 - val_loss: 0.0795 - val_accuracy: 0.9795\n",
            "Epoch 487/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0966 - accuracy: 0.9705 - val_loss: 0.0824 - val_accuracy: 0.9795\n",
            "Epoch 488/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0968 - accuracy: 0.9731 - val_loss: 0.0832 - val_accuracy: 0.9795\n",
            "Epoch 489/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0962 - accuracy: 0.9731 - val_loss: 0.0785 - val_accuracy: 0.9795\n",
            "Epoch 490/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0971 - accuracy: 0.9705 - val_loss: 0.0775 - val_accuracy: 0.9795\n",
            "Epoch 491/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0970 - accuracy: 0.9705 - val_loss: 0.0827 - val_accuracy: 0.9744\n",
            "Epoch 492/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0960 - accuracy: 0.9731 - val_loss: 0.0831 - val_accuracy: 0.9744\n",
            "Epoch 493/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0957 - accuracy: 0.9731 - val_loss: 0.0791 - val_accuracy: 0.9795\n",
            "Epoch 494/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0957 - accuracy: 0.9718 - val_loss: 0.0787 - val_accuracy: 0.9795\n",
            "Epoch 495/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0957 - accuracy: 0.9718 - val_loss: 0.0806 - val_accuracy: 0.9795\n",
            "Epoch 496/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0951 - accuracy: 0.9731 - val_loss: 0.0808 - val_accuracy: 0.9795\n",
            "Epoch 497/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0952 - accuracy: 0.9731 - val_loss: 0.0809 - val_accuracy: 0.9795\n",
            "Epoch 498/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0951 - accuracy: 0.9731 - val_loss: 0.0804 - val_accuracy: 0.9795\n",
            "Epoch 499/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0949 - accuracy: 0.9718 - val_loss: 0.0790 - val_accuracy: 0.9795\n",
            "Epoch 500/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0952 - accuracy: 0.9744 - val_loss: 0.0801 - val_accuracy: 0.9795\n",
            "Epoch 501/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0954 - accuracy: 0.9731 - val_loss: 0.0791 - val_accuracy: 0.9795\n",
            "Epoch 502/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0942 - accuracy: 0.9744 - val_loss: 0.0825 - val_accuracy: 0.9744\n",
            "Epoch 503/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9731 - val_loss: 0.0809 - val_accuracy: 0.9744\n",
            "Epoch 504/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0942 - accuracy: 0.9718 - val_loss: 0.0758 - val_accuracy: 0.9795\n",
            "Epoch 505/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0960 - accuracy: 0.9679 - val_loss: 0.0781 - val_accuracy: 0.9795\n",
            "Epoch 506/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0965 - accuracy: 0.9718 - val_loss: 0.0850 - val_accuracy: 0.9744\n",
            "Epoch 507/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0951 - accuracy: 0.9744 - val_loss: 0.0767 - val_accuracy: 0.9795\n",
            "Epoch 508/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0944 - accuracy: 0.9705 - val_loss: 0.0756 - val_accuracy: 0.9795\n",
            "Epoch 509/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0943 - accuracy: 0.9692 - val_loss: 0.0794 - val_accuracy: 0.9795\n",
            "Epoch 510/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0944 - accuracy: 0.9744 - val_loss: 0.0829 - val_accuracy: 0.9744\n",
            "Epoch 511/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0936 - accuracy: 0.9731 - val_loss: 0.0772 - val_accuracy: 0.9795\n",
            "Epoch 512/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0949 - accuracy: 0.9718 - val_loss: 0.0772 - val_accuracy: 0.9795\n",
            "Epoch 513/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0937 - accuracy: 0.9744 - val_loss: 0.0837 - val_accuracy: 0.9744\n",
            "Epoch 514/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0938 - accuracy: 0.9744 - val_loss: 0.0789 - val_accuracy: 0.9744\n",
            "Epoch 515/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0926 - accuracy: 0.9744 - val_loss: 0.0771 - val_accuracy: 0.9795\n",
            "Epoch 516/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0928 - accuracy: 0.9744 - val_loss: 0.0792 - val_accuracy: 0.9795\n",
            "Epoch 517/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0925 - accuracy: 0.9756 - val_loss: 0.0814 - val_accuracy: 0.9744\n",
            "Epoch 518/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0924 - accuracy: 0.9756 - val_loss: 0.0773 - val_accuracy: 0.9795\n",
            "Epoch 519/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0921 - accuracy: 0.9744 - val_loss: 0.0757 - val_accuracy: 0.9795\n",
            "Epoch 520/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0922 - accuracy: 0.9744 - val_loss: 0.0788 - val_accuracy: 0.9744\n",
            "Epoch 521/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0920 - accuracy: 0.9744 - val_loss: 0.0796 - val_accuracy: 0.9744\n",
            "Epoch 522/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0915 - accuracy: 0.9744 - val_loss: 0.0764 - val_accuracy: 0.9795\n",
            "Epoch 523/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0916 - accuracy: 0.9744 - val_loss: 0.0759 - val_accuracy: 0.9795\n",
            "Epoch 524/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0915 - accuracy: 0.9744 - val_loss: 0.0786 - val_accuracy: 0.9744\n",
            "Epoch 525/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0912 - accuracy: 0.9744 - val_loss: 0.0783 - val_accuracy: 0.9744\n",
            "Epoch 526/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0911 - accuracy: 0.9731 - val_loss: 0.0771 - val_accuracy: 0.9744\n",
            "Epoch 527/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0908 - accuracy: 0.9744 - val_loss: 0.0766 - val_accuracy: 0.9744\n",
            "Epoch 528/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0908 - accuracy: 0.9744 - val_loss: 0.0757 - val_accuracy: 0.9744\n",
            "Epoch 529/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0913 - accuracy: 0.9744 - val_loss: 0.0770 - val_accuracy: 0.9744\n",
            "Epoch 530/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0906 - accuracy: 0.9744 - val_loss: 0.0768 - val_accuracy: 0.9744\n",
            "Epoch 531/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0902 - accuracy: 0.9744 - val_loss: 0.0785 - val_accuracy: 0.9744\n",
            "Epoch 532/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0913 - accuracy: 0.9744 - val_loss: 0.0773 - val_accuracy: 0.9744\n",
            "Epoch 533/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0912 - accuracy: 0.9705 - val_loss: 0.0740 - val_accuracy: 0.9795\n",
            "Epoch 534/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0906 - accuracy: 0.9731 - val_loss: 0.0781 - val_accuracy: 0.9744\n",
            "Epoch 535/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9756 - val_loss: 0.0786 - val_accuracy: 0.9744\n",
            "Epoch 536/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0896 - accuracy: 0.9744 - val_loss: 0.0750 - val_accuracy: 0.9744\n",
            "Epoch 537/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9744 - val_loss: 0.0748 - val_accuracy: 0.9744\n",
            "Epoch 538/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0893 - accuracy: 0.9744 - val_loss: 0.0791 - val_accuracy: 0.9744\n",
            "Epoch 539/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0895 - accuracy: 0.9756 - val_loss: 0.0803 - val_accuracy: 0.9744\n",
            "Epoch 540/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0896 - accuracy: 0.9744 - val_loss: 0.0756 - val_accuracy: 0.9744\n",
            "Epoch 541/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0895 - accuracy: 0.9744 - val_loss: 0.0742 - val_accuracy: 0.9795\n",
            "Epoch 542/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0899 - accuracy: 0.9744 - val_loss: 0.0795 - val_accuracy: 0.9744\n",
            "Epoch 543/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0903 - accuracy: 0.9744 - val_loss: 0.0835 - val_accuracy: 0.9744\n",
            "Epoch 544/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0897 - accuracy: 0.9756 - val_loss: 0.0733 - val_accuracy: 0.9795\n",
            "Epoch 545/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0891 - accuracy: 0.9731 - val_loss: 0.0721 - val_accuracy: 0.9795\n",
            "Epoch 546/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0907 - accuracy: 0.9705 - val_loss: 0.0756 - val_accuracy: 0.9744\n",
            "Epoch 547/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0882 - accuracy: 0.9756 - val_loss: 0.0800 - val_accuracy: 0.9744\n",
            "Epoch 548/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0887 - accuracy: 0.9756 - val_loss: 0.0770 - val_accuracy: 0.9744\n",
            "Epoch 549/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0880 - accuracy: 0.9756 - val_loss: 0.0735 - val_accuracy: 0.9744\n",
            "Epoch 550/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0886 - accuracy: 0.9744 - val_loss: 0.0746 - val_accuracy: 0.9744\n",
            "Epoch 551/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0885 - accuracy: 0.9756 - val_loss: 0.0741 - val_accuracy: 0.9744\n",
            "Epoch 552/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0900 - accuracy: 0.9731 - val_loss: 0.0739 - val_accuracy: 0.9744\n",
            "Epoch 553/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0873 - accuracy: 0.9731 - val_loss: 0.0811 - val_accuracy: 0.9744\n",
            "Epoch 554/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0889 - accuracy: 0.9769 - val_loss: 0.0755 - val_accuracy: 0.9744\n",
            "Epoch 555/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9744 - val_loss: 0.0710 - val_accuracy: 0.9846\n",
            "Epoch 556/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0892 - accuracy: 0.9705 - val_loss: 0.0730 - val_accuracy: 0.9744\n",
            "Epoch 557/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0869 - accuracy: 0.9744 - val_loss: 0.0811 - val_accuracy: 0.9744\n",
            "Epoch 558/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0882 - accuracy: 0.9756 - val_loss: 0.0766 - val_accuracy: 0.9744\n",
            "Epoch 559/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0863 - accuracy: 0.9782 - val_loss: 0.0719 - val_accuracy: 0.9744\n",
            "Epoch 560/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0880 - accuracy: 0.9744 - val_loss: 0.0718 - val_accuracy: 0.9744\n",
            "Epoch 561/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0872 - accuracy: 0.9731 - val_loss: 0.0772 - val_accuracy: 0.9744\n",
            "Epoch 562/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0867 - accuracy: 0.9756 - val_loss: 0.0782 - val_accuracy: 0.9744\n",
            "Epoch 563/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0864 - accuracy: 0.9756 - val_loss: 0.0735 - val_accuracy: 0.9744\n",
            "Epoch 564/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0860 - accuracy: 0.9744 - val_loss: 0.0719 - val_accuracy: 0.9744\n",
            "Epoch 565/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0868 - accuracy: 0.9744 - val_loss: 0.0729 - val_accuracy: 0.9744\n",
            "Epoch 566/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0860 - accuracy: 0.9744 - val_loss: 0.0779 - val_accuracy: 0.9744\n",
            "Epoch 567/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0861 - accuracy: 0.9744 - val_loss: 0.0755 - val_accuracy: 0.9744\n",
            "Epoch 568/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0864 - accuracy: 0.9756 - val_loss: 0.0741 - val_accuracy: 0.9744\n",
            "Epoch 569/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0856 - accuracy: 0.9744 - val_loss: 0.0757 - val_accuracy: 0.9744\n",
            "Epoch 570/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0855 - accuracy: 0.9756 - val_loss: 0.0735 - val_accuracy: 0.9744\n",
            "Epoch 571/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0850 - accuracy: 0.9756 - val_loss: 0.0745 - val_accuracy: 0.9744\n",
            "Epoch 572/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0850 - accuracy: 0.9756 - val_loss: 0.0742 - val_accuracy: 0.9744\n",
            "Epoch 573/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0847 - accuracy: 0.9756 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
            "Epoch 574/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0847 - accuracy: 0.9756 - val_loss: 0.0740 - val_accuracy: 0.9744\n",
            "Epoch 575/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0845 - accuracy: 0.9769 - val_loss: 0.0754 - val_accuracy: 0.9744\n",
            "Epoch 576/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0850 - accuracy: 0.9769 - val_loss: 0.0743 - val_accuracy: 0.9744\n",
            "Epoch 577/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0847 - accuracy: 0.9769 - val_loss: 0.0742 - val_accuracy: 0.9744\n",
            "Epoch 578/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0842 - accuracy: 0.9769 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
            "Epoch 579/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0842 - accuracy: 0.9756 - val_loss: 0.0731 - val_accuracy: 0.9744\n",
            "Epoch 580/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0849 - accuracy: 0.9756 - val_loss: 0.0724 - val_accuracy: 0.9795\n",
            "Epoch 581/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0844 - accuracy: 0.9756 - val_loss: 0.0706 - val_accuracy: 0.9795\n",
            "Epoch 582/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0846 - accuracy: 0.9744 - val_loss: 0.0732 - val_accuracy: 0.9744\n",
            "Epoch 583/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0842 - accuracy: 0.9756 - val_loss: 0.0768 - val_accuracy: 0.9692\n",
            "Epoch 584/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0844 - accuracy: 0.9769 - val_loss: 0.0730 - val_accuracy: 0.9744\n",
            "Epoch 585/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0837 - accuracy: 0.9769 - val_loss: 0.0724 - val_accuracy: 0.9744\n",
            "Epoch 586/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0834 - accuracy: 0.9756 - val_loss: 0.0739 - val_accuracy: 0.9744\n",
            "Epoch 587/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0842 - accuracy: 0.9769 - val_loss: 0.0760 - val_accuracy: 0.9744\n",
            "Epoch 588/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0849 - accuracy: 0.9769 - val_loss: 0.0719 - val_accuracy: 0.9795\n",
            "Epoch 589/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0832 - accuracy: 0.9756 - val_loss: 0.0737 - val_accuracy: 0.9744\n",
            "Epoch 590/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0830 - accuracy: 0.9769 - val_loss: 0.0738 - val_accuracy: 0.9744\n",
            "Epoch 591/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0832 - accuracy: 0.9769 - val_loss: 0.0725 - val_accuracy: 0.9795\n",
            "Epoch 592/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0830 - accuracy: 0.9756 - val_loss: 0.0755 - val_accuracy: 0.9692\n",
            "Epoch 593/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0828 - accuracy: 0.9756 - val_loss: 0.0793 - val_accuracy: 0.9692\n",
            "Epoch 594/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0836 - accuracy: 0.9769 - val_loss: 0.0732 - val_accuracy: 0.9744\n",
            "Epoch 595/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0837 - accuracy: 0.9769 - val_loss: 0.0703 - val_accuracy: 0.9795\n",
            "Epoch 596/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0839 - accuracy: 0.9756 - val_loss: 0.0745 - val_accuracy: 0.9692\n",
            "Epoch 597/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9769 - val_loss: 0.0727 - val_accuracy: 0.9744\n",
            "Epoch 598/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0818 - accuracy: 0.9769 - val_loss: 0.0718 - val_accuracy: 0.9795\n",
            "Epoch 599/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9769 - val_loss: 0.0728 - val_accuracy: 0.9744\n",
            "Epoch 600/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0816 - accuracy: 0.9769 - val_loss: 0.0749 - val_accuracy: 0.9692\n",
            "Epoch 601/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0819 - accuracy: 0.9756 - val_loss: 0.0718 - val_accuracy: 0.9744\n",
            "Epoch 602/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0853 - accuracy: 0.9744 - val_loss: 0.0704 - val_accuracy: 0.9795\n",
            "Epoch 603/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0807 - accuracy: 0.9769 - val_loss: 0.0814 - val_accuracy: 0.9692\n",
            "Epoch 604/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0855 - accuracy: 0.9756 - val_loss: 0.0785 - val_accuracy: 0.9692\n",
            "Epoch 605/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0824 - accuracy: 0.9769 - val_loss: 0.0680 - val_accuracy: 0.9846\n",
            "Epoch 606/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0858 - accuracy: 0.9705 - val_loss: 0.0683 - val_accuracy: 0.9795\n",
            "Epoch 607/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0834 - accuracy: 0.9756 - val_loss: 0.0805 - val_accuracy: 0.9692\n",
            "Epoch 608/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0834 - accuracy: 0.9756 - val_loss: 0.0727 - val_accuracy: 0.9795\n",
            "Epoch 609/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0830 - accuracy: 0.9769 - val_loss: 0.0681 - val_accuracy: 0.9795\n",
            "Epoch 610/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0827 - accuracy: 0.9756 - val_loss: 0.0721 - val_accuracy: 0.9795\n",
            "Epoch 611/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0806 - accuracy: 0.9756 - val_loss: 0.0764 - val_accuracy: 0.9692\n",
            "Epoch 612/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0813 - accuracy: 0.9782 - val_loss: 0.0714 - val_accuracy: 0.9795\n",
            "Epoch 613/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0805 - accuracy: 0.9769 - val_loss: 0.0685 - val_accuracy: 0.9795\n",
            "Epoch 614/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0813 - accuracy: 0.9756 - val_loss: 0.0712 - val_accuracy: 0.9795\n",
            "Epoch 615/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0803 - accuracy: 0.9782 - val_loss: 0.0739 - val_accuracy: 0.9744\n",
            "Epoch 616/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0803 - accuracy: 0.9769 - val_loss: 0.0709 - val_accuracy: 0.9795\n",
            "Epoch 617/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0800 - accuracy: 0.9769 - val_loss: 0.0701 - val_accuracy: 0.9795\n",
            "Epoch 618/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9769 - val_loss: 0.0730 - val_accuracy: 0.9744\n",
            "Epoch 619/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0801 - accuracy: 0.9782 - val_loss: 0.0733 - val_accuracy: 0.9744\n",
            "Epoch 620/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0799 - accuracy: 0.9769 - val_loss: 0.0699 - val_accuracy: 0.9795\n",
            "Epoch 621/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9769 - val_loss: 0.0712 - val_accuracy: 0.9795\n",
            "Epoch 622/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0797 - accuracy: 0.9769 - val_loss: 0.0731 - val_accuracy: 0.9744\n",
            "Epoch 623/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0797 - accuracy: 0.9769 - val_loss: 0.0719 - val_accuracy: 0.9795\n",
            "Epoch 624/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0801 - accuracy: 0.9769 - val_loss: 0.0696 - val_accuracy: 0.9795\n",
            "Epoch 625/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0795 - accuracy: 0.9769 - val_loss: 0.0735 - val_accuracy: 0.9744\n",
            "Epoch 626/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0792 - accuracy: 0.9769 - val_loss: 0.0765 - val_accuracy: 0.9692\n",
            "Epoch 627/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0801 - accuracy: 0.9744 - val_loss: 0.0710 - val_accuracy: 0.9744\n",
            "Epoch 628/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0786 - accuracy: 0.9769 - val_loss: 0.0694 - val_accuracy: 0.9795\n",
            "Epoch 629/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0798 - accuracy: 0.9769 - val_loss: 0.0722 - val_accuracy: 0.9744\n",
            "Epoch 630/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0787 - accuracy: 0.9782 - val_loss: 0.0787 - val_accuracy: 0.9692\n",
            "Epoch 631/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0813 - accuracy: 0.9769 - val_loss: 0.0708 - val_accuracy: 0.9795\n",
            "Epoch 632/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0790 - accuracy: 0.9769 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
            "Epoch 633/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0791 - accuracy: 0.9782 - val_loss: 0.0672 - val_accuracy: 0.9795\n",
            "Epoch 634/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0796 - accuracy: 0.9769 - val_loss: 0.0695 - val_accuracy: 0.9795\n",
            "Epoch 635/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0786 - accuracy: 0.9769 - val_loss: 0.0757 - val_accuracy: 0.9692\n",
            "Epoch 636/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0790 - accuracy: 0.9769 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
            "Epoch 637/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0775 - accuracy: 0.9782 - val_loss: 0.0675 - val_accuracy: 0.9795\n",
            "Epoch 638/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0791 - accuracy: 0.9782 - val_loss: 0.0667 - val_accuracy: 0.9795\n",
            "Epoch 639/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0795 - accuracy: 0.9756 - val_loss: 0.0689 - val_accuracy: 0.9795\n",
            "Epoch 640/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0775 - accuracy: 0.9769 - val_loss: 0.0745 - val_accuracy: 0.9744\n",
            "Epoch 641/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0780 - accuracy: 0.9769 - val_loss: 0.0724 - val_accuracy: 0.9744\n",
            "Epoch 642/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0778 - accuracy: 0.9782 - val_loss: 0.0693 - val_accuracy: 0.9744\n",
            "Epoch 643/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0778 - accuracy: 0.9769 - val_loss: 0.0694 - val_accuracy: 0.9744\n",
            "Epoch 644/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0779 - accuracy: 0.9769 - val_loss: 0.0710 - val_accuracy: 0.9744\n",
            "Epoch 645/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0771 - accuracy: 0.9769 - val_loss: 0.0702 - val_accuracy: 0.9744\n",
            "Epoch 646/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0770 - accuracy: 0.9769 - val_loss: 0.0691 - val_accuracy: 0.9744\n",
            "Epoch 647/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0774 - accuracy: 0.9769 - val_loss: 0.0690 - val_accuracy: 0.9744\n",
            "Epoch 648/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0769 - accuracy: 0.9769 - val_loss: 0.0731 - val_accuracy: 0.9744\n",
            "Epoch 649/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0769 - accuracy: 0.9769 - val_loss: 0.0723 - val_accuracy: 0.9744\n",
            "Epoch 650/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0778 - accuracy: 0.9769 - val_loss: 0.0698 - val_accuracy: 0.9744\n",
            "Epoch 651/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0766 - accuracy: 0.9769 - val_loss: 0.0717 - val_accuracy: 0.9744\n",
            "Epoch 652/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0764 - accuracy: 0.9769 - val_loss: 0.0693 - val_accuracy: 0.9744\n",
            "Epoch 653/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9769 - val_loss: 0.0686 - val_accuracy: 0.9795\n",
            "Epoch 654/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0766 - accuracy: 0.9769 - val_loss: 0.0686 - val_accuracy: 0.9795\n",
            "Epoch 655/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0763 - accuracy: 0.9769 - val_loss: 0.0681 - val_accuracy: 0.9795\n",
            "Epoch 656/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0767 - accuracy: 0.9769 - val_loss: 0.0689 - val_accuracy: 0.9744\n",
            "Epoch 657/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0759 - accuracy: 0.9769 - val_loss: 0.0689 - val_accuracy: 0.9744\n",
            "Epoch 658/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0762 - accuracy: 0.9769 - val_loss: 0.0703 - val_accuracy: 0.9744\n",
            "Epoch 659/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0758 - accuracy: 0.9769 - val_loss: 0.0696 - val_accuracy: 0.9744\n",
            "Epoch 660/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0757 - accuracy: 0.9769 - val_loss: 0.0705 - val_accuracy: 0.9744\n",
            "Epoch 661/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0760 - accuracy: 0.9769 - val_loss: 0.0701 - val_accuracy: 0.9744\n",
            "Epoch 662/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9769 - val_loss: 0.0720 - val_accuracy: 0.9744\n",
            "Epoch 663/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0770 - accuracy: 0.9769 - val_loss: 0.0709 - val_accuracy: 0.9744\n",
            "Epoch 664/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0793 - accuracy: 0.9769 - val_loss: 0.0662 - val_accuracy: 0.9795\n",
            "Epoch 665/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0765 - accuracy: 0.9782 - val_loss: 0.0716 - val_accuracy: 0.9744\n",
            "Epoch 666/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0756 - accuracy: 0.9782 - val_loss: 0.0696 - val_accuracy: 0.9744\n",
            "Epoch 667/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0747 - accuracy: 0.9769 - val_loss: 0.0660 - val_accuracy: 0.9795\n",
            "Epoch 668/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0758 - accuracy: 0.9795 - val_loss: 0.0680 - val_accuracy: 0.9795\n",
            "Epoch 669/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0758 - accuracy: 0.9756 - val_loss: 0.0717 - val_accuracy: 0.9744\n",
            "Epoch 670/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0745 - accuracy: 0.9782 - val_loss: 0.0677 - val_accuracy: 0.9795\n",
            "Epoch 671/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0751 - accuracy: 0.9769 - val_loss: 0.0672 - val_accuracy: 0.9795\n",
            "Epoch 672/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0743 - accuracy: 0.9769 - val_loss: 0.0727 - val_accuracy: 0.9744\n",
            "Epoch 673/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0745 - accuracy: 0.9782 - val_loss: 0.0708 - val_accuracy: 0.9744\n",
            "Epoch 674/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0740 - accuracy: 0.9782 - val_loss: 0.0674 - val_accuracy: 0.9744\n",
            "Epoch 675/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0743 - accuracy: 0.9782 - val_loss: 0.0685 - val_accuracy: 0.9744\n",
            "Epoch 676/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0735 - accuracy: 0.9769 - val_loss: 0.0740 - val_accuracy: 0.9744\n",
            "Epoch 677/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0743 - accuracy: 0.9769 - val_loss: 0.0730 - val_accuracy: 0.9744\n",
            "Epoch 678/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0737 - accuracy: 0.9782 - val_loss: 0.0673 - val_accuracy: 0.9795\n",
            "Epoch 679/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0735 - accuracy: 0.9782 - val_loss: 0.0681 - val_accuracy: 0.9744\n",
            "Epoch 680/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.0734 - val_accuracy: 0.9744\n",
            "Epoch 681/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0728 - accuracy: 0.9782 - val_loss: 0.0697 - val_accuracy: 0.9744\n",
            "Epoch 682/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0717 - accuracy: 0.9795 - val_loss: 0.0659 - val_accuracy: 0.9795\n",
            "Epoch 683/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0729 - accuracy: 0.9795 - val_loss: 0.0659 - val_accuracy: 0.9795\n",
            "Epoch 684/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0729 - accuracy: 0.9782 - val_loss: 0.0703 - val_accuracy: 0.9744\n",
            "Epoch 685/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0723 - accuracy: 0.9795 - val_loss: 0.0749 - val_accuracy: 0.9744\n",
            "Epoch 686/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0729 - accuracy: 0.9769 - val_loss: 0.0676 - val_accuracy: 0.9744\n",
            "Epoch 687/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0716 - accuracy: 0.9795 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 688/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0739 - accuracy: 0.9795 - val_loss: 0.0677 - val_accuracy: 0.9795\n",
            "Epoch 689/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0717 - accuracy: 0.9808 - val_loss: 0.0734 - val_accuracy: 0.9744\n",
            "Epoch 690/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0721 - accuracy: 0.9782 - val_loss: 0.0695 - val_accuracy: 0.9744\n",
            "Epoch 691/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0710 - accuracy: 0.9782 - val_loss: 0.0668 - val_accuracy: 0.9795\n",
            "Epoch 692/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0720 - accuracy: 0.9782 - val_loss: 0.0675 - val_accuracy: 0.9744\n",
            "Epoch 693/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0725 - accuracy: 0.9782 - val_loss: 0.0680 - val_accuracy: 0.9744\n",
            "Epoch 694/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0712 - accuracy: 0.9782 - val_loss: 0.0665 - val_accuracy: 0.9744\n",
            "Epoch 695/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0710 - accuracy: 0.9782 - val_loss: 0.0696 - val_accuracy: 0.9744\n",
            "Epoch 696/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0705 - accuracy: 0.9795 - val_loss: 0.0732 - val_accuracy: 0.9744\n",
            "Epoch 697/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0714 - accuracy: 0.9782 - val_loss: 0.0694 - val_accuracy: 0.9744\n",
            "Epoch 698/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0716 - accuracy: 0.9782 - val_loss: 0.0693 - val_accuracy: 0.9744\n",
            "Epoch 699/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0714 - accuracy: 0.9756 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
            "Epoch 700/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0706 - accuracy: 0.9782 - val_loss: 0.0665 - val_accuracy: 0.9744\n",
            "Epoch 701/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0716 - accuracy: 0.9808 - val_loss: 0.0665 - val_accuracy: 0.9744\n",
            "Epoch 702/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0695 - accuracy: 0.9795 - val_loss: 0.0747 - val_accuracy: 0.9744\n",
            "Epoch 703/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0716 - accuracy: 0.9756 - val_loss: 0.0714 - val_accuracy: 0.9744\n",
            "Epoch 704/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9808 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 705/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0703 - accuracy: 0.9808 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 706/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0700 - accuracy: 0.9795 - val_loss: 0.0708 - val_accuracy: 0.9744\n",
            "Epoch 707/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0697 - accuracy: 0.9795 - val_loss: 0.0724 - val_accuracy: 0.9744\n",
            "Epoch 708/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9782 - val_loss: 0.0674 - val_accuracy: 0.9795\n",
            "Epoch 709/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0690 - accuracy: 0.9808 - val_loss: 0.0650 - val_accuracy: 0.9846\n",
            "Epoch 710/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0711 - accuracy: 0.9808 - val_loss: 0.0673 - val_accuracy: 0.9744\n",
            "Epoch 711/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0695 - accuracy: 0.9795 - val_loss: 0.0720 - val_accuracy: 0.9744\n",
            "Epoch 712/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0698 - accuracy: 0.9782 - val_loss: 0.0754 - val_accuracy: 0.9744\n",
            "Epoch 713/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9782 - val_loss: 0.0673 - val_accuracy: 0.9744\n",
            "Epoch 714/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0698 - accuracy: 0.9808 - val_loss: 0.0653 - val_accuracy: 0.9795\n",
            "Epoch 715/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0697 - accuracy: 0.9808 - val_loss: 0.0700 - val_accuracy: 0.9744\n",
            "Epoch 716/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0689 - accuracy: 0.9795 - val_loss: 0.0739 - val_accuracy: 0.9744\n",
            "Epoch 717/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0690 - accuracy: 0.9795 - val_loss: 0.0669 - val_accuracy: 0.9795\n",
            "Epoch 718/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0703 - accuracy: 0.9795 - val_loss: 0.0660 - val_accuracy: 0.9795\n",
            "Epoch 719/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0679 - accuracy: 0.9808 - val_loss: 0.0745 - val_accuracy: 0.9744\n",
            "Epoch 720/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0734 - accuracy: 0.9744 - val_loss: 0.0723 - val_accuracy: 0.9744\n",
            "Epoch 721/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9795 - val_loss: 0.0647 - val_accuracy: 0.9846\n",
            "Epoch 722/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0731 - accuracy: 0.9795 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 723/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0704 - accuracy: 0.9782 - val_loss: 0.0740 - val_accuracy: 0.9744\n",
            "Epoch 724/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0696 - accuracy: 0.9769 - val_loss: 0.0749 - val_accuracy: 0.9744\n",
            "Epoch 725/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0684 - accuracy: 0.9756 - val_loss: 0.0671 - val_accuracy: 0.9744\n",
            "Epoch 726/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0684 - accuracy: 0.9808 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
            "Epoch 727/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0693 - accuracy: 0.9795 - val_loss: 0.0707 - val_accuracy: 0.9744\n",
            "Epoch 728/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0683 - accuracy: 0.9808 - val_loss: 0.0694 - val_accuracy: 0.9744\n",
            "Epoch 729/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0696 - accuracy: 0.9808 - val_loss: 0.0674 - val_accuracy: 0.9795\n",
            "Epoch 730/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0677 - accuracy: 0.9795 - val_loss: 0.0719 - val_accuracy: 0.9744\n",
            "Epoch 731/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0668 - accuracy: 0.9795 - val_loss: 0.0695 - val_accuracy: 0.9744\n",
            "Epoch 732/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0663 - accuracy: 0.9808 - val_loss: 0.0680 - val_accuracy: 0.9795\n",
            "Epoch 733/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0675 - accuracy: 0.9821 - val_loss: 0.0694 - val_accuracy: 0.9744\n",
            "Epoch 734/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0666 - accuracy: 0.9808 - val_loss: 0.0708 - val_accuracy: 0.9744\n",
            "Epoch 735/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0663 - accuracy: 0.9795 - val_loss: 0.0706 - val_accuracy: 0.9744\n",
            "Epoch 736/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0661 - accuracy: 0.9808 - val_loss: 0.0707 - val_accuracy: 0.9744\n",
            "Epoch 737/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0658 - accuracy: 0.9808 - val_loss: 0.0697 - val_accuracy: 0.9744\n",
            "Epoch 738/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0658 - accuracy: 0.9808 - val_loss: 0.0699 - val_accuracy: 0.9744\n",
            "Epoch 739/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0658 - accuracy: 0.9808 - val_loss: 0.0703 - val_accuracy: 0.9744\n",
            "Epoch 740/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.0687 - val_accuracy: 0.9744\n",
            "Epoch 741/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.0711 - val_accuracy: 0.9744\n",
            "Epoch 742/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0657 - accuracy: 0.9808 - val_loss: 0.0717 - val_accuracy: 0.9744\n",
            "Epoch 743/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0661 - accuracy: 0.9808 - val_loss: 0.0682 - val_accuracy: 0.9744\n",
            "Epoch 744/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0650 - accuracy: 0.9808 - val_loss: 0.0708 - val_accuracy: 0.9744\n",
            "Epoch 745/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0718 - val_accuracy: 0.9744\n",
            "Epoch 746/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0650 - accuracy: 0.9808 - val_loss: 0.0697 - val_accuracy: 0.9795\n",
            "Epoch 747/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9821 - val_loss: 0.0684 - val_accuracy: 0.9795\n",
            "Epoch 748/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0651 - accuracy: 0.9821 - val_loss: 0.0704 - val_accuracy: 0.9795\n",
            "Epoch 749/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9821 - val_loss: 0.0724 - val_accuracy: 0.9744\n",
            "Epoch 750/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0645 - accuracy: 0.9795 - val_loss: 0.0695 - val_accuracy: 0.9795\n",
            "Epoch 751/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0649 - accuracy: 0.9821 - val_loss: 0.0681 - val_accuracy: 0.9795\n",
            "Epoch 752/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 0.9808 - val_loss: 0.0729 - val_accuracy: 0.9795\n",
            "Epoch 753/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0644 - accuracy: 0.9821 - val_loss: 0.0722 - val_accuracy: 0.9795\n",
            "Epoch 754/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0644 - accuracy: 0.9821 - val_loss: 0.0682 - val_accuracy: 0.9795\n",
            "Epoch 755/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0640 - accuracy: 0.9808 - val_loss: 0.0707 - val_accuracy: 0.9795\n",
            "Epoch 756/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0647 - accuracy: 0.9808 - val_loss: 0.0717 - val_accuracy: 0.9795\n",
            "Epoch 757/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0631 - accuracy: 0.9821 - val_loss: 0.0678 - val_accuracy: 0.9795\n",
            "Epoch 758/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0649 - accuracy: 0.9808 - val_loss: 0.0697 - val_accuracy: 0.9795\n",
            "Epoch 759/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0629 - accuracy: 0.9821 - val_loss: 0.0769 - val_accuracy: 0.9846\n",
            "Epoch 760/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0656 - accuracy: 0.9769 - val_loss: 0.0734 - val_accuracy: 0.9795\n",
            "Epoch 761/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0635 - accuracy: 0.9782 - val_loss: 0.0663 - val_accuracy: 0.9846\n",
            "Epoch 762/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0640 - accuracy: 0.9795 - val_loss: 0.0658 - val_accuracy: 0.9846\n",
            "Epoch 763/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0648 - accuracy: 0.9808 - val_loss: 0.0700 - val_accuracy: 0.9795\n",
            "Epoch 764/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0638 - accuracy: 0.9769 - val_loss: 0.0713 - val_accuracy: 0.9795\n",
            "Epoch 765/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9769 - val_loss: 0.0702 - val_accuracy: 0.9795\n",
            "Epoch 766/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0619 - accuracy: 0.9795 - val_loss: 0.0656 - val_accuracy: 0.9846\n",
            "Epoch 767/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0652 - accuracy: 0.9795 - val_loss: 0.0670 - val_accuracy: 0.9846\n",
            "Epoch 768/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9795 - val_loss: 0.0764 - val_accuracy: 0.9846\n",
            "Epoch 769/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0647 - accuracy: 0.9769 - val_loss: 0.0671 - val_accuracy: 0.9846\n",
            "Epoch 770/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 771/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0613 - accuracy: 0.9821 - val_loss: 0.0743 - val_accuracy: 0.9846\n",
            "Epoch 772/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0642 - accuracy: 0.9769 - val_loss: 0.0714 - val_accuracy: 0.9795\n",
            "Epoch 773/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0614 - accuracy: 0.9795 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 774/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0648 - accuracy: 0.9821 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
            "Epoch 775/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.0747 - val_accuracy: 0.9846\n",
            "Epoch 776/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0636 - accuracy: 0.9795 - val_loss: 0.0711 - val_accuracy: 0.9846\n",
            "Epoch 777/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0618 - accuracy: 0.9782 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 778/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0628 - accuracy: 0.9795 - val_loss: 0.0664 - val_accuracy: 0.9846\n",
            "Epoch 779/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0606 - accuracy: 0.9795 - val_loss: 0.0726 - val_accuracy: 0.9846\n",
            "Epoch 780/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0628 - accuracy: 0.9795 - val_loss: 0.0677 - val_accuracy: 0.9846\n",
            "Epoch 781/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0610 - accuracy: 0.9782 - val_loss: 0.0638 - val_accuracy: 0.9846\n",
            "Epoch 782/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0622 - accuracy: 0.9833 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 783/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0602 - accuracy: 0.9821 - val_loss: 0.0719 - val_accuracy: 0.9846\n",
            "Epoch 784/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0616 - accuracy: 0.9782 - val_loss: 0.0671 - val_accuracy: 0.9846\n",
            "Epoch 785/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0604 - accuracy: 0.9846 - val_loss: 0.0644 - val_accuracy: 0.9846\n",
            "Epoch 786/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9833 - val_loss: 0.0666 - val_accuracy: 0.9846\n",
            "Epoch 787/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0600 - accuracy: 0.9846 - val_loss: 0.0674 - val_accuracy: 0.9846\n",
            "Epoch 788/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9821 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
            "Epoch 789/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9833 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 790/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0595 - accuracy: 0.9833 - val_loss: 0.0654 - val_accuracy: 0.9846\n",
            "Epoch 791/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0597 - accuracy: 0.9821 - val_loss: 0.0672 - val_accuracy: 0.9846\n",
            "Epoch 792/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0604 - accuracy: 0.9846 - val_loss: 0.0670 - val_accuracy: 0.9846\n",
            "Epoch 793/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0689 - val_accuracy: 0.9846\n",
            "Epoch 794/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0593 - accuracy: 0.9808 - val_loss: 0.0667 - val_accuracy: 0.9846\n",
            "Epoch 795/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0587 - accuracy: 0.9859 - val_loss: 0.0658 - val_accuracy: 0.9846\n",
            "Epoch 796/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0587 - accuracy: 0.9833 - val_loss: 0.0669 - val_accuracy: 0.9846\n",
            "Epoch 797/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0588 - accuracy: 0.9846 - val_loss: 0.0666 - val_accuracy: 0.9846\n",
            "Epoch 798/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0600 - accuracy: 0.9821 - val_loss: 0.0664 - val_accuracy: 0.9846\n",
            "Epoch 799/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0595 - accuracy: 0.9808 - val_loss: 0.0714 - val_accuracy: 0.9846\n",
            "Epoch 800/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0589 - accuracy: 0.9846 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
            "Epoch 801/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0598 - accuracy: 0.9821 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 802/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0592 - accuracy: 0.9833 - val_loss: 0.0712 - val_accuracy: 0.9846\n",
            "Epoch 803/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0591 - accuracy: 0.9833 - val_loss: 0.0650 - val_accuracy: 0.9846\n",
            "Epoch 804/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0579 - accuracy: 0.9833 - val_loss: 0.0636 - val_accuracy: 0.9846\n",
            "Epoch 805/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 806/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0586 - accuracy: 0.9808 - val_loss: 0.0712 - val_accuracy: 0.9846\n",
            "Epoch 807/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0582 - accuracy: 0.9833 - val_loss: 0.0643 - val_accuracy: 0.9846\n",
            "Epoch 808/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0578 - accuracy: 0.9846 - val_loss: 0.0630 - val_accuracy: 0.9846\n",
            "Epoch 809/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0589 - accuracy: 0.9833 - val_loss: 0.0675 - val_accuracy: 0.9846\n",
            "Epoch 810/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0590 - accuracy: 0.9821 - val_loss: 0.0726 - val_accuracy: 0.9846\n",
            "Epoch 811/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0612 - accuracy: 0.9808 - val_loss: 0.0642 - val_accuracy: 0.9846\n",
            "Epoch 812/2000\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0577 - accuracy: 0.9846 - val_loss: 0.0657 - val_accuracy: 0.9897\n",
            "Epoch 813/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0577 - accuracy: 0.9833 - val_loss: 0.0698 - val_accuracy: 0.9846\n",
            "Epoch 814/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0578 - accuracy: 0.9846 - val_loss: 0.0642 - val_accuracy: 0.9846\n",
            "Epoch 815/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0570 - accuracy: 0.9846 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 816/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0565 - accuracy: 0.9833 - val_loss: 0.0690 - val_accuracy: 0.9846\n",
            "Epoch 817/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0571 - accuracy: 0.9846 - val_loss: 0.0679 - val_accuracy: 0.9846\n",
            "Epoch 818/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0563 - accuracy: 0.9821 - val_loss: 0.0633 - val_accuracy: 0.9846\n",
            "Epoch 819/2000\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0570 - accuracy: 0.9833 - val_loss: 0.0626 - val_accuracy: 0.9846\n",
            "Epoch 820/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9833 - val_loss: 0.0653 - val_accuracy: 0.9897\n",
            "Epoch 821/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0563 - accuracy: 0.9821 - val_loss: 0.0674 - val_accuracy: 0.9897\n",
            "Epoch 822/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0560 - accuracy: 0.9833 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 823/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0572 - accuracy: 0.9859 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 824/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0562 - accuracy: 0.9846 - val_loss: 0.0708 - val_accuracy: 0.9846\n",
            "Epoch 825/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0575 - accuracy: 0.9833 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 826/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0556 - accuracy: 0.9846 - val_loss: 0.0624 - val_accuracy: 0.9846\n",
            "Epoch 827/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0581 - accuracy: 0.9821 - val_loss: 0.0656 - val_accuracy: 0.9897\n",
            "Epoch 828/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0560 - accuracy: 0.9846 - val_loss: 0.0728 - val_accuracy: 0.9846\n",
            "Epoch 829/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0571 - accuracy: 0.9859 - val_loss: 0.0652 - val_accuracy: 0.9897\n",
            "Epoch 830/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0586 - accuracy: 0.9795 - val_loss: 0.0626 - val_accuracy: 0.9846\n",
            "Epoch 831/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0597 - accuracy: 0.9808 - val_loss: 0.0680 - val_accuracy: 0.9846\n",
            "Epoch 832/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0561 - accuracy: 0.9833 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 833/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0549 - accuracy: 0.9846 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 834/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0655 - val_accuracy: 0.9897\n",
            "Epoch 835/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0653 - val_accuracy: 0.9897\n",
            "Epoch 836/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9846 - val_loss: 0.0659 - val_accuracy: 0.9897\n",
            "Epoch 837/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0546 - accuracy: 0.9846 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 838/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0551 - accuracy: 0.9859 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 839/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0543 - accuracy: 0.9846 - val_loss: 0.0672 - val_accuracy: 0.9846\n",
            "Epoch 840/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0556 - accuracy: 0.9833 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 841/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0558 - accuracy: 0.9833 - val_loss: 0.0619 - val_accuracy: 0.9846\n",
            "Epoch 842/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0549 - accuracy: 0.9846 - val_loss: 0.0677 - val_accuracy: 0.9846\n",
            "Epoch 843/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0549 - accuracy: 0.9846 - val_loss: 0.0707 - val_accuracy: 0.9846\n",
            "Epoch 844/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0557 - accuracy: 0.9846 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 845/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0551 - accuracy: 0.9859 - val_loss: 0.0629 - val_accuracy: 0.9897\n",
            "Epoch 846/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0545 - accuracy: 0.9859 - val_loss: 0.0694 - val_accuracy: 0.9846\n",
            "Epoch 847/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0551 - accuracy: 0.9846 - val_loss: 0.0651 - val_accuracy: 0.9897\n",
            "Epoch 848/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0538 - accuracy: 0.9846 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 849/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0555 - accuracy: 0.9846 - val_loss: 0.0674 - val_accuracy: 0.9846\n",
            "Epoch 850/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0557 - accuracy: 0.9846 - val_loss: 0.0715 - val_accuracy: 0.9846\n",
            "Epoch 851/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0563 - accuracy: 0.9859 - val_loss: 0.0615 - val_accuracy: 0.9897\n",
            "Epoch 852/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0553 - accuracy: 0.9846 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 853/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0544 - accuracy: 0.9846 - val_loss: 0.0685 - val_accuracy: 0.9846\n",
            "Epoch 854/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9859 - val_loss: 0.0653 - val_accuracy: 0.9897\n",
            "Epoch 855/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9859 - val_loss: 0.0653 - val_accuracy: 0.9897\n",
            "Epoch 856/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0540 - accuracy: 0.9859 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 857/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0554 - accuracy: 0.9821 - val_loss: 0.0674 - val_accuracy: 0.9846\n",
            "Epoch 858/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0533 - accuracy: 0.9846 - val_loss: 0.0637 - val_accuracy: 0.9897\n",
            "Epoch 859/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9859 - val_loss: 0.0628 - val_accuracy: 0.9897\n",
            "Epoch 860/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0526 - accuracy: 0.9872 - val_loss: 0.0663 - val_accuracy: 0.9846\n",
            "Epoch 861/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0532 - accuracy: 0.9859 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 862/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0529 - accuracy: 0.9859 - val_loss: 0.0630 - val_accuracy: 0.9897\n",
            "Epoch 863/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0527 - accuracy: 0.9872 - val_loss: 0.0643 - val_accuracy: 0.9897\n",
            "Epoch 864/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0547 - accuracy: 0.9846 - val_loss: 0.0655 - val_accuracy: 0.9897\n",
            "Epoch 865/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0526 - accuracy: 0.9846 - val_loss: 0.0630 - val_accuracy: 0.9897\n",
            "Epoch 866/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0545 - accuracy: 0.9859 - val_loss: 0.0635 - val_accuracy: 0.9897\n",
            "Epoch 867/2000\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.0534 - accuracy: 0.9859 - val_loss: 0.0684 - val_accuracy: 0.9846\n",
            "Epoch 868/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0530 - accuracy: 0.9872 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 869/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0521 - accuracy: 0.9859 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 870/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0522 - accuracy: 0.9859 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 871/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0526 - accuracy: 0.9859 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 872/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0523 - accuracy: 0.9859 - val_loss: 0.0651 - val_accuracy: 0.9846\n",
            "Epoch 873/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0534 - accuracy: 0.9821 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
            "Epoch 874/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0523 - accuracy: 0.9846 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 875/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0527 - accuracy: 0.9859 - val_loss: 0.0645 - val_accuracy: 0.9846\n",
            "Epoch 876/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0515 - accuracy: 0.9859 - val_loss: 0.0677 - val_accuracy: 0.9846\n",
            "Epoch 877/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0520 - accuracy: 0.9859 - val_loss: 0.0656 - val_accuracy: 0.9846\n",
            "Epoch 878/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0522 - accuracy: 0.9872 - val_loss: 0.0619 - val_accuracy: 0.9897\n",
            "Epoch 879/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0515 - accuracy: 0.9859 - val_loss: 0.0650 - val_accuracy: 0.9846\n",
            "Epoch 880/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9872 - val_loss: 0.0667 - val_accuracy: 0.9846\n",
            "Epoch 881/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.0643 - val_accuracy: 0.9897\n",
            "Epoch 882/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0520 - accuracy: 0.9846 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 883/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0508 - accuracy: 0.9859 - val_loss: 0.0692 - val_accuracy: 0.9846\n",
            "Epoch 884/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0518 - accuracy: 0.9846 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 885/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0509 - accuracy: 0.9872 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 886/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0518 - accuracy: 0.9846 - val_loss: 0.0637 - val_accuracy: 0.9897\n",
            "Epoch 887/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0519 - accuracy: 0.9872 - val_loss: 0.0671 - val_accuracy: 0.9846\n",
            "Epoch 888/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9859 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 889/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0514 - accuracy: 0.9846 - val_loss: 0.0634 - val_accuracy: 0.9897\n",
            "Epoch 890/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 891/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0505 - accuracy: 0.9872 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 892/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0499 - accuracy: 0.9859 - val_loss: 0.0626 - val_accuracy: 0.9897\n",
            "Epoch 893/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0511 - accuracy: 0.9846 - val_loss: 0.0623 - val_accuracy: 0.9897\n",
            "Epoch 894/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0503 - accuracy: 0.9872 - val_loss: 0.0612 - val_accuracy: 0.9897\n",
            "Epoch 895/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0507 - accuracy: 0.9872 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 896/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0504 - accuracy: 0.9872 - val_loss: 0.0671 - val_accuracy: 0.9846\n",
            "Epoch 897/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0522 - accuracy: 0.9846 - val_loss: 0.0656 - val_accuracy: 0.9846\n",
            "Epoch 898/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9859 - val_loss: 0.0601 - val_accuracy: 0.9897\n",
            "Epoch 899/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0527 - accuracy: 0.9859 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 900/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0513 - accuracy: 0.9885 - val_loss: 0.0744 - val_accuracy: 0.9846\n",
            "Epoch 901/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0534 - accuracy: 0.9833 - val_loss: 0.0597 - val_accuracy: 0.9897\n",
            "Epoch 902/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0531 - accuracy: 0.9846 - val_loss: 0.0596 - val_accuracy: 0.9897\n",
            "Epoch 903/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0520 - accuracy: 0.9859 - val_loss: 0.0681 - val_accuracy: 0.9846\n",
            "Epoch 904/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0512 - accuracy: 0.9846 - val_loss: 0.0703 - val_accuracy: 0.9846\n",
            "Epoch 905/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0524 - accuracy: 0.9846 - val_loss: 0.0628 - val_accuracy: 0.9897\n",
            "Epoch 906/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0492 - accuracy: 0.9872 - val_loss: 0.0617 - val_accuracy: 0.9897\n",
            "Epoch 907/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0506 - accuracy: 0.9859 - val_loss: 0.0626 - val_accuracy: 0.9897\n",
            "Epoch 908/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0498 - accuracy: 0.9872 - val_loss: 0.0681 - val_accuracy: 0.9846\n",
            "Epoch 909/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0501 - accuracy: 0.9872 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
            "Epoch 910/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0493 - accuracy: 0.9872 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 911/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.9859 - val_loss: 0.0666 - val_accuracy: 0.9846\n",
            "Epoch 912/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0499 - accuracy: 0.9846 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 913/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0501 - accuracy: 0.9872 - val_loss: 0.0638 - val_accuracy: 0.9897\n",
            "Epoch 914/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0498 - accuracy: 0.9859 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 915/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0495 - accuracy: 0.9885 - val_loss: 0.0659 - val_accuracy: 0.9846\n",
            "Epoch 916/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0495 - accuracy: 0.9859 - val_loss: 0.0640 - val_accuracy: 0.9897\n",
            "Epoch 917/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9885 - val_loss: 0.0629 - val_accuracy: 0.9897\n",
            "Epoch 918/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9872 - val_loss: 0.0626 - val_accuracy: 0.9897\n",
            "Epoch 919/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0488 - accuracy: 0.9872 - val_loss: 0.0624 - val_accuracy: 0.9897\n",
            "Epoch 920/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0494 - accuracy: 0.9872 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 921/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9885 - val_loss: 0.0694 - val_accuracy: 0.9846\n",
            "Epoch 922/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0498 - accuracy: 0.9846 - val_loss: 0.0645 - val_accuracy: 0.9846\n",
            "Epoch 923/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0479 - accuracy: 0.9872 - val_loss: 0.0599 - val_accuracy: 0.9897\n",
            "Epoch 924/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0511 - accuracy: 0.9859 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 925/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0497 - accuracy: 0.9846 - val_loss: 0.0734 - val_accuracy: 0.9846\n",
            "Epoch 926/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0507 - accuracy: 0.9846 - val_loss: 0.0610 - val_accuracy: 0.9897\n",
            "Epoch 927/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0497 - accuracy: 0.9872 - val_loss: 0.0594 - val_accuracy: 0.9897\n",
            "Epoch 928/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0500 - accuracy: 0.9859 - val_loss: 0.0670 - val_accuracy: 0.9846\n",
            "Epoch 929/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0522 - accuracy: 0.9833 - val_loss: 0.0673 - val_accuracy: 0.9846\n",
            "Epoch 930/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0502 - accuracy: 0.9846 - val_loss: 0.0601 - val_accuracy: 0.9897\n",
            "Epoch 931/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0516 - accuracy: 0.9859 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 932/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9897 - val_loss: 0.0727 - val_accuracy: 0.9846\n",
            "Epoch 933/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0510 - accuracy: 0.9846 - val_loss: 0.0633 - val_accuracy: 0.9897\n",
            "Epoch 934/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0484 - accuracy: 0.9846 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 935/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0478 - accuracy: 0.9859 - val_loss: 0.0653 - val_accuracy: 0.9846\n",
            "Epoch 936/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0480 - accuracy: 0.9859 - val_loss: 0.0661 - val_accuracy: 0.9846\n",
            "Epoch 937/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0483 - accuracy: 0.9859 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 938/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9859 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 939/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9872 - val_loss: 0.0643 - val_accuracy: 0.9846\n",
            "Epoch 940/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0473 - accuracy: 0.9872 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 941/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0472 - accuracy: 0.9885 - val_loss: 0.0603 - val_accuracy: 0.9897\n",
            "Epoch 942/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0477 - accuracy: 0.9859 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 943/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0476 - accuracy: 0.9859 - val_loss: 0.0637 - val_accuracy: 0.9897\n",
            "Epoch 944/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0468 - accuracy: 0.9885 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 945/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0472 - accuracy: 0.9872 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 946/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9859 - val_loss: 0.0639 - val_accuracy: 0.9897\n",
            "Epoch 947/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9872 - val_loss: 0.0613 - val_accuracy: 0.9897\n",
            "Epoch 948/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0481 - accuracy: 0.9872 - val_loss: 0.0617 - val_accuracy: 0.9897\n",
            "Epoch 949/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0467 - accuracy: 0.9872 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 950/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0477 - accuracy: 0.9846 - val_loss: 0.0635 - val_accuracy: 0.9897\n",
            "Epoch 951/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0475 - accuracy: 0.9846 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
            "Epoch 952/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0467 - accuracy: 0.9846 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 953/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0463 - accuracy: 0.9885 - val_loss: 0.0630 - val_accuracy: 0.9897\n",
            "Epoch 954/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0465 - accuracy: 0.9872 - val_loss: 0.0637 - val_accuracy: 0.9897\n",
            "Epoch 955/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0463 - accuracy: 0.9885 - val_loss: 0.0628 - val_accuracy: 0.9897\n",
            "Epoch 956/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9885 - val_loss: 0.0638 - val_accuracy: 0.9846\n",
            "Epoch 957/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0466 - accuracy: 0.9872 - val_loss: 0.0646 - val_accuracy: 0.9846\n",
            "Epoch 958/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 0.0616 - val_accuracy: 0.9897\n",
            "Epoch 959/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9872 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 960/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9885 - val_loss: 0.0657 - val_accuracy: 0.9846\n",
            "Epoch 961/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0664 - val_accuracy: 0.9846\n",
            "Epoch 962/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0458 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 963/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 964/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0458 - accuracy: 0.9885 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 965/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9859 - val_loss: 0.0643 - val_accuracy: 0.9897\n",
            "Epoch 966/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0460 - accuracy: 0.9885 - val_loss: 0.0621 - val_accuracy: 0.9897\n",
            "Epoch 967/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0456 - accuracy: 0.9885 - val_loss: 0.0621 - val_accuracy: 0.9897\n",
            "Epoch 968/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9872 - val_loss: 0.0630 - val_accuracy: 0.9897\n",
            "Epoch 969/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0456 - accuracy: 0.9897 - val_loss: 0.0633 - val_accuracy: 0.9897\n",
            "Epoch 970/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0454 - accuracy: 0.9897 - val_loss: 0.0649 - val_accuracy: 0.9897\n",
            "Epoch 971/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9859 - val_loss: 0.0658 - val_accuracy: 0.9846\n",
            "Epoch 972/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0457 - accuracy: 0.9846 - val_loss: 0.0638 - val_accuracy: 0.9897\n",
            "Epoch 973/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9846 - val_loss: 0.0604 - val_accuracy: 0.9897\n",
            "Epoch 974/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0461 - accuracy: 0.9872 - val_loss: 0.0590 - val_accuracy: 0.9897\n",
            "Epoch 975/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0469 - accuracy: 0.9872 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 976/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9846 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 977/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0447 - accuracy: 0.9885 - val_loss: 0.0609 - val_accuracy: 0.9897\n",
            "Epoch 978/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0463 - accuracy: 0.9846 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 979/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9885 - val_loss: 0.0699 - val_accuracy: 0.9846\n",
            "Epoch 980/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0466 - accuracy: 0.9859 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 981/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0457 - accuracy: 0.9885 - val_loss: 0.0594 - val_accuracy: 0.9897\n",
            "Epoch 982/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0459 - accuracy: 0.9872 - val_loss: 0.0652 - val_accuracy: 0.9846\n",
            "Epoch 983/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0465 - accuracy: 0.9872 - val_loss: 0.0693 - val_accuracy: 0.9846\n",
            "Epoch 984/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0462 - accuracy: 0.9859 - val_loss: 0.0589 - val_accuracy: 0.9897\n",
            "Epoch 985/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0462 - accuracy: 0.9872 - val_loss: 0.0583 - val_accuracy: 0.9897\n",
            "Epoch 986/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0458 - accuracy: 0.9859 - val_loss: 0.0675 - val_accuracy: 0.9846\n",
            "Epoch 987/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9846 - val_loss: 0.0736 - val_accuracy: 0.9795\n",
            "Epoch 988/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0477 - accuracy: 0.9846 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 989/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0461 - accuracy: 0.9859 - val_loss: 0.0590 - val_accuracy: 0.9897\n",
            "Epoch 990/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0459 - accuracy: 0.9872 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 991/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0459 - accuracy: 0.9859 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 992/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0458 - accuracy: 0.9859 - val_loss: 0.0598 - val_accuracy: 0.9897\n",
            "Epoch 993/2000\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0453 - accuracy: 0.9885 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 994/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0455 - accuracy: 0.9846 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 995/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0443 - accuracy: 0.9846 - val_loss: 0.0600 - val_accuracy: 0.9897\n",
            "Epoch 996/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0456 - accuracy: 0.9872 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 997/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0441 - accuracy: 0.9859 - val_loss: 0.0673 - val_accuracy: 0.9897\n",
            "Epoch 998/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0460 - accuracy: 0.9846 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 999/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0607 - val_accuracy: 0.9897\n",
            "Epoch 1000/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0440 - accuracy: 0.9885 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 1001/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0444 - accuracy: 0.9872 - val_loss: 0.0671 - val_accuracy: 0.9846\n",
            "Epoch 1002/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 0.0635 - val_accuracy: 0.9897\n",
            "Epoch 1003/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0442 - accuracy: 0.9885 - val_loss: 0.0604 - val_accuracy: 0.9897\n",
            "Epoch 1004/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0439 - accuracy: 0.9859 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1005/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0437 - accuracy: 0.9885 - val_loss: 0.0621 - val_accuracy: 0.9897\n",
            "Epoch 1006/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0435 - accuracy: 0.9897 - val_loss: 0.0616 - val_accuracy: 0.9897\n",
            "Epoch 1007/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0435 - accuracy: 0.9897 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 1008/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0668 - val_accuracy: 0.9846\n",
            "Epoch 1009/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0447 - accuracy: 0.9859 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 1010/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0442 - accuracy: 0.9885 - val_loss: 0.0586 - val_accuracy: 0.9897\n",
            "Epoch 1011/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0457 - accuracy: 0.9859 - val_loss: 0.0636 - val_accuracy: 0.9897\n",
            "Epoch 1012/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0465 - accuracy: 0.9846 - val_loss: 0.0677 - val_accuracy: 0.9846\n",
            "Epoch 1013/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0448 - accuracy: 0.9846 - val_loss: 0.0606 - val_accuracy: 0.9897\n",
            "Epoch 1014/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0440 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1015/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0434 - accuracy: 0.9885 - val_loss: 0.0644 - val_accuracy: 0.9897\n",
            "Epoch 1016/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9859 - val_loss: 0.0647 - val_accuracy: 0.9897\n",
            "Epoch 1017/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0425 - accuracy: 0.9885 - val_loss: 0.0603 - val_accuracy: 0.9897\n",
            "Epoch 1018/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9859 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 1019/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0439 - accuracy: 0.9897 - val_loss: 0.0686 - val_accuracy: 0.9846\n",
            "Epoch 1020/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0448 - accuracy: 0.9833 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 1021/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0434 - accuracy: 0.9897 - val_loss: 0.0607 - val_accuracy: 0.9897\n",
            "Epoch 1022/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0432 - accuracy: 0.9885 - val_loss: 0.0655 - val_accuracy: 0.9897\n",
            "Epoch 1023/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0432 - accuracy: 0.9859 - val_loss: 0.0663 - val_accuracy: 0.9897\n",
            "Epoch 1024/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0428 - accuracy: 0.9872 - val_loss: 0.0588 - val_accuracy: 0.9897\n",
            "Epoch 1025/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.0584 - val_accuracy: 0.9897\n",
            "Epoch 1026/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0442 - accuracy: 0.9846 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 1027/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0441 - accuracy: 0.9846 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 1028/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0429 - accuracy: 0.9872 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 1029/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0424 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1030/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.9897 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1031/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9872 - val_loss: 0.0628 - val_accuracy: 0.9897\n",
            "Epoch 1032/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0417 - accuracy: 0.9885 - val_loss: 0.0596 - val_accuracy: 0.9897\n",
            "Epoch 1033/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0427 - accuracy: 0.9885 - val_loss: 0.0603 - val_accuracy: 0.9897\n",
            "Epoch 1034/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0418 - accuracy: 0.9897 - val_loss: 0.0657 - val_accuracy: 0.9897\n",
            "Epoch 1035/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0431 - accuracy: 0.9859 - val_loss: 0.0636 - val_accuracy: 0.9897\n",
            "Epoch 1036/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0595 - val_accuracy: 0.9897\n",
            "Epoch 1037/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.0616 - val_accuracy: 0.9897\n",
            "Epoch 1038/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0428 - accuracy: 0.9872 - val_loss: 0.0660 - val_accuracy: 0.9897\n",
            "Epoch 1039/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0430 - accuracy: 0.9859 - val_loss: 0.0655 - val_accuracy: 0.9897\n",
            "Epoch 1040/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0419 - accuracy: 0.9859 - val_loss: 0.0646 - val_accuracy: 0.9897\n",
            "Epoch 1041/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.9846 - val_loss: 0.0607 - val_accuracy: 0.9897\n",
            "Epoch 1042/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9910 - val_loss: 0.0581 - val_accuracy: 0.9897\n",
            "Epoch 1043/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 1044/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0449 - accuracy: 0.9833 - val_loss: 0.0657 - val_accuracy: 0.9897\n",
            "Epoch 1045/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 0.9885 - val_loss: 0.0590 - val_accuracy: 0.9897\n",
            "Epoch 1046/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0447 - accuracy: 0.9872 - val_loss: 0.0599 - val_accuracy: 0.9897\n",
            "Epoch 1047/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0429 - accuracy: 0.9885 - val_loss: 0.0665 - val_accuracy: 0.9846\n",
            "Epoch 1048/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0428 - accuracy: 0.9859 - val_loss: 0.0652 - val_accuracy: 0.9897\n",
            "Epoch 1049/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9885 - val_loss: 0.0600 - val_accuracy: 0.9897\n",
            "Epoch 1050/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0428 - accuracy: 0.9872 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1051/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0439 - accuracy: 0.9885 - val_loss: 0.0674 - val_accuracy: 0.9846\n",
            "Epoch 1052/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0431 - accuracy: 0.9846 - val_loss: 0.0601 - val_accuracy: 0.9897\n",
            "Epoch 1053/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0417 - accuracy: 0.9885 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 1054/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9872 - val_loss: 0.0662 - val_accuracy: 0.9897\n",
            "Epoch 1055/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0414 - accuracy: 0.9859 - val_loss: 0.0623 - val_accuracy: 0.9897\n",
            "Epoch 1056/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0428 - accuracy: 0.9872 - val_loss: 0.0609 - val_accuracy: 0.9897\n",
            "Epoch 1057/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0422 - accuracy: 0.9859 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 1058/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0419 - accuracy: 0.9859 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 1059/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9885 - val_loss: 0.0578 - val_accuracy: 0.9897\n",
            "Epoch 1060/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0416 - accuracy: 0.9872 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 1061/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 0.0663 - val_accuracy: 0.9897\n",
            "Epoch 1062/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0421 - accuracy: 0.9859 - val_loss: 0.0598 - val_accuracy: 0.9897\n",
            "Epoch 1063/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.0626 - val_accuracy: 0.9897\n",
            "Epoch 1064/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.0689 - val_accuracy: 0.9795\n",
            "Epoch 1065/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 1066/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0401 - accuracy: 0.9885 - val_loss: 0.0604 - val_accuracy: 0.9897\n",
            "Epoch 1067/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0408 - accuracy: 0.9897 - val_loss: 0.0604 - val_accuracy: 0.9897\n",
            "Epoch 1068/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0412 - accuracy: 0.9872 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 1069/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.9859 - val_loss: 0.0603 - val_accuracy: 0.9897\n",
            "Epoch 1070/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0405 - accuracy: 0.9897 - val_loss: 0.0594 - val_accuracy: 0.9897\n",
            "Epoch 1071/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0415 - accuracy: 0.9897 - val_loss: 0.0605 - val_accuracy: 0.9897\n",
            "Epoch 1072/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0415 - accuracy: 0.9885 - val_loss: 0.0661 - val_accuracy: 0.9897\n",
            "Epoch 1073/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0407 - accuracy: 0.9859 - val_loss: 0.0634 - val_accuracy: 0.9897\n",
            "Epoch 1074/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0400 - accuracy: 0.9897 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 1075/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0403 - accuracy: 0.9897 - val_loss: 0.0613 - val_accuracy: 0.9897\n",
            "Epoch 1076/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0403 - accuracy: 0.9872 - val_loss: 0.0646 - val_accuracy: 0.9897\n",
            "Epoch 1077/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0403 - accuracy: 0.9859 - val_loss: 0.0621 - val_accuracy: 0.9897\n",
            "Epoch 1078/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0406 - accuracy: 0.9897 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1079/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0411 - accuracy: 0.9872 - val_loss: 0.0658 - val_accuracy: 0.9897\n",
            "Epoch 1080/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 0.0623 - val_accuracy: 0.9897\n",
            "Epoch 1081/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9897 - val_loss: 0.0609 - val_accuracy: 0.9897\n",
            "Epoch 1082/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0406 - accuracy: 0.9897 - val_loss: 0.0613 - val_accuracy: 0.9897\n",
            "Epoch 1083/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0400 - accuracy: 0.9885 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 1084/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0396 - accuracy: 0.9885 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 1085/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 1086/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0393 - accuracy: 0.9897 - val_loss: 0.0631 - val_accuracy: 0.9897\n",
            "Epoch 1087/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0407 - accuracy: 0.9897 - val_loss: 0.0660 - val_accuracy: 0.9897\n",
            "Epoch 1088/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 1089/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0416 - accuracy: 0.9859 - val_loss: 0.0585 - val_accuracy: 0.9897\n",
            "Epoch 1090/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9872 - val_loss: 0.0621 - val_accuracy: 0.9897\n",
            "Epoch 1091/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9885 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 1092/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.0603 - val_accuracy: 0.9897\n",
            "Epoch 1093/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0403 - accuracy: 0.9885 - val_loss: 0.0612 - val_accuracy: 0.9897\n",
            "Epoch 1094/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0394 - accuracy: 0.9897 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 1095/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0392 - accuracy: 0.9872 - val_loss: 0.0635 - val_accuracy: 0.9897\n",
            "Epoch 1096/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0399 - accuracy: 0.9859 - val_loss: 0.0593 - val_accuracy: 0.9897\n",
            "Epoch 1097/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0404 - accuracy: 0.9872 - val_loss: 0.0578 - val_accuracy: 0.9897\n",
            "Epoch 1098/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0421 - accuracy: 0.9872 - val_loss: 0.0628 - val_accuracy: 0.9897\n",
            "Epoch 1099/2000\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0400 - accuracy: 0.9897 - val_loss: 0.0678 - val_accuracy: 0.9846\n",
            "Epoch 1100/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0396 - accuracy: 0.9872 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 1101/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0409 - accuracy: 0.9885 - val_loss: 0.0602 - val_accuracy: 0.9897\n",
            "Epoch 1102/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0395 - accuracy: 0.9885 - val_loss: 0.0665 - val_accuracy: 0.9846\n",
            "Epoch 1103/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0401 - accuracy: 0.9846 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 1104/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 1105/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.9897 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 1106/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0388 - accuracy: 0.9885 - val_loss: 0.0624 - val_accuracy: 0.9897\n",
            "Epoch 1107/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0387 - accuracy: 0.9897 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 1108/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0393 - accuracy: 0.9897 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 1109/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9885 - val_loss: 0.0624 - val_accuracy: 0.9897\n",
            "Epoch 1110/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0387 - accuracy: 0.9885 - val_loss: 0.0589 - val_accuracy: 0.9897\n",
            "Epoch 1111/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0403 - accuracy: 0.9885 - val_loss: 0.0582 - val_accuracy: 0.9897\n",
            "Epoch 1112/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.9872 - val_loss: 0.0660 - val_accuracy: 0.9846\n",
            "Epoch 1113/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 1114/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 0.0604 - val_accuracy: 0.9897\n",
            "Epoch 1115/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0397 - accuracy: 0.9897 - val_loss: 0.0607 - val_accuracy: 0.9897\n",
            "Epoch 1116/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0381 - accuracy: 0.9910 - val_loss: 0.0655 - val_accuracy: 0.9846\n",
            "Epoch 1117/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0415 - accuracy: 0.9846 - val_loss: 0.0640 - val_accuracy: 0.9897\n",
            "Epoch 1118/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0403 - accuracy: 0.9859 - val_loss: 0.0601 - val_accuracy: 0.9897\n",
            "Epoch 1119/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0401 - accuracy: 0.9897 - val_loss: 0.0654 - val_accuracy: 0.9897\n",
            "Epoch 1120/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0407 - accuracy: 0.9846 - val_loss: 0.0697 - val_accuracy: 0.9846\n",
            "Epoch 1121/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0390 - accuracy: 0.9859 - val_loss: 0.0598 - val_accuracy: 0.9897\n",
            "Epoch 1122/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 0.0596 - val_accuracy: 0.9897\n",
            "Epoch 1123/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0420 - accuracy: 0.9859 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 1124/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0398 - accuracy: 0.9885 - val_loss: 0.0647 - val_accuracy: 0.9897\n",
            "Epoch 1125/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9872 - val_loss: 0.0591 - val_accuracy: 0.9897\n",
            "Epoch 1126/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0391 - accuracy: 0.9885 - val_loss: 0.0597 - val_accuracy: 0.9897\n",
            "Epoch 1127/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0378 - accuracy: 0.9885 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 1128/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0393 - accuracy: 0.9859 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 1129/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9885 - val_loss: 0.0608 - val_accuracy: 0.9897\n",
            "Epoch 1130/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0400 - accuracy: 0.9872 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 1131/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0379 - accuracy: 0.9897 - val_loss: 0.0694 - val_accuracy: 0.9846\n",
            "Epoch 1132/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0400 - accuracy: 0.9859 - val_loss: 0.0669 - val_accuracy: 0.9846\n",
            "Epoch 1133/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0382 - accuracy: 0.9885 - val_loss: 0.0589 - val_accuracy: 0.9897\n",
            "Epoch 1134/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0594 - val_accuracy: 0.9897\n",
            "Epoch 1135/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0381 - accuracy: 0.9885 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 1136/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0386 - accuracy: 0.9897 - val_loss: 0.0639 - val_accuracy: 0.9897\n",
            "Epoch 1137/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9897 - val_loss: 0.0617 - val_accuracy: 0.9897\n",
            "Epoch 1138/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9897 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 1139/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0371 - accuracy: 0.9897 - val_loss: 0.0650 - val_accuracy: 0.9897\n",
            "Epoch 1140/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0378 - accuracy: 0.9859 - val_loss: 0.0643 - val_accuracy: 0.9897\n",
            "Epoch 1141/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9872 - val_loss: 0.0616 - val_accuracy: 0.9897\n",
            "Epoch 1142/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9910 - val_loss: 0.0609 - val_accuracy: 0.9897\n",
            "Epoch 1143/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0379 - accuracy: 0.9897 - val_loss: 0.0640 - val_accuracy: 0.9897\n",
            "Epoch 1144/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0369 - accuracy: 0.9885 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1145/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0377 - accuracy: 0.9872 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 1146/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0625 - val_accuracy: 0.9897\n",
            "Epoch 1147/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9897 - val_loss: 0.0636 - val_accuracy: 0.9897\n",
            "Epoch 1148/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0377 - accuracy: 0.9885 - val_loss: 0.0633 - val_accuracy: 0.9897\n",
            "Epoch 1149/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0370 - accuracy: 0.9885 - val_loss: 0.0663 - val_accuracy: 0.9846\n",
            "Epoch 1150/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9897 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 1151/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0371 - accuracy: 0.9897 - val_loss: 0.0638 - val_accuracy: 0.9897\n",
            "Epoch 1152/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9910 - val_loss: 0.0662 - val_accuracy: 0.9846\n",
            "Epoch 1153/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0380 - accuracy: 0.9846 - val_loss: 0.0606 - val_accuracy: 0.9897\n",
            "Epoch 1154/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0380 - accuracy: 0.9910 - val_loss: 0.0590 - val_accuracy: 0.9897\n",
            "Epoch 1155/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0395 - accuracy: 0.9872 - val_loss: 0.0638 - val_accuracy: 0.9897\n",
            "Epoch 1156/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0368 - accuracy: 0.9897 - val_loss: 0.0656 - val_accuracy: 0.9846\n",
            "Epoch 1157/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.0617 - val_accuracy: 0.9897\n",
            "Epoch 1158/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9885 - val_loss: 0.0590 - val_accuracy: 0.9897\n",
            "Epoch 1159/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0381 - accuracy: 0.9872 - val_loss: 0.0614 - val_accuracy: 0.9897\n",
            "Epoch 1160/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0369 - accuracy: 0.9897 - val_loss: 0.0693 - val_accuracy: 0.9795\n",
            "Epoch 1161/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0378 - accuracy: 0.9859 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 1162/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0367 - accuracy: 0.9897 - val_loss: 0.0609 - val_accuracy: 0.9897\n",
            "Epoch 1163/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9885 - val_loss: 0.0636 - val_accuracy: 0.9897\n",
            "Epoch 1164/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0366 - accuracy: 0.9885 - val_loss: 0.0667 - val_accuracy: 0.9846\n",
            "Epoch 1165/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0372 - accuracy: 0.9859 - val_loss: 0.0600 - val_accuracy: 0.9897\n",
            "Epoch 1166/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0391 - accuracy: 0.9885 - val_loss: 0.0600 - val_accuracy: 0.9897\n",
            "Epoch 1167/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0386 - accuracy: 0.9859 - val_loss: 0.0689 - val_accuracy: 0.9846\n",
            "Epoch 1168/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0374 - accuracy: 0.9859 - val_loss: 0.0648 - val_accuracy: 0.9897\n",
            "Epoch 1169/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0360 - accuracy: 0.9897 - val_loss: 0.0611 - val_accuracy: 0.9897\n",
            "Epoch 1170/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0368 - accuracy: 0.9910 - val_loss: 0.0630 - val_accuracy: 0.9897\n",
            "Epoch 1171/2000\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0363 - accuracy: 0.9872 - val_loss: 0.0640 - val_accuracy: 0.9897\n",
            "Epoch 1172/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0358 - accuracy: 0.9897 - val_loss: 0.0620 - val_accuracy: 0.9897\n",
            "Epoch 1173/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0373 - accuracy: 0.9910 - val_loss: 0.0622 - val_accuracy: 0.9897\n",
            "Epoch 1174/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0368 - accuracy: 0.9910 - val_loss: 0.0679 - val_accuracy: 0.9846\n",
            "Epoch 1175/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0389 - accuracy: 0.9859 - val_loss: 0.0645 - val_accuracy: 0.9897\n",
            "Epoch 1176/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0373 - accuracy: 0.9897 - val_loss: 0.0582 - val_accuracy: 0.9897\n",
            "Epoch 1177/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0381 - accuracy: 0.9872 - val_loss: 0.0617 - val_accuracy: 0.9897\n",
            "Epoch 1178/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0362 - accuracy: 0.9872 - val_loss: 0.0688 - val_accuracy: 0.9846\n",
            "Epoch 1179/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0371 - accuracy: 0.9872 - val_loss: 0.0635 - val_accuracy: 0.9897\n",
            "Epoch 1180/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0359 - accuracy: 0.9910 - val_loss: 0.0639 - val_accuracy: 0.9897\n",
            "Epoch 1181/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0364 - accuracy: 0.9897 - val_loss: 0.0656 - val_accuracy: 0.9897\n",
            "Epoch 1182/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0363 - accuracy: 0.9885 - val_loss: 0.0641 - val_accuracy: 0.9897\n",
            "Epoch 1183/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0357 - accuracy: 0.9897 - val_loss: 0.0647 - val_accuracy: 0.9897\n",
            "Epoch 1184/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 0.0601 - val_accuracy: 0.9897\n",
            "Epoch 1185/2000\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0365 - accuracy: 0.9885 - val_loss: 0.0615 - val_accuracy: 0.9897\n",
            "Epoch 1186/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0361 - accuracy: 0.9885 - val_loss: 0.0677 - val_accuracy: 0.9846\n",
            "Epoch 1187/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0372 - accuracy: 0.9872 - val_loss: 0.0658 - val_accuracy: 0.9897\n",
            "Epoch 1188/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0353 - accuracy: 0.9910 - val_loss: 0.0633 - val_accuracy: 0.9897\n",
            "Epoch 1189/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.9897 - val_loss: 0.0627 - val_accuracy: 0.9897\n",
            "Epoch 1190/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0364 - accuracy: 0.9897 - val_loss: 0.0632 - val_accuracy: 0.9897\n",
            "Epoch 1191/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0670 - val_accuracy: 0.9846\n",
            "Epoch 1192/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0375 - accuracy: 0.9846 - val_loss: 0.0616 - val_accuracy: 0.9897\n",
            "Epoch 1193/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0345 - accuracy: 0.9910 - val_loss: 0.0598 - val_accuracy: 0.9897\n",
            "Epoch 1194/2000\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0402 - accuracy: 0.9846 - val_loss: 0.0634 - val_accuracy: 0.9897\n",
            "Epoch 1195/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0352 - accuracy: 0.9885 - val_loss: 0.0750 - val_accuracy: 0.9795\n",
            "Epoch 1196/2000\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0389 - accuracy: 0.9872 - val_loss: 0.0619 - val_accuracy: 0.9897\n",
            "Epoch 1197/2000\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0349 - accuracy: 0.9885 - val_loss: 0.0586 - val_accuracy: 0.9897\n",
            "31/31 [==============================] - 0s 795us/step - loss: 0.0434 - accuracy: 0.9867\n",
            "\n",
            " Accuracy: 0.9867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEWbiQD9mjUn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "131c937c-c53c-4e0a-803b-c6025955b99c"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "#seed값 설정\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)\n",
        "\n",
        "#데이터 불러오기 및 피처(속성)/타겟(클래스) 분리\n",
        "df_pre = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/wine.csv', header=None)\n",
        "df = df_pre.sample(frac = 0.15)\n",
        "\n",
        "dataset = df.values\n",
        "X = dataset[:, 0:12]\n",
        "Y = dataset[:, 12]\n",
        "\n",
        "# 모델설정\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim = 12, activation='relu'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 모델컴파일\n",
        "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
        "\n",
        "# 모델 저장 폴더 설정\n",
        "MODEL_DIR = './model/'\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "# 모델 저장 조건 설정\n",
        "modelpath = './model/{epoch:02d} - {val_loss:.4f}.hdf5'\n",
        "checkpointer = ModelCheckpoint(filepath = modelpath, monitor = 'val_loss', verbose = 1, save_best_only = True)\n",
        "\n",
        "# 학습자동 중단 설정\n",
        "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 100)\n",
        "\n",
        "model.fit(X, Y, validation_split=0.2, epochs=3500, batch_size=500, verbose = 0, callbacks=[early_stopping_callback, checkpointer])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.20689, saving model to ./model/01 - 1.2069.hdf5\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.20689 to 0.86037, saving model to ./model/02 - 0.8604.hdf5\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86037 to 0.57772, saving model to ./model/03 - 0.5777.hdf5\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.57772\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.57772\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.57772 to 0.53607, saving model to ./model/06 - 0.5361.hdf5\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.53607 to 0.44392, saving model to ./model/07 - 0.4439.hdf5\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.44392 to 0.44263, saving model to ./model/08 - 0.4426.hdf5\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.44263 to 0.43578, saving model to ./model/09 - 0.4358.hdf5\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.43578 to 0.41058, saving model to ./model/10 - 0.4106.hdf5\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.41058 to 0.37877, saving model to ./model/11 - 0.3788.hdf5\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.37877 to 0.36487, saving model to ./model/12 - 0.3649.hdf5\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.36487\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.36487\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.36487\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.36487 to 0.35215, saving model to ./model/16 - 0.3522.hdf5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.35215 to 0.31528, saving model to ./model/17 - 0.3153.hdf5\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.31528 to 0.29911, saving model to ./model/18 - 0.2991.hdf5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.29911 to 0.28986, saving model to ./model/19 - 0.2899.hdf5\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.28986 to 0.28323, saving model to ./model/20 - 0.2832.hdf5\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.28323 to 0.28166, saving model to ./model/21 - 0.2817.hdf5\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.28166\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.28166\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.28166\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.28166\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.28166 to 0.26935, saving model to ./model/26 - 0.2693.hdf5\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.26935 to 0.25668, saving model to ./model/27 - 0.2567.hdf5\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.25668 to 0.24824, saving model to ./model/28 - 0.2482.hdf5\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.24824 to 0.24302, saving model to ./model/29 - 0.2430.hdf5\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.24302 to 0.23980, saving model to ./model/30 - 0.2398.hdf5\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.23980 to 0.23846, saving model to ./model/31 - 0.2385.hdf5\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.23846\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.23846\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.23846 to 0.23710, saving model to ./model/34 - 0.2371.hdf5\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.23710 to 0.23291, saving model to ./model/35 - 0.2329.hdf5\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.23291 to 0.22793, saving model to ./model/36 - 0.2279.hdf5\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.22793 to 0.22266, saving model to ./model/37 - 0.2227.hdf5\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.22266 to 0.21854, saving model to ./model/38 - 0.2185.hdf5\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.21854 to 0.21621, saving model to ./model/39 - 0.2162.hdf5\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.21621 to 0.21398, saving model to ./model/40 - 0.2140.hdf5\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.21398 to 0.21335, saving model to ./model/41 - 0.2133.hdf5\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.21335 to 0.21262, saving model to ./model/42 - 0.2126.hdf5\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.21262 to 0.21169, saving model to ./model/43 - 0.2117.hdf5\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.21169 to 0.20995, saving model to ./model/44 - 0.2100.hdf5\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.20995 to 0.20813, saving model to ./model/45 - 0.2081.hdf5\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.20813 to 0.20665, saving model to ./model/46 - 0.2066.hdf5\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.20665 to 0.20257, saving model to ./model/47 - 0.2026.hdf5\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.20257 to 0.19806, saving model to ./model/48 - 0.1981.hdf5\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.19806 to 0.19452, saving model to ./model/49 - 0.1945.hdf5\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.19452 to 0.19231, saving model to ./model/50 - 0.1923.hdf5\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.19231 to 0.19156, saving model to ./model/51 - 0.1916.hdf5\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.19156\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.19156 to 0.19009, saving model to ./model/59 - 0.1901.hdf5\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.19009\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.19009 to 0.18921, saving model to ./model/68 - 0.1892.hdf5\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.18921 to 0.18814, saving model to ./model/69 - 0.1881.hdf5\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.18814\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.18814\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.18814\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.18814 to 0.18801, saving model to ./model/73 - 0.1880.hdf5\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.18801 to 0.18742, saving model to ./model/74 - 0.1874.hdf5\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.18742 to 0.18653, saving model to ./model/75 - 0.1865.hdf5\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.18653 to 0.18605, saving model to ./model/76 - 0.1860.hdf5\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.18605 to 0.18508, saving model to ./model/77 - 0.1851.hdf5\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.18508\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.18508 to 0.18504, saving model to ./model/79 - 0.1850.hdf5\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.18504 to 0.18501, saving model to ./model/80 - 0.1850.hdf5\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.18501\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.18501\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.18501\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.18501\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.18501\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.18501 to 0.18484, saving model to ./model/86 - 0.1848.hdf5\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.18484 to 0.18374, saving model to ./model/87 - 0.1837.hdf5\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.18374 to 0.18233, saving model to ./model/88 - 0.1823.hdf5\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.18233\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.18233 to 0.18187, saving model to ./model/96 - 0.1819.hdf5\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.18187 to 0.18075, saving model to ./model/97 - 0.1807.hdf5\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.18075 to 0.18032, saving model to ./model/98 - 0.1803.hdf5\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.18032 to 0.17989, saving model to ./model/99 - 0.1799.hdf5\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.17989 to 0.17936, saving model to ./model/100 - 0.1794.hdf5\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.17936 to 0.17862, saving model to ./model/101 - 0.1786.hdf5\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.17862 to 0.17679, saving model to ./model/102 - 0.1768.hdf5\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.17679\n",
            "\n",
            "Epoch 00113: val_loss improved from 0.17679 to 0.17671, saving model to ./model/113 - 0.1767.hdf5\n",
            "\n",
            "Epoch 00114: val_loss improved from 0.17671 to 0.17461, saving model to ./model/114 - 0.1746.hdf5\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.17461 to 0.17293, saving model to ./model/115 - 0.1729.hdf5\n",
            "\n",
            "Epoch 00116: val_loss improved from 0.17293 to 0.17260, saving model to ./model/116 - 0.1726.hdf5\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.17260\n",
            "\n",
            "Epoch 00130: val_loss improved from 0.17260 to 0.17000, saving model to ./model/130 - 0.1700.hdf5\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.17000 to 0.16961, saving model to ./model/131 - 0.1696.hdf5\n",
            "\n",
            "Epoch 00132: val_loss improved from 0.16961 to 0.16949, saving model to ./model/132 - 0.1695.hdf5\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.16949 to 0.16906, saving model to ./model/133 - 0.1691.hdf5\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.16906\n",
            "\n",
            "Epoch 00140: val_loss improved from 0.16906 to 0.16836, saving model to ./model/140 - 0.1684.hdf5\n",
            "\n",
            "Epoch 00141: val_loss improved from 0.16836 to 0.16620, saving model to ./model/141 - 0.1662.hdf5\n",
            "\n",
            "Epoch 00142: val_loss improved from 0.16620 to 0.16568, saving model to ./model/142 - 0.1657.hdf5\n",
            "\n",
            "Epoch 00143: val_loss improved from 0.16568 to 0.16512, saving model to ./model/143 - 0.1651.hdf5\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.16512\n",
            "\n",
            "Epoch 00150: val_loss improved from 0.16512 to 0.16455, saving model to ./model/150 - 0.1646.hdf5\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.16455\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.16455\n",
            "\n",
            "Epoch 00153: val_loss improved from 0.16455 to 0.16406, saving model to ./model/153 - 0.1641.hdf5\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.16406\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.16406\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.16406\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.16406\n",
            "\n",
            "Epoch 00158: val_loss improved from 0.16406 to 0.16244, saving model to ./model/158 - 0.1624.hdf5\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.16244\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.16244\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.16244\n",
            "\n",
            "Epoch 00162: val_loss improved from 0.16244 to 0.16177, saving model to ./model/162 - 0.1618.hdf5\n",
            "\n",
            "Epoch 00163: val_loss improved from 0.16177 to 0.15902, saving model to ./model/163 - 0.1590.hdf5\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.15902\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.15902\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.15902\n",
            "\n",
            "Epoch 00167: val_loss improved from 0.15902 to 0.15741, saving model to ./model/167 - 0.1574.hdf5\n",
            "\n",
            "Epoch 00168: val_loss improved from 0.15741 to 0.15449, saving model to ./model/168 - 0.1545.hdf5\n",
            "\n",
            "Epoch 00169: val_loss improved from 0.15449 to 0.15390, saving model to ./model/169 - 0.1539.hdf5\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.15390\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.15390\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.15390\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.15390\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.15390\n",
            "\n",
            "Epoch 00175: val_loss improved from 0.15390 to 0.15363, saving model to ./model/175 - 0.1536.hdf5\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.15363\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.15363\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.15363\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.15363\n",
            "\n",
            "Epoch 00180: val_loss improved from 0.15363 to 0.15078, saving model to ./model/180 - 0.1508.hdf5\n",
            "\n",
            "Epoch 00181: val_loss improved from 0.15078 to 0.14860, saving model to ./model/181 - 0.1486.hdf5\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.14860\n",
            "\n",
            "Epoch 00192: val_loss improved from 0.14860 to 0.14855, saving model to ./model/192 - 0.1485.hdf5\n",
            "\n",
            "Epoch 00193: val_loss improved from 0.14855 to 0.14519, saving model to ./model/193 - 0.1452.hdf5\n",
            "\n",
            "Epoch 00194: val_loss improved from 0.14519 to 0.14473, saving model to ./model/194 - 0.1447.hdf5\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.14473\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.14473\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.14473\n",
            "\n",
            "Epoch 00198: val_loss improved from 0.14473 to 0.14374, saving model to ./model/198 - 0.1437.hdf5\n",
            "\n",
            "Epoch 00199: val_loss improved from 0.14374 to 0.14204, saving model to ./model/199 - 0.1420.hdf5\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.14204\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.14204\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.14204\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.14204\n",
            "\n",
            "Epoch 00204: val_loss improved from 0.14204 to 0.14028, saving model to ./model/204 - 0.1403.hdf5\n",
            "\n",
            "Epoch 00205: val_loss improved from 0.14028 to 0.13996, saving model to ./model/205 - 0.1400.hdf5\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.13996\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.13996\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.13996\n",
            "\n",
            "Epoch 00209: val_loss improved from 0.13996 to 0.13981, saving model to ./model/209 - 0.1398.hdf5\n",
            "\n",
            "Epoch 00210: val_loss improved from 0.13981 to 0.13687, saving model to ./model/210 - 0.1369.hdf5\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.13687\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.13687\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.13687\n",
            "\n",
            "Epoch 00214: val_loss improved from 0.13687 to 0.13585, saving model to ./model/214 - 0.1358.hdf5\n",
            "\n",
            "Epoch 00215: val_loss improved from 0.13585 to 0.13558, saving model to ./model/215 - 0.1356.hdf5\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.13558\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.13558\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.13558\n",
            "\n",
            "Epoch 00219: val_loss improved from 0.13558 to 0.13465, saving model to ./model/219 - 0.1346.hdf5\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.13465\n",
            "\n",
            "Epoch 00227: val_loss improved from 0.13465 to 0.13245, saving model to ./model/227 - 0.1324.hdf5\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.13245\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.13245\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.13245\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.13245\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.13245\n",
            "\n",
            "Epoch 00233: val_loss improved from 0.13245 to 0.12950, saving model to ./model/233 - 0.1295.hdf5\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.12950\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.12950\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.12950\n",
            "\n",
            "Epoch 00237: val_loss improved from 0.12950 to 0.12701, saving model to ./model/237 - 0.1270.hdf5\n",
            "\n",
            "Epoch 00238: val_loss improved from 0.12701 to 0.12629, saving model to ./model/238 - 0.1263.hdf5\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.12629\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.12629\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.12629\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.12629\n",
            "\n",
            "Epoch 00243: val_loss improved from 0.12629 to 0.12205, saving model to ./model/243 - 0.1221.hdf5\n",
            "\n",
            "Epoch 00244: val_loss improved from 0.12205 to 0.12167, saving model to ./model/244 - 0.1217.hdf5\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.12167\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.12167\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.12167\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.12167\n",
            "\n",
            "Epoch 00249: val_loss improved from 0.12167 to 0.12020, saving model to ./model/249 - 0.1202.hdf5\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.12020\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.12020\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.12020\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.12020\n",
            "\n",
            "Epoch 00254: val_loss improved from 0.12020 to 0.11818, saving model to ./model/254 - 0.1182.hdf5\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.11818\n",
            "\n",
            "Epoch 00272: val_loss improved from 0.11818 to 0.11803, saving model to ./model/272 - 0.1180.hdf5\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.11803\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.11803\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.11803\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.11803\n",
            "\n",
            "Epoch 00277: val_loss improved from 0.11803 to 0.11785, saving model to ./model/277 - 0.1178.hdf5\n",
            "\n",
            "Epoch 00278: val_loss improved from 0.11785 to 0.11757, saving model to ./model/278 - 0.1176.hdf5\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.11757\n",
            "\n",
            "Epoch 00280: val_loss improved from 0.11757 to 0.11680, saving model to ./model/280 - 0.1168.hdf5\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.11680\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.11680\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.11680\n",
            "\n",
            "Epoch 00284: val_loss improved from 0.11680 to 0.11624, saving model to ./model/284 - 0.1162.hdf5\n",
            "\n",
            "Epoch 00285: val_loss improved from 0.11624 to 0.11612, saving model to ./model/285 - 0.1161.hdf5\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.11612\n",
            "\n",
            "Epoch 00287: val_loss improved from 0.11612 to 0.11449, saving model to ./model/287 - 0.1145.hdf5\n",
            "\n",
            "Epoch 00288: val_loss improved from 0.11449 to 0.11174, saving model to ./model/288 - 0.1117.hdf5\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.11174\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.11174\n",
            "\n",
            "Epoch 00291: val_loss improved from 0.11174 to 0.11081, saving model to ./model/291 - 0.1108.hdf5\n",
            "\n",
            "Epoch 00292: val_loss improved from 0.11081 to 0.10960, saving model to ./model/292 - 0.1096.hdf5\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.10960\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.10960\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.10960\n",
            "\n",
            "Epoch 00296: val_loss improved from 0.10960 to 0.10758, saving model to ./model/296 - 0.1076.hdf5\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.10758\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.10758\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.10758\n",
            "\n",
            "Epoch 00300: val_loss improved from 0.10758 to 0.10601, saving model to ./model/300 - 0.1060.hdf5\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.10601\n",
            "\n",
            "Epoch 00317: val_loss improved from 0.10601 to 0.10281, saving model to ./model/317 - 0.1028.hdf5\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.10281\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.10281\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.10281\n",
            "\n",
            "Epoch 00321: val_loss improved from 0.10281 to 0.10200, saving model to ./model/321 - 0.1020.hdf5\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.10200\n",
            "\n",
            "Epoch 00338: val_loss improved from 0.10200 to 0.10127, saving model to ./model/338 - 0.1013.hdf5\n",
            "\n",
            "Epoch 00339: val_loss improved from 0.10127 to 0.10086, saving model to ./model/339 - 0.1009.hdf5\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.10086\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.10086\n",
            "\n",
            "Epoch 00342: val_loss improved from 0.10086 to 0.09853, saving model to ./model/342 - 0.0985.hdf5\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.09853\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.09853\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.09853\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.09853\n",
            "\n",
            "Epoch 00347: val_loss improved from 0.09853 to 0.09698, saving model to ./model/347 - 0.0970.hdf5\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.09698\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.09698\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.09698\n",
            "\n",
            "Epoch 00351: val_loss improved from 0.09698 to 0.09485, saving model to ./model/351 - 0.0949.hdf5\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.09485\n",
            "\n",
            "Epoch 00355: val_loss improved from 0.09485 to 0.09436, saving model to ./model/355 - 0.0944.hdf5\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.09436\n",
            "\n",
            "Epoch 00384: val_loss improved from 0.09436 to 0.09380, saving model to ./model/384 - 0.0938.hdf5\n",
            "\n",
            "Epoch 00385: val_loss improved from 0.09380 to 0.09268, saving model to ./model/385 - 0.0927.hdf5\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.09268\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.09268\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.09268\n",
            "\n",
            "Epoch 00389: val_loss improved from 0.09268 to 0.09046, saving model to ./model/389 - 0.0905.hdf5\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.09046\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.09046\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.09046\n",
            "\n",
            "Epoch 00393: val_loss improved from 0.09046 to 0.09010, saving model to ./model/393 - 0.0901.hdf5\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.09010\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.09010\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.09010\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.09010\n",
            "\n",
            "Epoch 00398: val_loss improved from 0.09010 to 0.08875, saving model to ./model/398 - 0.0887.hdf5\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.08875\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.08875\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.08875\n",
            "\n",
            "Epoch 00402: val_loss improved from 0.08875 to 0.08859, saving model to ./model/402 - 0.0886.hdf5\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.08859\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.08859\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.08859\n",
            "\n",
            "Epoch 00406: val_loss improved from 0.08859 to 0.08717, saving model to ./model/406 - 0.0872.hdf5\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.08717\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.08717\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.08717\n",
            "\n",
            "Epoch 00410: val_loss improved from 0.08717 to 0.08713, saving model to ./model/410 - 0.0871.hdf5\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.08713\n",
            "\n",
            "Epoch 00418: val_loss improved from 0.08713 to 0.08556, saving model to ./model/418 - 0.0856.hdf5\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.08556\n",
            "\n",
            "Epoch 00430: val_loss improved from 0.08556 to 0.08519, saving model to ./model/430 - 0.0852.hdf5\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.08519\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.08519\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.08519\n",
            "\n",
            "Epoch 00434: val_loss improved from 0.08519 to 0.08465, saving model to ./model/434 - 0.0846.hdf5\n",
            "\n",
            "Epoch 00435: val_loss improved from 0.08465 to 0.08381, saving model to ./model/435 - 0.0838.hdf5\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.08381\n",
            "\n",
            "Epoch 00443: val_loss improved from 0.08381 to 0.08253, saving model to ./model/443 - 0.0825.hdf5\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.08253\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.08253\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.08253\n",
            "\n",
            "Epoch 00447: val_loss improved from 0.08253 to 0.08246, saving model to ./model/447 - 0.0825.hdf5\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.08246\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.08246\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.08246\n",
            "\n",
            "Epoch 00451: val_loss improved from 0.08246 to 0.08245, saving model to ./model/451 - 0.0825.hdf5\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.08245\n",
            "\n",
            "Epoch 00462: val_loss improved from 0.08245 to 0.08242, saving model to ./model/462 - 0.0824.hdf5\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.08242\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.08242\n",
            "\n",
            "Epoch 00465: val_loss improved from 0.08242 to 0.08233, saving model to ./model/465 - 0.0823.hdf5\n",
            "\n",
            "Epoch 00466: val_loss improved from 0.08233 to 0.08064, saving model to ./model/466 - 0.0806.hdf5\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.08064\n",
            "\n",
            "Epoch 00485: val_loss improved from 0.08064 to 0.07855, saving model to ./model/485 - 0.0786.hdf5\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.07855\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.07855\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.07855\n",
            "\n",
            "Epoch 00489: val_loss improved from 0.07855 to 0.07853, saving model to ./model/489 - 0.0785.hdf5\n",
            "\n",
            "Epoch 00490: val_loss improved from 0.07853 to 0.07753, saving model to ./model/490 - 0.0775.hdf5\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.07753\n",
            "\n",
            "Epoch 00504: val_loss improved from 0.07753 to 0.07578, saving model to ./model/504 - 0.0758.hdf5\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.07578\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.07578\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.07578\n",
            "\n",
            "Epoch 00508: val_loss improved from 0.07578 to 0.07562, saving model to ./model/508 - 0.0756.hdf5\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.07562\n",
            "\n",
            "Epoch 00533: val_loss improved from 0.07562 to 0.07396, saving model to ./model/533 - 0.0740.hdf5\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.07396\n",
            "\n",
            "Epoch 00544: val_loss improved from 0.07396 to 0.07327, saving model to ./model/544 - 0.0733.hdf5\n",
            "\n",
            "Epoch 00545: val_loss improved from 0.07327 to 0.07208, saving model to ./model/545 - 0.0721.hdf5\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.07208\n",
            "\n",
            "Epoch 00555: val_loss improved from 0.07208 to 0.07099, saving model to ./model/555 - 0.0710.hdf5\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.07099\n",
            "\n",
            "Epoch 00581: val_loss improved from 0.07099 to 0.07058, saving model to ./model/581 - 0.0706.hdf5\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.07058\n",
            "\n",
            "Epoch 00595: val_loss improved from 0.07058 to 0.07033, saving model to ./model/595 - 0.0703.hdf5\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.07033\n",
            "\n",
            "Epoch 00605: val_loss improved from 0.07033 to 0.06804, saving model to ./model/605 - 0.0680.hdf5\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.06804\n",
            "\n",
            "Epoch 00633: val_loss improved from 0.06804 to 0.06717, saving model to ./model/633 - 0.0672.hdf5\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.06717\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.06717\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.06717\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.06717\n",
            "\n",
            "Epoch 00638: val_loss improved from 0.06717 to 0.06668, saving model to ./model/638 - 0.0667.hdf5\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.06668\n",
            "\n",
            "Epoch 00664: val_loss improved from 0.06668 to 0.06617, saving model to ./model/664 - 0.0662.hdf5\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.06617\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.06617\n",
            "\n",
            "Epoch 00667: val_loss improved from 0.06617 to 0.06598, saving model to ./model/667 - 0.0660.hdf5\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.06598\n",
            "\n",
            "Epoch 00682: val_loss improved from 0.06598 to 0.06594, saving model to ./model/682 - 0.0659.hdf5\n",
            "\n",
            "Epoch 00683: val_loss improved from 0.06594 to 0.06586, saving model to ./model/683 - 0.0659.hdf5\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.06586\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.06586\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.06586\n",
            "\n",
            "Epoch 00687: val_loss improved from 0.06586 to 0.06514, saving model to ./model/687 - 0.0651.hdf5\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.06514\n",
            "\n",
            "Epoch 00709: val_loss improved from 0.06514 to 0.06496, saving model to ./model/709 - 0.0650.hdf5\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.06496\n",
            "\n",
            "Epoch 00721: val_loss improved from 0.06496 to 0.06470, saving model to ./model/721 - 0.0647.hdf5\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00751: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00752: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00753: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00754: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00755: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00756: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00757: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00758: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00759: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00760: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00761: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00762: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00763: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00764: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00765: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00766: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00767: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00768: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00769: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00770: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00771: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00772: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00773: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00774: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00775: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00776: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00777: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00778: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00779: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00780: val_loss did not improve from 0.06470\n",
            "\n",
            "Epoch 00781: val_loss improved from 0.06470 to 0.06380, saving model to ./model/781 - 0.0638.hdf5\n",
            "\n",
            "Epoch 00782: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00783: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00784: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00785: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00786: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00787: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00788: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00789: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00790: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00791: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00792: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00793: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00794: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00795: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00796: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00797: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00798: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00799: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00800: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00801: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00802: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00803: val_loss did not improve from 0.06380\n",
            "\n",
            "Epoch 00804: val_loss improved from 0.06380 to 0.06357, saving model to ./model/804 - 0.0636.hdf5\n",
            "\n",
            "Epoch 00805: val_loss did not improve from 0.06357\n",
            "\n",
            "Epoch 00806: val_loss did not improve from 0.06357\n",
            "\n",
            "Epoch 00807: val_loss did not improve from 0.06357\n",
            "\n",
            "Epoch 00808: val_loss improved from 0.06357 to 0.06297, saving model to ./model/808 - 0.0630.hdf5\n",
            "\n",
            "Epoch 00809: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00810: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00811: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00812: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00813: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00814: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00815: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00816: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00817: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00818: val_loss did not improve from 0.06297\n",
            "\n",
            "Epoch 00819: val_loss improved from 0.06297 to 0.06263, saving model to ./model/819 - 0.0626.hdf5\n",
            "\n",
            "Epoch 00820: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00821: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00822: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00823: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00824: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00825: val_loss did not improve from 0.06263\n",
            "\n",
            "Epoch 00826: val_loss improved from 0.06263 to 0.06237, saving model to ./model/826 - 0.0624.hdf5\n",
            "\n",
            "Epoch 00827: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00828: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00829: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00830: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00831: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00832: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00833: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00834: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00835: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00836: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00837: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00838: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00839: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00840: val_loss did not improve from 0.06237\n",
            "\n",
            "Epoch 00841: val_loss improved from 0.06237 to 0.06192, saving model to ./model/841 - 0.0619.hdf5\n",
            "\n",
            "Epoch 00842: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00843: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00844: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00845: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00846: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00847: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00848: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00849: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00850: val_loss did not improve from 0.06192\n",
            "\n",
            "Epoch 00851: val_loss improved from 0.06192 to 0.06146, saving model to ./model/851 - 0.0615.hdf5\n",
            "\n",
            "Epoch 00852: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00853: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00854: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00855: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00856: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00857: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00858: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00859: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00860: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00861: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00862: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00863: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00864: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00865: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00866: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00867: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00868: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00869: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00870: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00871: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00872: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00873: val_loss did not improve from 0.06146\n",
            "\n",
            "Epoch 00874: val_loss improved from 0.06146 to 0.06144, saving model to ./model/874 - 0.0614.hdf5\n",
            "\n",
            "Epoch 00875: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00876: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00877: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00878: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00879: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00880: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00881: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00882: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00883: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00884: val_loss did not improve from 0.06144\n",
            "\n",
            "Epoch 00885: val_loss improved from 0.06144 to 0.06080, saving model to ./model/885 - 0.0608.hdf5\n",
            "\n",
            "Epoch 00886: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00887: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00888: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00889: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00890: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00891: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00892: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00893: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00894: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00895: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00896: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00897: val_loss did not improve from 0.06080\n",
            "\n",
            "Epoch 00898: val_loss improved from 0.06080 to 0.06015, saving model to ./model/898 - 0.0601.hdf5\n",
            "\n",
            "Epoch 00899: val_loss did not improve from 0.06015\n",
            "\n",
            "Epoch 00900: val_loss did not improve from 0.06015\n",
            "\n",
            "Epoch 00901: val_loss improved from 0.06015 to 0.05967, saving model to ./model/901 - 0.0597.hdf5\n",
            "\n",
            "Epoch 00902: val_loss improved from 0.05967 to 0.05965, saving model to ./model/902 - 0.0596.hdf5\n",
            "\n",
            "Epoch 00903: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00904: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00905: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00906: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00907: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00908: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00909: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00910: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00911: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00912: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00913: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00914: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00915: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00916: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00917: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00918: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00919: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00920: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00921: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00922: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00923: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00924: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00925: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00926: val_loss did not improve from 0.05965\n",
            "\n",
            "Epoch 00927: val_loss improved from 0.05965 to 0.05944, saving model to ./model/927 - 0.0594.hdf5\n",
            "\n",
            "Epoch 00928: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00929: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00930: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00931: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00932: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00933: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00934: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00935: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00936: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00937: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00938: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00939: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00940: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00941: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00942: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00943: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00944: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00945: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00946: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00947: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00948: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00949: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00950: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00951: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00952: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00953: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00954: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00955: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00956: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00957: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00958: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00959: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00960: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00961: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00962: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00963: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00964: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00965: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00966: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00967: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00968: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00969: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00970: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00971: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00972: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00973: val_loss did not improve from 0.05944\n",
            "\n",
            "Epoch 00974: val_loss improved from 0.05944 to 0.05903, saving model to ./model/974 - 0.0590.hdf5\n",
            "\n",
            "Epoch 00975: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00976: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00977: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00978: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00979: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00980: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00981: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00982: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00983: val_loss did not improve from 0.05903\n",
            "\n",
            "Epoch 00984: val_loss improved from 0.05903 to 0.05890, saving model to ./model/984 - 0.0589.hdf5\n",
            "\n",
            "Epoch 00985: val_loss improved from 0.05890 to 0.05834, saving model to ./model/985 - 0.0583.hdf5\n",
            "\n",
            "Epoch 00986: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00987: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00988: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00989: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00990: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00991: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00992: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00993: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00994: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00995: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00996: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00997: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00998: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 00999: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01000: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01001: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01002: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01003: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01004: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01005: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01006: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01007: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01008: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01009: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01010: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01011: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01012: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01013: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01014: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01015: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01016: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01017: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01018: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01019: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01020: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01021: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01022: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01023: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01024: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01025: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01026: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01027: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01028: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01029: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01030: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01031: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01032: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01033: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01034: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01035: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01036: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01037: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01038: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01039: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01040: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01041: val_loss did not improve from 0.05834\n",
            "\n",
            "Epoch 01042: val_loss improved from 0.05834 to 0.05813, saving model to ./model/1042 - 0.0581.hdf5\n",
            "\n",
            "Epoch 01043: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01044: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01045: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01046: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01047: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01048: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01049: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01050: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01051: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01052: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01053: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01054: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01055: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01056: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01057: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01058: val_loss did not improve from 0.05813\n",
            "\n",
            "Epoch 01059: val_loss improved from 0.05813 to 0.05778, saving model to ./model/1059 - 0.0578.hdf5\n",
            "\n",
            "Epoch 01060: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01061: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01062: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01063: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01064: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01065: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01066: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01067: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01068: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01069: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01070: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01071: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01072: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01073: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01074: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01075: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01076: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01077: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01078: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01079: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01080: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01081: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01082: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01083: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01084: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01085: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01086: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01087: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01088: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01089: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01090: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01091: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01092: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01093: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01094: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01095: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01096: val_loss did not improve from 0.05778\n",
            "\n",
            "Epoch 01097: val_loss improved from 0.05778 to 0.05775, saving model to ./model/1097 - 0.0578.hdf5\n",
            "\n",
            "Epoch 01098: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01099: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01100: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01101: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01102: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01103: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01104: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01105: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01106: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01107: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01108: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01109: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01110: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01111: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01112: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01113: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01114: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01115: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01116: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01117: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01118: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01119: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01120: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01121: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01122: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01123: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01124: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01125: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01126: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01127: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01128: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01129: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01130: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01131: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01132: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01133: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01134: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01135: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01136: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01137: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01138: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01139: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01140: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01141: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01142: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01143: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01144: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01145: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01146: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01147: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01148: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01149: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01150: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01151: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01152: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01153: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01154: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01155: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01156: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01157: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01158: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01159: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01160: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01161: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01162: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01163: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01164: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01165: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01166: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01167: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01168: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01169: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01170: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01171: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01172: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01173: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01174: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01175: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01176: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01177: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01178: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01179: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01180: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01181: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01182: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01183: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01184: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01185: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01186: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01187: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01188: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01189: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01190: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01191: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01192: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01193: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01194: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01195: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01196: val_loss did not improve from 0.05775\n",
            "\n",
            "Epoch 01197: val_loss did not improve from 0.05775\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f87d073fb70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoxJBiWkoXoG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "47388951-f60a-4ae9-e302-c711fd9adb10"
      },
      "source": [
        "#-*- coding: utf-8 -*-\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "# seed 값 설정\n",
        "seed = 0\n",
        "numpy.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "df = pd.read_csv('/gdrive/My Drive/data_analysis/modu_deep_learning/deeplearning/dataset/housing.csv', delim_whitespace= True, header = None)\n",
        "'''\n",
        "print(df.info())\n",
        "print(df.head())\n",
        "'''\n",
        "dataset = df.values\n",
        "X = dataset[:,0:13]\n",
        "Y = dataset[:,13]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(loss='mean_squared_error',\n",
        "              optimizer='adam')\n",
        "\n",
        "model.fit(X_train, Y_train, epochs=500, batch_size=10)\n",
        "\n",
        "# 예측 값과 실제 값의 비교\n",
        "Y_prediction = model.predict(X_test).flatten()\n",
        "for i in range(10):\n",
        "    label = Y_test[i]\n",
        "    prediction = Y_prediction[i]\n",
        "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 8438.1172\n",
            "Epoch 2/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 732.8683\n",
            "Epoch 3/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 512.5950\n",
            "Epoch 4/500\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 388.6990\n",
            "Epoch 5/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 256.9753\n",
            "Epoch 6/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 170.7661\n",
            "Epoch 7/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 135.6643\n",
            "Epoch 8/500\n",
            "36/36 [==============================] - 0s 992us/step - loss: 123.0021\n",
            "Epoch 9/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 92.7171\n",
            "Epoch 10/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 80.1417\n",
            "Epoch 11/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 72.3035\n",
            "Epoch 12/500\n",
            "36/36 [==============================] - 0s 969us/step - loss: 69.8260\n",
            "Epoch 13/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 67.6638\n",
            "Epoch 14/500\n",
            "36/36 [==============================] - 0s 960us/step - loss: 65.8864\n",
            "Epoch 15/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 65.1428\n",
            "Epoch 16/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 63.5601\n",
            "Epoch 17/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 64.5949\n",
            "Epoch 18/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 61.6633\n",
            "Epoch 19/500\n",
            "36/36 [==============================] - 0s 971us/step - loss: 62.5243\n",
            "Epoch 20/500\n",
            "36/36 [==============================] - 0s 949us/step - loss: 59.6989\n",
            "Epoch 21/500\n",
            "36/36 [==============================] - 0s 965us/step - loss: 59.3723\n",
            "Epoch 22/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 58.5467\n",
            "Epoch 23/500\n",
            "36/36 [==============================] - 0s 975us/step - loss: 58.7318\n",
            "Epoch 24/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 56.2303\n",
            "Epoch 25/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 56.2374\n",
            "Epoch 26/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 56.2391\n",
            "Epoch 27/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 54.9049\n",
            "Epoch 28/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 54.9681\n",
            "Epoch 29/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 54.0103\n",
            "Epoch 30/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 53.4721\n",
            "Epoch 31/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 52.6120\n",
            "Epoch 32/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 52.3308\n",
            "Epoch 33/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 51.6922\n",
            "Epoch 34/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 51.4115\n",
            "Epoch 35/500\n",
            "36/36 [==============================] - 0s 972us/step - loss: 50.9594\n",
            "Epoch 36/500\n",
            "36/36 [==============================] - 0s 973us/step - loss: 50.0806\n",
            "Epoch 37/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 49.6587\n",
            "Epoch 38/500\n",
            "36/36 [==============================] - 0s 934us/step - loss: 50.1437\n",
            "Epoch 39/500\n",
            "36/36 [==============================] - 0s 952us/step - loss: 48.7939\n",
            "Epoch 40/500\n",
            "36/36 [==============================] - 0s 995us/step - loss: 48.2098\n",
            "Epoch 41/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 48.1594\n",
            "Epoch 42/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 47.6336\n",
            "Epoch 43/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 46.4014\n",
            "Epoch 44/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 46.0603\n",
            "Epoch 45/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 46.2956\n",
            "Epoch 46/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 46.1978\n",
            "Epoch 47/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 44.8460\n",
            "Epoch 48/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 43.1158\n",
            "Epoch 49/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 42.7315\n",
            "Epoch 50/500\n",
            "36/36 [==============================] - 0s 991us/step - loss: 42.7363\n",
            "Epoch 51/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 41.1165\n",
            "Epoch 52/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 41.3375\n",
            "Epoch 53/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 40.9050\n",
            "Epoch 54/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 39.9034\n",
            "Epoch 55/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 38.7777\n",
            "Epoch 56/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 39.4455\n",
            "Epoch 57/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 39.0700\n",
            "Epoch 58/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 37.7630\n",
            "Epoch 59/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 36.5026\n",
            "Epoch 60/500\n",
            "36/36 [==============================] - 0s 957us/step - loss: 36.5739\n",
            "Epoch 61/500\n",
            "36/36 [==============================] - 0s 988us/step - loss: 38.2261\n",
            "Epoch 62/500\n",
            "36/36 [==============================] - 0s 1000us/step - loss: 36.1457\n",
            "Epoch 63/500\n",
            "36/36 [==============================] - 0s 983us/step - loss: 36.0968\n",
            "Epoch 64/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 34.8202\n",
            "Epoch 65/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.8550\n",
            "Epoch 66/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 35.0674\n",
            "Epoch 67/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 33.6298\n",
            "Epoch 68/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.4776\n",
            "Epoch 69/500\n",
            "36/36 [==============================] - 0s 969us/step - loss: 37.0617\n",
            "Epoch 70/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.4909\n",
            "Epoch 71/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 32.8610\n",
            "Epoch 72/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.0652\n",
            "Epoch 73/500\n",
            "36/36 [==============================] - 0s 993us/step - loss: 31.9464\n",
            "Epoch 74/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.9932\n",
            "Epoch 75/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 32.2534\n",
            "Epoch 76/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.4632\n",
            "Epoch 77/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.7118\n",
            "Epoch 78/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.5142\n",
            "Epoch 79/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.4484\n",
            "Epoch 80/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.6032\n",
            "Epoch 81/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.3145\n",
            "Epoch 82/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.3669\n",
            "Epoch 83/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.6188\n",
            "Epoch 84/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.0275\n",
            "Epoch 85/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.3375\n",
            "Epoch 86/500\n",
            "36/36 [==============================] - 0s 970us/step - loss: 28.4044\n",
            "Epoch 87/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.4635\n",
            "Epoch 88/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.5894\n",
            "Epoch 89/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.4563\n",
            "Epoch 90/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 27.0863\n",
            "Epoch 91/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.2004\n",
            "Epoch 92/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.7010\n",
            "Epoch 93/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 27.8506\n",
            "Epoch 94/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.4518\n",
            "Epoch 95/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.2899\n",
            "Epoch 96/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.6983\n",
            "Epoch 97/500\n",
            "36/36 [==============================] - 0s 993us/step - loss: 25.7705\n",
            "Epoch 98/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.6901\n",
            "Epoch 99/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.3404\n",
            "Epoch 100/500\n",
            "36/36 [==============================] - 0s 976us/step - loss: 25.3621\n",
            "Epoch 101/500\n",
            "36/36 [==============================] - 0s 961us/step - loss: 25.5198\n",
            "Epoch 102/500\n",
            "36/36 [==============================] - 0s 995us/step - loss: 26.4034\n",
            "Epoch 103/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 27.0355\n",
            "Epoch 104/500\n",
            "36/36 [==============================] - 0s 985us/step - loss: 29.2855\n",
            "Epoch 105/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.3013\n",
            "Epoch 106/500\n",
            "36/36 [==============================] - 0s 999us/step - loss: 25.3194\n",
            "Epoch 107/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.9012\n",
            "Epoch 108/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.4835\n",
            "Epoch 109/500\n",
            "36/36 [==============================] - 0s 986us/step - loss: 24.7045\n",
            "Epoch 110/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.2438\n",
            "Epoch 111/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.0588\n",
            "Epoch 112/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.9572\n",
            "Epoch 113/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.1499\n",
            "Epoch 114/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 25.3550\n",
            "Epoch 115/500\n",
            "36/36 [==============================] - 0s 977us/step - loss: 23.2317\n",
            "Epoch 116/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.9471\n",
            "Epoch 117/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.0546\n",
            "Epoch 118/500\n",
            "36/36 [==============================] - 0s 961us/step - loss: 23.1469\n",
            "Epoch 119/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.7542\n",
            "Epoch 120/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.8985\n",
            "Epoch 121/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.4217\n",
            "Epoch 122/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.0718\n",
            "Epoch 123/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.9580\n",
            "Epoch 124/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 24.8591\n",
            "Epoch 125/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.1669\n",
            "Epoch 126/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.6643\n",
            "Epoch 127/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.3838\n",
            "Epoch 128/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.6999\n",
            "Epoch 129/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.6268\n",
            "Epoch 130/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 21.9588\n",
            "Epoch 131/500\n",
            "36/36 [==============================] - 0s 995us/step - loss: 23.2078\n",
            "Epoch 132/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.1276\n",
            "Epoch 133/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 23.4127\n",
            "Epoch 134/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 24.1749\n",
            "Epoch 135/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.8193\n",
            "Epoch 136/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.3802\n",
            "Epoch 137/500\n",
            "36/36 [==============================] - 0s 972us/step - loss: 21.4158\n",
            "Epoch 138/500\n",
            "36/36 [==============================] - 0s 959us/step - loss: 23.3578\n",
            "Epoch 139/500\n",
            "36/36 [==============================] - 0s 969us/step - loss: 20.7983\n",
            "Epoch 140/500\n",
            "36/36 [==============================] - 0s 939us/step - loss: 23.1516\n",
            "Epoch 141/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.0328\n",
            "Epoch 142/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.3153\n",
            "Epoch 143/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 23.2594\n",
            "Epoch 144/500\n",
            "36/36 [==============================] - 0s 979us/step - loss: 23.0867\n",
            "Epoch 145/500\n",
            "36/36 [==============================] - 0s 952us/step - loss: 27.0087\n",
            "Epoch 146/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 21.7518\n",
            "Epoch 147/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.0327\n",
            "Epoch 148/500\n",
            "36/36 [==============================] - 0s 951us/step - loss: 22.8959\n",
            "Epoch 149/500\n",
            "36/36 [==============================] - 0s 970us/step - loss: 22.4905\n",
            "Epoch 150/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 21.5998\n",
            "Epoch 151/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 23.4205\n",
            "Epoch 152/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 21.1253\n",
            "Epoch 153/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 20.1864\n",
            "Epoch 154/500\n",
            "36/36 [==============================] - 0s 975us/step - loss: 22.7077\n",
            "Epoch 155/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.3380\n",
            "Epoch 156/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 22.0297\n",
            "Epoch 157/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 20.7166\n",
            "Epoch 158/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 21.2586\n",
            "Epoch 159/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.4158\n",
            "Epoch 160/500\n",
            "36/36 [==============================] - 0s 994us/step - loss: 21.2197\n",
            "Epoch 161/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.8700\n",
            "Epoch 162/500\n",
            "36/36 [==============================] - 0s 958us/step - loss: 20.1266\n",
            "Epoch 163/500\n",
            "36/36 [==============================] - 0s 973us/step - loss: 20.3223\n",
            "Epoch 164/500\n",
            "36/36 [==============================] - 0s 997us/step - loss: 21.2626\n",
            "Epoch 165/500\n",
            "36/36 [==============================] - 0s 947us/step - loss: 19.6495\n",
            "Epoch 166/500\n",
            "36/36 [==============================] - 0s 977us/step - loss: 19.0726\n",
            "Epoch 167/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.4881\n",
            "Epoch 168/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 19.1696\n",
            "Epoch 169/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.7511\n",
            "Epoch 170/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.2152\n",
            "Epoch 171/500\n",
            "36/36 [==============================] - 0s 986us/step - loss: 18.2944\n",
            "Epoch 172/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.2965\n",
            "Epoch 173/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.2690\n",
            "Epoch 174/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.5533\n",
            "Epoch 175/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 18.9418\n",
            "Epoch 176/500\n",
            "36/36 [==============================] - 0s 988us/step - loss: 18.7512\n",
            "Epoch 177/500\n",
            "36/36 [==============================] - 0s 990us/step - loss: 18.7615\n",
            "Epoch 178/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.5743\n",
            "Epoch 179/500\n",
            "36/36 [==============================] - 0s 944us/step - loss: 17.7814\n",
            "Epoch 180/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.7651\n",
            "Epoch 181/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0862\n",
            "Epoch 182/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.2538\n",
            "Epoch 183/500\n",
            "36/36 [==============================] - 0s 988us/step - loss: 17.8485\n",
            "Epoch 184/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.4784\n",
            "Epoch 185/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 19.8855\n",
            "Epoch 186/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0514\n",
            "Epoch 187/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.9898\n",
            "Epoch 188/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.8454\n",
            "Epoch 189/500\n",
            "36/36 [==============================] - 0s 968us/step - loss: 17.3030\n",
            "Epoch 190/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9978\n",
            "Epoch 191/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.2636\n",
            "Epoch 192/500\n",
            "36/36 [==============================] - 0s 972us/step - loss: 17.8245\n",
            "Epoch 193/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.4317\n",
            "Epoch 194/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.5795\n",
            "Epoch 195/500\n",
            "36/36 [==============================] - 0s 977us/step - loss: 16.7098\n",
            "Epoch 196/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.5683\n",
            "Epoch 197/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0497\n",
            "Epoch 198/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.3721\n",
            "Epoch 199/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.3602\n",
            "Epoch 200/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.9043\n",
            "Epoch 201/500\n",
            "36/36 [==============================] - 0s 941us/step - loss: 17.7000\n",
            "Epoch 202/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.4169\n",
            "Epoch 203/500\n",
            "36/36 [==============================] - 0s 950us/step - loss: 18.7279\n",
            "Epoch 204/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.3570\n",
            "Epoch 205/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 17.1441\n",
            "Epoch 206/500\n",
            "36/36 [==============================] - 0s 960us/step - loss: 16.9227\n",
            "Epoch 207/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2464\n",
            "Epoch 208/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.5572\n",
            "Epoch 209/500\n",
            "36/36 [==============================] - 0s 988us/step - loss: 17.5568\n",
            "Epoch 210/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.8207\n",
            "Epoch 211/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.2549\n",
            "Epoch 212/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.2758\n",
            "Epoch 213/500\n",
            "36/36 [==============================] - 0s 994us/step - loss: 18.1820\n",
            "Epoch 214/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.1127\n",
            "Epoch 215/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.7504\n",
            "Epoch 216/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9299\n",
            "Epoch 217/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.0683\n",
            "Epoch 218/500\n",
            "36/36 [==============================] - 0s 997us/step - loss: 18.1730\n",
            "Epoch 219/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 16.4303\n",
            "Epoch 220/500\n",
            "36/36 [==============================] - 0s 995us/step - loss: 16.8563\n",
            "Epoch 221/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.6995\n",
            "Epoch 222/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.0377\n",
            "Epoch 223/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.8701\n",
            "Epoch 224/500\n",
            "36/36 [==============================] - 0s 969us/step - loss: 17.5707\n",
            "Epoch 225/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.2174\n",
            "Epoch 226/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.7042\n",
            "Epoch 227/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8505\n",
            "Epoch 228/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.6024\n",
            "Epoch 229/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9724\n",
            "Epoch 230/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.5188\n",
            "Epoch 231/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0533\n",
            "Epoch 232/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.3868\n",
            "Epoch 233/500\n",
            "36/36 [==============================] - 0s 966us/step - loss: 17.1703\n",
            "Epoch 234/500\n",
            "36/36 [==============================] - 0s 979us/step - loss: 16.2265\n",
            "Epoch 235/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.1297\n",
            "Epoch 236/500\n",
            "36/36 [==============================] - 0s 949us/step - loss: 15.4727\n",
            "Epoch 237/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.1580\n",
            "Epoch 238/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.3351\n",
            "Epoch 239/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2554\n",
            "Epoch 240/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.1162\n",
            "Epoch 241/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2902\n",
            "Epoch 242/500\n",
            "36/36 [==============================] - 0s 956us/step - loss: 16.0977\n",
            "Epoch 243/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.6062\n",
            "Epoch 244/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.5882\n",
            "Epoch 245/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.1776\n",
            "Epoch 246/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 17.2489\n",
            "Epoch 247/500\n",
            "36/36 [==============================] - 0s 971us/step - loss: 15.7997\n",
            "Epoch 248/500\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 15.5170\n",
            "Epoch 249/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2952\n",
            "Epoch 250/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7062\n",
            "Epoch 251/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7237\n",
            "Epoch 252/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8035\n",
            "Epoch 253/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.5397\n",
            "Epoch 254/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8606\n",
            "Epoch 255/500\n",
            "36/36 [==============================] - 0s 991us/step - loss: 15.3825\n",
            "Epoch 256/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 16.7219\n",
            "Epoch 257/500\n",
            "36/36 [==============================] - 0s 973us/step - loss: 15.6718\n",
            "Epoch 258/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8412\n",
            "Epoch 259/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.1103\n",
            "Epoch 260/500\n",
            "36/36 [==============================] - 0s 948us/step - loss: 15.3924\n",
            "Epoch 261/500\n",
            "36/36 [==============================] - 0s 951us/step - loss: 17.1231\n",
            "Epoch 262/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.1461\n",
            "Epoch 263/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0019\n",
            "Epoch 264/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.3364\n",
            "Epoch 265/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.5488\n",
            "Epoch 266/500\n",
            "36/36 [==============================] - 0s 953us/step - loss: 16.6981\n",
            "Epoch 267/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.6284\n",
            "Epoch 268/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.6322\n",
            "Epoch 269/500\n",
            "36/36 [==============================] - 0s 968us/step - loss: 15.2490\n",
            "Epoch 270/500\n",
            "36/36 [==============================] - 0s 954us/step - loss: 17.9617\n",
            "Epoch 271/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 20.2294\n",
            "Epoch 272/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.7793\n",
            "Epoch 273/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1797\n",
            "Epoch 274/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7560\n",
            "Epoch 275/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 15.0628\n",
            "Epoch 276/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2174\n",
            "Epoch 277/500\n",
            "36/36 [==============================] - 0s 982us/step - loss: 16.6075\n",
            "Epoch 278/500\n",
            "36/36 [==============================] - 0s 992us/step - loss: 16.0650\n",
            "Epoch 279/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.0659\n",
            "Epoch 280/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.8256\n",
            "Epoch 281/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.0061\n",
            "Epoch 282/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.4724\n",
            "Epoch 283/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.0268\n",
            "Epoch 284/500\n",
            "36/36 [==============================] - 0s 992us/step - loss: 15.6225\n",
            "Epoch 285/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.8782\n",
            "Epoch 286/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9390\n",
            "Epoch 287/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7772\n",
            "Epoch 288/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.1138\n",
            "Epoch 289/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.9582\n",
            "Epoch 290/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2621\n",
            "Epoch 291/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1928\n",
            "Epoch 292/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 18.1654\n",
            "Epoch 293/500\n",
            "36/36 [==============================] - 0s 978us/step - loss: 15.2244\n",
            "Epoch 294/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.4251\n",
            "Epoch 295/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.7134\n",
            "Epoch 296/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.4380\n",
            "Epoch 297/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9540\n",
            "Epoch 298/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.0872\n",
            "Epoch 299/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1552\n",
            "Epoch 300/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.0205\n",
            "Epoch 301/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 15.4208\n",
            "Epoch 302/500\n",
            "36/36 [==============================] - 0s 997us/step - loss: 15.9812\n",
            "Epoch 303/500\n",
            "36/36 [==============================] - 0s 990us/step - loss: 17.2468\n",
            "Epoch 304/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.6473\n",
            "Epoch 305/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7488\n",
            "Epoch 306/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.3806\n",
            "Epoch 307/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8375\n",
            "Epoch 308/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3971\n",
            "Epoch 309/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.5611\n",
            "Epoch 310/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9426\n",
            "Epoch 311/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 19.7271\n",
            "Epoch 312/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2459\n",
            "Epoch 313/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2922\n",
            "Epoch 314/500\n",
            "36/36 [==============================] - 0s 990us/step - loss: 16.9760\n",
            "Epoch 315/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.6491\n",
            "Epoch 316/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.1080\n",
            "Epoch 317/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 15.0583\n",
            "Epoch 318/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.3291\n",
            "Epoch 319/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3406\n",
            "Epoch 320/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.7196\n",
            "Epoch 321/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1568\n",
            "Epoch 322/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.9220\n",
            "Epoch 323/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1044\n",
            "Epoch 324/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 16.0277\n",
            "Epoch 325/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8159\n",
            "Epoch 326/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7553\n",
            "Epoch 327/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 17.0138\n",
            "Epoch 328/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8213\n",
            "Epoch 329/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.5577\n",
            "Epoch 330/500\n",
            "36/36 [==============================] - 0s 993us/step - loss: 14.9216\n",
            "Epoch 331/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7029\n",
            "Epoch 332/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2386\n",
            "Epoch 333/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 14.6427\n",
            "Epoch 334/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7886\n",
            "Epoch 335/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2298\n",
            "Epoch 336/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 17.2208\n",
            "Epoch 337/500\n",
            "36/36 [==============================] - 0s 969us/step - loss: 15.6517\n",
            "Epoch 338/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8319\n",
            "Epoch 339/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9555\n",
            "Epoch 340/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3515\n",
            "Epoch 341/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.6436\n",
            "Epoch 342/500\n",
            "36/36 [==============================] - 0s 983us/step - loss: 15.5887\n",
            "Epoch 343/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1906\n",
            "Epoch 344/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 15.1762\n",
            "Epoch 345/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.6993\n",
            "Epoch 346/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.6348\n",
            "Epoch 347/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1131\n",
            "Epoch 348/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 18.6760\n",
            "Epoch 349/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8558\n",
            "Epoch 350/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.6860\n",
            "Epoch 351/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 15.1306\n",
            "Epoch 352/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1682\n",
            "Epoch 353/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8328\n",
            "Epoch 354/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7920\n",
            "Epoch 355/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5984\n",
            "Epoch 356/500\n",
            "36/36 [==============================] - 0s 997us/step - loss: 16.1613\n",
            "Epoch 357/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.6552\n",
            "Epoch 358/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8628\n",
            "Epoch 359/500\n",
            "36/36 [==============================] - 0s 971us/step - loss: 14.2003\n",
            "Epoch 360/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3367\n",
            "Epoch 361/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8174\n",
            "Epoch 362/500\n",
            "36/36 [==============================] - 0s 987us/step - loss: 15.3475\n",
            "Epoch 363/500\n",
            "36/36 [==============================] - 0s 973us/step - loss: 16.0952\n",
            "Epoch 364/500\n",
            "36/36 [==============================] - 0s 933us/step - loss: 17.8531\n",
            "Epoch 365/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9803\n",
            "Epoch 366/500\n",
            "36/36 [==============================] - 0s 952us/step - loss: 14.8569\n",
            "Epoch 367/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3676\n",
            "Epoch 368/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.2904\n",
            "Epoch 369/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5509\n",
            "Epoch 370/500\n",
            "36/36 [==============================] - 0s 995us/step - loss: 14.2986\n",
            "Epoch 371/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 19.1486\n",
            "Epoch 372/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3944\n",
            "Epoch 373/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7448\n",
            "Epoch 374/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.0690\n",
            "Epoch 375/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.9093\n",
            "Epoch 376/500\n",
            "36/36 [==============================] - 0s 963us/step - loss: 13.9533\n",
            "Epoch 377/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 14.1858\n",
            "Epoch 378/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.7706\n",
            "Epoch 379/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.8297\n",
            "Epoch 380/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.9684\n",
            "Epoch 381/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.6728\n",
            "Epoch 382/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.9462\n",
            "Epoch 383/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 14.0110\n",
            "Epoch 384/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8745\n",
            "Epoch 385/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3182\n",
            "Epoch 386/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5385\n",
            "Epoch 387/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9748\n",
            "Epoch 388/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5529\n",
            "Epoch 389/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7923\n",
            "Epoch 390/500\n",
            "36/36 [==============================] - 0s 997us/step - loss: 14.4234\n",
            "Epoch 391/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.4758\n",
            "Epoch 392/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 13.9802\n",
            "Epoch 393/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.4638\n",
            "Epoch 394/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.9003\n",
            "Epoch 395/500\n",
            "36/36 [==============================] - 0s 964us/step - loss: 14.7650\n",
            "Epoch 396/500\n",
            "36/36 [==============================] - 0s 994us/step - loss: 15.2293\n",
            "Epoch 397/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5486\n",
            "Epoch 398/500\n",
            "36/36 [==============================] - 0s 987us/step - loss: 13.7544\n",
            "Epoch 399/500\n",
            "36/36 [==============================] - 0s 975us/step - loss: 14.0797\n",
            "Epoch 400/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8367\n",
            "Epoch 401/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5180\n",
            "Epoch 402/500\n",
            "36/36 [==============================] - 0s 954us/step - loss: 15.1463\n",
            "Epoch 403/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.8602\n",
            "Epoch 404/500\n",
            "36/36 [==============================] - 0s 984us/step - loss: 13.8532\n",
            "Epoch 405/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.4736\n",
            "Epoch 406/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.1634\n",
            "Epoch 407/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.2346\n",
            "Epoch 408/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.1737\n",
            "Epoch 409/500\n",
            "36/36 [==============================] - 0s 990us/step - loss: 14.2390\n",
            "Epoch 410/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5482\n",
            "Epoch 411/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.1496\n",
            "Epoch 412/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.2904\n",
            "Epoch 413/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.9705\n",
            "Epoch 414/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7663\n",
            "Epoch 415/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.6598\n",
            "Epoch 416/500\n",
            "36/36 [==============================] - 0s 961us/step - loss: 13.6604\n",
            "Epoch 417/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7701\n",
            "Epoch 418/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7163\n",
            "Epoch 419/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.1144\n",
            "Epoch 420/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.4224\n",
            "Epoch 421/500\n",
            "36/36 [==============================] - 0s 955us/step - loss: 13.7578\n",
            "Epoch 422/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.3877\n",
            "Epoch 423/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9763\n",
            "Epoch 424/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 14.0655\n",
            "Epoch 425/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.7501\n",
            "Epoch 426/500\n",
            "36/36 [==============================] - 0s 996us/step - loss: 14.1879\n",
            "Epoch 427/500\n",
            "36/36 [==============================] - 0s 978us/step - loss: 13.6733\n",
            "Epoch 428/500\n",
            "36/36 [==============================] - 0s 998us/step - loss: 15.6268\n",
            "Epoch 429/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.4527\n",
            "Epoch 430/500\n",
            "36/36 [==============================] - 0s 975us/step - loss: 13.9305\n",
            "Epoch 431/500\n",
            "36/36 [==============================] - 0s 973us/step - loss: 13.3554\n",
            "Epoch 432/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0323\n",
            "Epoch 433/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7044\n",
            "Epoch 434/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.5251\n",
            "Epoch 435/500\n",
            "36/36 [==============================] - 0s 977us/step - loss: 13.8080\n",
            "Epoch 436/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 22.6001\n",
            "Epoch 437/500\n",
            "36/36 [==============================] - 0s 950us/step - loss: 13.1989\n",
            "Epoch 438/500\n",
            "36/36 [==============================] - 0s 988us/step - loss: 15.0137\n",
            "Epoch 439/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.8114\n",
            "Epoch 440/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.9122\n",
            "Epoch 441/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5668\n",
            "Epoch 442/500\n",
            "36/36 [==============================] - 0s 982us/step - loss: 14.4559\n",
            "Epoch 443/500\n",
            "36/36 [==============================] - 0s 980us/step - loss: 13.1531\n",
            "Epoch 444/500\n",
            "36/36 [==============================] - 0s 968us/step - loss: 14.8038\n",
            "Epoch 445/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.2744\n",
            "Epoch 446/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.3789\n",
            "Epoch 447/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.6952\n",
            "Epoch 448/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.6575\n",
            "Epoch 449/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1863\n",
            "Epoch 450/500\n",
            "36/36 [==============================] - 0s 958us/step - loss: 14.4244\n",
            "Epoch 451/500\n",
            "36/36 [==============================] - 0s 959us/step - loss: 13.8176\n",
            "Epoch 452/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.9439\n",
            "Epoch 453/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.8212\n",
            "Epoch 454/500\n",
            "36/36 [==============================] - 0s 974us/step - loss: 13.7300\n",
            "Epoch 455/500\n",
            "36/36 [==============================] - 0s 972us/step - loss: 13.5792\n",
            "Epoch 456/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0921\n",
            "Epoch 457/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.4980\n",
            "Epoch 458/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.5896\n",
            "Epoch 459/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 14.1733\n",
            "Epoch 460/500\n",
            "36/36 [==============================] - 0s 983us/step - loss: 15.1430\n",
            "Epoch 461/500\n",
            "36/36 [==============================] - 0s 966us/step - loss: 13.7520\n",
            "Epoch 462/500\n",
            "36/36 [==============================] - 0s 979us/step - loss: 15.6210\n",
            "Epoch 463/500\n",
            "36/36 [==============================] - 0s 983us/step - loss: 14.1622\n",
            "Epoch 464/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.8945\n",
            "Epoch 465/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7102\n",
            "Epoch 466/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.4806\n",
            "Epoch 467/500\n",
            "36/36 [==============================] - 0s 999us/step - loss: 13.8032\n",
            "Epoch 468/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.2762\n",
            "Epoch 469/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0244\n",
            "Epoch 470/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.2895\n",
            "Epoch 471/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 21.0620\n",
            "Epoch 472/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0084\n",
            "Epoch 473/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.6990\n",
            "Epoch 474/500\n",
            "36/36 [==============================] - 0s 985us/step - loss: 14.0826\n",
            "Epoch 475/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.7523\n",
            "Epoch 476/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.2991\n",
            "Epoch 477/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.4057\n",
            "Epoch 478/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0238\n",
            "Epoch 479/500\n",
            "36/36 [==============================] - 0s 950us/step - loss: 14.4466\n",
            "Epoch 480/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 16.0106\n",
            "Epoch 481/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.0092\n",
            "Epoch 482/500\n",
            "36/36 [==============================] - 0s 994us/step - loss: 13.8614\n",
            "Epoch 483/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.1553\n",
            "Epoch 484/500\n",
            "36/36 [==============================] - 0s 991us/step - loss: 14.4741\n",
            "Epoch 485/500\n",
            "36/36 [==============================] - 0s 999us/step - loss: 13.7864\n",
            "Epoch 486/500\n",
            "36/36 [==============================] - 0s 975us/step - loss: 13.4512\n",
            "Epoch 487/500\n",
            "36/36 [==============================] - 0s 999us/step - loss: 14.7266\n",
            "Epoch 488/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.9146\n",
            "Epoch 489/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.8888\n",
            "Epoch 490/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 12.7450\n",
            "Epoch 491/500\n",
            "36/36 [==============================] - 0s 989us/step - loss: 12.7037\n",
            "Epoch 492/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.2927\n",
            "Epoch 493/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.3202\n",
            "Epoch 494/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 15.1460\n",
            "Epoch 495/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 12.7563\n",
            "Epoch 496/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.2830\n",
            "Epoch 497/500\n",
            "36/36 [==============================] - 0s 981us/step - loss: 13.5112\n",
            "Epoch 498/500\n",
            "36/36 [==============================] - 0s 933us/step - loss: 12.7978\n",
            "Epoch 499/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 13.5248\n",
            "Epoch 500/500\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 14.0182\n",
            "실제가격: 22.600, 예상가격: 20.869\n",
            "실제가격: 50.000, 예상가격: 26.512\n",
            "실제가격: 23.000, 예상가격: 22.993\n",
            "실제가격: 8.300, 예상가격: 11.552\n",
            "실제가격: 21.200, 예상가격: 19.796\n",
            "실제가격: 19.900, 예상가격: 22.126\n",
            "실제가격: 20.600, 예상가격: 17.534\n",
            "실제가격: 18.700, 예상가격: 23.117\n",
            "실제가격: 16.100, 예상가격: 18.259\n",
            "실제가격: 18.600, 예상가격: 13.516\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szdJzSADODdN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 861
        },
        "outputId": "e030c23c-1a08-4a65-d05f-c962986ede26"
      },
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "# seed값 설정\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "# MNIST 데이터셋 불러오기\n",
        "(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n",
        "\n",
        "print(\"학습셋 이미지 수 : %d 개\" % (X_train.shape[0]))\n",
        "print(\"테스트셋 이미지 수 : %d 개\" % (X_test.shape[0]))\n",
        "\n",
        "# 그래프로 확인\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(X_train[0], cmap='Greys')\n",
        "\n",
        "# 코드로 확인\n",
        "for x in X_train[0]:\n",
        "  for i in x:\n",
        "    sys.stdout.write('%d\\t' % i)\n",
        "  sys.stdout.write('\\n')\n",
        "\n",
        "# 차원 변환 과정\n",
        "X_train = X_train.reshape(X_train.shape[0], 784)\n",
        "X_train = X_train.astype('float64')\n",
        "X_train = X_train / 255\n",
        "\n",
        "X_test = X_test.reshape(X_test.shape[0], 784).astype('float64') / 255\n",
        "\n",
        "# 클래스 값 확인\n",
        "print(\"class : %d\" % (Y_class_train[0]))\n",
        "\n",
        "# 바이너리화 과정\n",
        "Y_train = np_utils.to_categorical(Y_class_train, 10)\n",
        "Y_test = np_utils.to_categorical(Y_class_test, 10)\n",
        "\n",
        "print(Y_train[0])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "학습셋 이미지 수 : 60000 개\n",
            "테스트셋 이미지 수 : 10000 개\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t3\t18\t18\t18\t126\t136\t175\t26\t166\t255\t247\t127\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t30\t36\t94\t154\t170\t253\t253\t253\t253\t253\t225\t172\t253\t242\t195\t64\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t49\t238\t253\t253\t253\t253\t253\t253\t253\t253\t251\t93\t82\t82\t56\t39\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t18\t219\t253\t253\t253\t253\t253\t198\t182\t247\t241\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t80\t156\t107\t253\t253\t205\t11\t0\t43\t154\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t14\t1\t154\t253\t90\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t139\t253\t190\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t11\t190\t253\t70\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t35\t241\t225\t160\t108\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t81\t240\t253\t253\t119\t25\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t45\t186\t253\t253\t150\t27\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t16\t93\t252\t253\t187\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t249\t253\t249\t64\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t46\t130\t183\t253\t253\t207\t2\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t39\t148\t229\t253\t253\t253\t250\t182\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t24\t114\t221\t253\t253\t253\t253\t201\t78\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t23\t66\t213\t253\t253\t253\t253\t198\t81\t2\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t18\t171\t219\t253\t253\t253\t253\t195\t80\t9\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t55\t172\t226\t253\t253\t253\t253\t244\t133\t11\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t136\t253\t253\t253\t212\t135\t132\t16\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t\n",
            "class : 5\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHUY4vXUW-ZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}